Our Naive Bayes classifier will look very similar. Essentially, we will have a feature distribution p(x i | y) for each class label y where x i is a feature. Given an unseen document, we will calculate the most likely class distribution that it is generated from. That is, we wish to calculate p(y | x) for each label y âˆˆ Y. Let's use our knowledge of Bayes' rule from Chapter 2 to rewrite this into a form we can use programmatically given a document x. Notice that we eliminate the denominator produced by Bayes' Rule since it does not change the arg max result. The final simplification is the independence assumption that none of the features depend on one another, letting us simply multiply all the probabilities together when finding the joint probability. It is for this reason that Naive Bayes is called naive. This means we need to estimate the following distributions: p(y) for all classes and p(x i | y) for each feature in each class. This estimation is done in the exact same way as our unigram language model estimation. That is, an easy inference method is maximum likelihood estimation, where we count the number of times a feature occurs in a class divided by its total number of occurrences. As discussed in Chapter 2 this may lead to some issues with unseen words or sparse data. In this case, we can smooth the estimated probabilities using any smoothing method we'd like as discussed in Chapter 6. We've covered Dirichlet prior smoothing and Jelinek-Mercer interpolation, among others. Finally, we need to calculate p(y), which is just the probability of each class label. This parameter is essential when the class labels are unbalanced; that is, we don't want to predict a label that occurs only a few times in the training data at the same rate that we predict the majority label. Whereas k-NN spent most of its calculation time in testing, Naive Bayes spends its time in training while estimating the model parameters. In testing, |Y| calculations are performed to find the most likely label. When learning the parameters, a forward index is used so it is known which class label to attribute features to; that is, look up the counts in each document, and update the parameter for that 15.5 Classification Algorithms 311 document's true class label. An inverted index is not necessary for this usage. Memory aside from the forward index is required to store the parameters, which can be represented as O(|Y| + |V | . |Y|) floating point numbers. Due to its simplicity and strong independence assumptions, Naive Bayes is often outperformed by more sophisticated classification methods, many of which are based on the linear classifiers discussed in the next section. 