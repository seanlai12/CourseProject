The classic reference for PageRank is Page et al. [1999], and that for HITS is Kleinberg [1999]. Lin and Dyer [2010] provides an excellent introduction to using MapReduce for text processing applications, including particularly a detailed treatment of how to use MapReduce for constructing an inverted index. Liu [2009] gives an excellent survey of research work on learning to rank. to schedule crawling these links and to not get stuck in a cycle. This is easily avoided by keeping a list of already visited sites. Using wget is not the only way to build a simple web crawler. You can make your own in Ruby or Python. These languages have many useful libraries for crawling. It's also possible to use an existing web crawler. For Python, Scrapy and Beautiful Soup are two popular crawling tools. The last important point is to have a short wait period in between page requests. Not only is it considered polite to wait a few seconds between requests to avoid hammering the server, you may get blocked if you attempt to crawl too fast! Use one of the above-mentioned tools to create a simple web crawler. Limit your crawling to 100 pages initially. 