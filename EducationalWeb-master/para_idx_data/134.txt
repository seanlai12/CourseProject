For example, if we want to score the term computer, which is term ID 56, we look up 56 in the lexicon. The information we receive could be: Term ID: 56 Document frequency: 78 Total number of occurrences: 443 Offset into postings file: 8923754 Of course, the actual lexicon would just store 56 â†’ {78, 443, 8923754}. Since the tokenizer assigned term IDs sequentially, we could represent the lexicon as a large array indexed by term ID. Each element in the large array would store tuples of (document frequency, total count, offset) information. If we seek to position 8,923,754 in the large postings file, we could see something like which is the counts and position information for the 78 documents that term ID 56 appears in. Notice how the doc IDs (and positions) are stored in increasing order; this is a fact we will take advantage of when compressing the postings file. Also make note of the large difference in size of the lexicon and postings file. For each entry in the lexicon, we know we will only store three values per term. In the postings file, we store at least three values (doc ID, count, positions) for each document that the term appears in. If the term appears in all documents, we'd have a list of the length of the number of documents in the corpus. This is true for all unique terms. For this reason, we often assume that the lexicon can fit into main memory and the postings file resides on disk, and is seeked into based on pointers from the lexicon. Indexing is the process of creating these data structures based on a set of tokenized documents. A popular approach for indexing is the following sorting-based approach. . Scan the raw document stream sequentially. In tokenization, assign each document an ID. Tokenize each document to obtain term IDs, creating new term IDs as needed. . While scanning documents, collect term counts for each term-document pair and build an inverted index for a subset of documents in memory. When we reach the limit of memory, write the incomplete inverted index into the disk. (It will be the same format as the resulting postings file, just smaller.) . Continue this process to generate many incomplete inverted indices (called "runs") all written on disk. . Merge all these runs in a pair-wise manner to produce a single sorted (by term ID) postings file. This algorithm is essentially the merge function from mergesort. . Once the postings file is created, create the lexicon by scanning through the postings file and assigning the offset values for each term ID. The terms from multiple documents are then sorted by term ID in small postings chunks that fit in memory before they are flushed to the disk.  