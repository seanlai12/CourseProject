In the previous section, we derived a TF-IDF weighting formula using the vector space model and showed that this model actually works pretty well for the examples shown in the figures-except for d 5 , which has a very high score. This document is intuitively non-relevant, so its position is not desirable. Now, we're going to discuss how to use a TF transformation to solve this problem. Before we discuss the details, let's take a look at the formula for the TF-IDF weighting and ranking function we previously derived. It is shown in Figure 6.13. If you look at the formula carefully, you will see it involves a sum over all the matched query terms. Inside the sum, each matched query term has a particular weight; this weight is TF-IDF weighting. It has an IDF component where we see two variables: one is the total number of documents in the collection, M. The other is the document frequency, df(w), which is the number of documents that contain w. The other variables involved in the formula include the count of the query term 6.3 Vector Space Retrieval Models 103 Total number of documents in collection Document frequency All matched query words in document w in the query, and the count of w in the document, represented as c(w, q) and c(w, d), respectively. Looking at d 5 again, it's not hard to realize that the reason why it has received a high score is because it has a very high count of the term campaign. Its count in d 5 is four, which is much higher than the other documents, and has contributed to the high score of this document. Intriguingly, in order to lower the score for this document, we need to somehow restrict the contribution of matching this term in the document. Essentially, we shouldn't reward multiple occurrences so generously. The first occurrence of a term says a lot about matching of this term because it goes from a zero count to a count of one, and that increase is very informative. Once we see a word in the document, it's very likely that the document is talking about this word. If we see an extra occurrence on top of the first occurrence, that is to go from one to two, then we also can say the second occurrence confirmed that it's not an accidental mention of the word. But imagine we have seen, let's say, 50 occurrences of the word in the document. Then, adding one extra occurrence is not going to bring new evidence about the term because we are already sure that this document is about this word. Thus, we should restrict the contribution of a high-count term. That is exactly the idea of TF transformation, illustrated in Figure 6.14. This transformation function is going to turn the raw count of word into a TF weight for the word in the document. On the x-axis is the raw count, and on the y-axis is the TF weight. In the previous ranking functions, we actually have implicitly used some kind of transformation. For example, in the zero-one bit vector representation, we actually used the binary transformation function as shown here. If the count is zero then it has zero weight. Otherwise it would have a weight of one. Then, we considered term count as a TF weight, which is a linear function. We just saw that this is not desirable. With a logarithm, we can have a sublinear transformation that looks like the red lines in the figure. This will control the influence of a very high weight because it's going to lower its influence, yet it will retain the influence of a small count. We might even want to bend the curve more by applying a logarithm twice. Researchers have tried all these methods and they are indeed working better than the linear transformation, but so far what works the best seems to be this special transformation called BM25 TF, illustrated in Figure 6.15, where BM stands for best matching. In this transformation, there is a parameter k which controls the upper bound of this function. It's easy to see this function has a upper bound, because if you look at the x x+k as being multiplied by (k + 1), the fraction will never exceed one, since the numerator is always less than the denominator. Thus, it's upper-bounded by (k + 1). This is also the difference between the BM25 TF function and the logarithm transformation, which doesn't have an upper bound. Furthermore, one interesting property of this function is that as we vary k, we can actually simulate different transformation functions including the two extremes that are shown in the figure. When k = 0, we have a zero one bit transformation. If we set k to a very large number, on the other hand, it's going to look more like the linear transformation function. In this sense, this transformation is very flexible since it allows us to control the shape of the TF curve quite easily. It also has the nice property of a simple upper bound. This upper bound is useful to control the influence of a particular term. For example, we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term. In other words, this upper bound  Very large k ensures that all terms will be counted when we aggregate the weights to compute a score. To summarize, we need to capture some sublinearity in the TF function. This ensures that we represent the intuition of diminishing return from high term counts. It also avoids a dominance by one single term over all others. The BM25 TF formula we discussed has an upper bound while being robust and effective. If we plug this function into our TF-IDF vector space model, then we would end up having a ranking function with a BM25 TF component. This is very close to a state-of-the-art ranking function called BM25. We'll see the entire BM25 formula soon. 