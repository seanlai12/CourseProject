As with information retrieval evaluation, we can use precision, recall, and F 1 score by considering true positives, false positives, true negatives, and false negatives. We are also usually more concerned about accuracy (the number of correct predictions divided by the number of total predictions). Training and testing splits were mentioned in the previous sections, but another partition of the total corpus is also sometimes used; this is the development set, used for parameter tuning. Typically, a corpus is split into about 80% training, 10% development, and 10% testing. For example, consider the problem of determining a good k value for k-NN. An index is created over the training documents, for (e.g.) k = 5. The accuracy is determined using the development documents. This is repeated for k = 10, 15, 20, 25. The best-performing k-value is then finally run on the testing set to find the overall accuracy. The purpose of the development set is to prevent overfitting, or tailoring the learning algorithm too much to a particular corpus subset and losing generality. A trained model is robust if it is not prone to overfitting. Another evaluation paradigm is n-fold cross validation. This splits the corpus into n partitions. In n rounds, one partition is selected as the testing set and the remaining n âˆ’ 1 are used for training. The final accuracy, F 1 score, or any other evaluation metric is then averaged over the n folds. The variance in scores between the folds can be a hint at the overfitting potential of your algorithm. If the variance is high, it means that the accuracies are not very similar between folds. Having one fold with a very high accuracy suggests that your learning algorithm may have overfit during that training stage; when using that trained algorithm on a separate corpus, it's likely that the accuracy would be very low since it modeled noise or other uninformative features from that particular split (i.e., it overfit). Another important concept is baseline accuracy. This represents the minimum score to "beat" when using your classifier. Say there are 3,000 documents consisting of three classes, each with 1,000 documents. In this case, random guessing would give you about 33% accuracy, since you'd be correct approximately 1 3 of the time. Your classifier would have to do better than 33% accuracy in order to make it useful! In another example, consider the 3,000 documents and three classes, but with an uneven class distribution: one class has 2,000 documents and the other two classes have 500 each. In this case, the baseline accuracy is 66%, since picking the majority class label will result in correct predictions 2 3 of the time. Thus, it's important to take class imbalances into consideration when evaluating a classifier. A confusion matrix is a way to examine a classifier's performance at a per-label level. Consider Figure 15.5, the output from running META on a three-class classification problem to determine the native language of the author of English text. Each (row, column) index in the table shows the fraction of times that row was classified as column. Therefore, the rows all sum to one. The diagonal represents the true positive rate, and hopefully most of the probability mass lies here, indicating a good classifier. Based on the matrix, we see that predicting Chinese was 80.2% accurate, with native English and Japanese as 80.7% and 99.1%, respectively. This shows that while English and Chinese had relatively the same difficulty, Japanese was very easy for the classifier to distinguish. We also see that if the classifier was wrong in a prediction on a Chinese or English true label, it almost always chose Japanese as the answer. Based on the matrix, the classifier seems to default to the label "Japanese". The table doesn't tell us why this is, but we can make some hypotheses based on our dataset. Based on this observation, we may want to tweak our classifier's parameters or do a more thorough feature analysis. 