The numerator p(X = x , Y = y) is the probability of exactly the configuration we're looking for (i.e., both x and y have been observed), which is normalized by p(Y = y), the probability that the condition is true (i.e., y has been observed). Using this knowledge, we can calculate p(x C = green | x S = circle): p(x C = green | x S = circle) = p(x C = green, x S = circle) p(x S = circle) = 1/6 1/2 = 1 3 . One other important concept to mention is independence. In the previous examples, the two random variables were dependent, meaning the value of one will influence the value of the other. Consider another situation where we have c 1 , c 2 ∼ θ C . That is, we draw two colors from the color distribution. Does the knowledge of c 1 inform the probability of c 2 ? No, since each draw is done "independently" of the other. In the case where two random variables X and Y are independent, p(X, Y ) = p(X)p(Y ). Can you see why this is the case? Both conditional and joint probabilities can be used to answer interesting questions about text. For example, given a document, what is the probability of observing the word information and retrieval in the same sentence? What is the probability of observing retrieval if we know information has occurred? 