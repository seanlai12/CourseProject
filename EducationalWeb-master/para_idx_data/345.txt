If we assume that most of the elements in an opinion representation are already known, then our only task may be sentiment classification. That is, suppose we know the opinion holder and the opinion target, and also know the content and 394 Chapter 18 Opinion Mining and Sentiment Analysis the context of the opinion. The only component remaining is to decide the opinion sentiment of the review. Sentiment classification can be defined more specifically as follows: the input is an opinionated text object and the output is typically a sentiment label (or a sentiment tag) that can be defined in two ways. One is polarity analysis, where we have categories such as positive, negative, or neutral. The other is emotion analysis that can go beyond polarity to characterize the precise feeling of the opinion holder. In the case of polarity analysis, we sometimes also have numerical ratings as you often see in some reviews on the Web. A rating of five might denote the most positive, and one may be the most negative, for example. In emotion analysis there are also different ways to design the categories. Some typical categories are happy, sad, fearful, angry, surprised, and disgusted. Thus, the task is essentially a classification task, or categorization task, as we've seen before. If we simply apply default classification techniques, the accuracy may not be good since sentiment classification requires some improvement over regular text categorization techniques. In particular, it needs two kind of improvements. One is to use more sophisticated features that may be more appropriate for sentiment tagging. The other is to consider the order of these categories, especially in polarity analysis since there is a clear order among the choices. For example, we could use ordinal regression to predict a value within some range. We'll discuss this idea in the next section. For now, let's talk about some features that are often very useful for text categorization and text mining in general, but also especially needed for sentiment analysis. The simplest feature is character n-grams, i.e., sequences of n adjacent characters treated as a unit. This is a very general and robust way to represent text data since we can use this method for any language. This is also robust to spelling errors or recognition errors; if you misspell a word by one character, this representation still allows you to match the word as well as when it occurs in the text correctly. Of course, such a representation would not be as discriminating as words. Next, we have word n-grams, a sequence of words as opposed to characters. We can have a mix of these with different n-values. Unigrams are often very effective for text processing tasks; it's mostly because words are the basic unit of information used by humans for communication. However, unigram words may not be sufficient for a task like sentiment analysis. For example, we might see a sentence, "It's not good" or "It's not as good as something else." In such a case, if we just take the feature good, that would suggest a positive text sample. Clearly, this would not be accurate. If we take a bigram (n = 2) representation, the bigram not good would appear, making our representation more accurate. Thus, longer n-grams are generally more discriminative. However, long n-grams may cause overfitting because 18.1 Sentiment Classification 395 they create very unique features that machine learning programs associate as being highly correlated with a particular class label when in reality they are not. For example, if a 7-gram phrase appears only in a positive training document, that 7gram would always be associated with positive sentiment. In reality, though, the 7-gram just happened to occur with the positive document and no others because it was so rare. We can consider n-grams of part-of-speech tags. A bigram feature could be an adjective followed by a noun. We can mix n-grams of words and n-grams of POS tags. For example, the word great might be followed by a noun, and this could become a feature-a hybrid feature-that could be useful for sentiment analysis. Next, we can have word classes. These classes can be syntactic like POS tags, or could be semantic by representing concepts in a thesaurus or ontology like Word-Net [Princeton University 2010]. Or, they can be recognized name entities (like people or place), and these categories can be used to enrich the representation as additional features. We also can learn word clusters since we've talked about mining associations of words in Chapter 13. We can have clusters of paradigmatically related words or syntagmatically related words, and these clusters can be features to supplement the base word representation. Furthermore, we can have a frequent pattern syntax which represents a frequent word set; these are words that do not necessarily occur next to each other but often occur in the same context. We'll also have locations where the words may occur more closely together, and such patterns provide more discriminative features than words. They may generalize better than just regular n-grams because they are frequent, meaning they are expected to occur in testing data, although they might still face the problem of overfitting as the features become more complex. This is a problem in general, and the same is true for parse tree-based features, e.g., frequent subtrees. Those are even more discriminating, but they're also more likely to cause overfitting. In general, pattern discovery algorithms are very useful for feature construction because they allow us to search a large space of possible features that are more complex than words, and natural language processing is very important to help us derive complex features that can enrich text representations. As we've mentioned in Chapter 15, feature design greatly affects categorization accuracy and is arguably the most important part of any machine learning application. It would be most effective if you can combine machine learning, error analysis, and specific domain knowledge when designing features. First, we want to use domain knowledge, that is, a specialized understanding of the problem. With this, we can design a basic feature space with many possible features for the machine 396 Chapter 18 Opinion Mining and Sentiment Analysis learning program to work on. Machine learning methods can be applied to select the most effective features or to even construct new features. These features can then be further analyzed by humans through error analysis, using evaluation techniques we discuss in this book. We can look at categorization errors and further analyze what features can help us recover from those errors or what features cause overfitting. This can lead into feature validation that will cause a revision in the feature set. These steps are then iterated until a desired accuracy is achieved. In conclusion, a main challenge in designing features is to optimize a tradeoff between exhaustivity and specificity. This tradeoff turns out to be very difficult. Exhaustivity means we want the features to have high coverage on many documents. In that sense, we want the features to be frequent. Specificity requires the features to be discriminative, so naturally the features tend to be less frequent. Clearly, this causes a tradeoff between frequent versus infrequent features. Particularly in our case of sentiment analysis, feature engineering is a critical task. 