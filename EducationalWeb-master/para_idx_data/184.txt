In this section, we discuss using machine learning to combine many different features into a single ranking function to optimize search results. Previously, we've discussed a number of ways to rank documents. We talked about some retrieval models like BM25 or query likelihood; these can generate a content-based score for matching document text with a query. We also talked about the link-based approaches like PageRank that can give additional scores to help us improve ranking. The question now is how can we combine all these features (and potentially many other features) to do ranking? This will be very useful for ranking web pages not only just to improve accuracy, but also to improve the robustness of the ranking function so that's it not easy for a spammer to just perturb one or a few features to promote a page. The general idea of learning to rank is to use machine learning to combine these features, optimizing the weight on different features to generate the best ranking function. We assume that given a query-document pair (q , d), we can define a number of features. These features don't necessarily have to be contentbased features. They could be a score of the document with respect to the query according to a retrieval function such as BM25, query likelihood, pivoted length normalization, PL2, etc. There also can be a link-based score like PageRank or HITS, or an application of retrieval models to the anchor text of the page, which 10.4 Learning to Rank 209 are the descriptions of links that point to d. These can all be clues about whether this document is relevant or not to the query. We can even include a feature such as whether the URL has a tilde because this might indicate a home page. The question is, of course, how can we combine these features into a single score? In this approach, we simply hypothesize that the probability that this document is relevant to this query is a function of all these features. We hypothesize that the probability of relevance is related to these features through a particular function that has some parameters. These parameters control the influence of different features on the final relevance. This is, of course, just an assumption. Whether this assumption really makes sense is still an open question. Naturally, the next question is how to estimate those parameters. How do we know which features should have high weight and which features should have low weight? This is a task of training or learning. In this approach, we use training data. This is data that have been judged by users, so we already know the relevance judgments. We know which documents should be highly ranked for which queries, and this information can be based on real judgments by users or can be approximated by just using clickthrough information as we discussed in Chapter 7. We will try to optimize our search engine's retrieval accuracy (using, e.g., MAP or NDCG) on the training data by adjusting these parameters. The training data would look like a table of tuples. Each tuple has three elements: the query, the document, and the judgment. Let's take a look at a specific method that's based on logistic regression: (10.5) This is one of many different methods, and actually one of the simpler ones. In this approach, we simply assume the relevance of a document with respect to the query is related to a linear combination of all the features. Here we have X i to denote the i th feature value, and we can have as many features as we would like. We assume that these features can be combined in a linear manner. The weight of feature X i is controlled by a parameter β i . A larger β i would mean the feature would have a higher weight and it would contribute more to the scoring function. The specific form of the function also gives the following probability of relevance: (10.6) 210 Chapter 10 Web Search We know that the probability of relevance is within the range [0, 1] and we assume that the scoring function is a transformed form of the linear combination of features. We could have had a scoring function directly based on the linear combination of β and X, but then the value of this linear combination could easily go beyond 1. Thus the reason why we use the logistic regression instead of linear regression is to map this combination onto the range [0, 1]. This allows us to connect the probability of relevance (which is between 0 and 1) to a linear combination of arbitrary coefficients. If we rewrite this combination of weights into a probability function, we will get the predicted score. If this combination of features and weights gives us a high value, then the document is more likely relevant. This isn't necessarily the best hypothesis, but it is a simple way to connect these features with the probability of relevance. The next task is to see how we estimate the parameters so that the function can truly be applied; that is, we need to estimate the β values. Let's take a look at a simple example shown in Figure 10.9. In this example, we have three features. One is the BM25 score of the document for the query. One is the PageRank score of the document, which might or might not depend on the query. We might also have a topic-sensitive PageRank score that would depend on the query. Lastly, we have a BM25 score on the anchor text of the document. These are then the three feature values for a particular (document, query) pair. In this case the document is d 1 and the judgment says that it's relevant. The document d 2 is another training instance with different feature values, but in this case it's non-relevant. Of course, this is an overlysimplified example where we just have two instances, but it's sufficient to illustrate the point. We use the maximum likelihood estimator to estimate the parameters. That is, we're going to predict the relevance status of the document based on the feature values. The likelihood of observing the relevance status of these two documents using our model is 10.4 Learning to Rank 211 3 . We hypothesize that the probability of relevance is related to the features in this way. We're going to see for what values of β we can predict the relevance effectively. The expression for d 1 should give a higher value than the expression for d 2 ; in fact, we hope d 1 's value is close to one since it's a relevant document. Let's see how this can be mathematically expressed. It's similar to expressing the probability of a document, only we are not talking about the probability of words, but the probability of relevance. We need to plug in the X values. The β values are still unknown, but this expression gives us the probability that this document is relevant if we assume such a model. We want to maximize this probability for d 1 since this is a relevant document. For the second document, we want to predict the probability that the document is non-relevant. This means we have to compute 1 minus the probability of relevance. That's the reasoning behind this whole expression then; it's our probability of predicting these two relevance values. The whole equation is our probability of observing a R = 1 and R = 0 for d 1 and d 2 respectively. Our goal is then to adjust the β values to make the whole expression reach its maximum value. In other words, we will look at the function and choose β values to make this expression as large as possible. After we learn the regression parameters, we can use this expression for any new query and new document once we have their features. This formula is then applied to generate a ranking score for a particular query. There are many more advanced learning algorithms than the regression-based reproaches. They generally attempt to theoretically optimize a retrieval measure such as MAP or NDCG. Note that the optimization objective we just discussed is not directly related to a retrieval measure. By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. One can imagine that while our prediction may not be too bad, the ranking can be wrong. We might have a larger probability of relevance for d 2 than d 1 . So, that won't be good from a retrieval perspective, even though by likelihood the function is not bad. More advanced approaches will try to correct this problem. Of course, then the challenge is that the optimization problem will be harder to solve. In contrast, we might have another case where we predicted probabilities of relevance around 0.9 for non-relevant documents. Even though the predicted score is very high, as long as the truly relevant documents receive scores that are greater than 0.9, the ranking will still be acceptable to a user. These learning to rank approaches are actually quite general. They can be applied to many other ranking problems aside from retrieval problems. For example, recommender systems, computational advertising, summarization, and many other relevant applications can all be solved using this approach. To summarize, we talked about using machine learning to combine features to predict a ranking result. Actually, the use of machine learning in information retrieval began many decades ago. Rocchio feedback, discussed in Chapter 7, was a machine learning approach applied to learn the optimal feedback. Many algorithms are driven by the availability of massive amounts of training data in the form of clickthroughs. This data provides much useful knowledge about relevance, and so machine learning methods are applied to leverage this. The need for machine learning is also driven by the desire to combine many different feature types to predict an accurate ranking. web search especially drives this need since there are more features available on the web that can be taken advantage of for search. Using many different features also increases the robustness of the scoring function, which is useful in combating spam. Modern search engines all use some kind of machine learning techniques to combine many features to optimize ranking, and this is a major feature of current engines such as Google and Bing. 