Caching While we designed our inverted index structure to be very efficient, we still have the issue of disk latency. For this reason, it is very common for a real-world search engine to also employ some sort of caching structure stored in memory for postings data objects. In this section, we'll overview two different caching strategies. The basic idea of a cache is to make frequently accessed objects fast to acquire. To accomplish this, we attempt to keep the frequently accessed objects in memory so we don't need to seek to the disk. In our case, the objects we access are postings lists. Due to Zipf's law [Zipf 1949  postings lists (keyed by term ID) will be accessed the majority of the time. But how do we know which terms to store? Even if we knew which terms are most queried, how do we set a cutoff for memory consumption? The two cache designs we describe address both these issues. That is: (1) the most-frequently used items are fast to access, and (2) we can set a maximum size for our cache so it doesn't get too large. 