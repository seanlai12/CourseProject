This general approach relies on two specific technical components: a topic model and a causality measure. The former has already been introduced earlier in the book, so we briefly discuss the latter. There are various ways to measure causality between two time series. The simplest measure is Pearson correlation. Pearson correlation is one of the most common methods used to measure the correlation between two variables. It gives us a correlation value in the range of [−1, +1], and the sign of the output value indicates the orientation of the correlation (which we will exploit to quantify the impact in the case of a causal relation). We can also measure the significance of the correlation value. If used directly, the basic Pearson correlation would have zero lag because it compares values on the same time stamp. However, we can compute a lagged correlation by shifting one of the input time series variables by the lag and measuring the Pearson correlation after the shift. A more common method for causality test on time series data is the Granger Test. The Granger test performs a statistical significance test with different time lags by using autoregression to see if one time series has a causal relationship with another series. Formally, let y t and x t be two time series to be tested, where we hope to see if x t has Granger causality for y t with a maximum p time lag. The basic formula for the Granger test is the following: y t = a 0 + a 1 y t−1 + . . . + a p y t−p + b 1 x t−1 + . . . + b p x t−p . (19.1) We then perform an F -test to evaluate if retaining or removing the lagged x terms would make a statistically significant difference in fitting the data. Because the Granger test is essentially an F -test, it naturally gives us a significance value of causality. We can estimate the impact of x on y based on the coefficients of the x i terms; for example, we can take the average of the x i term coefficients, p i=1 b i p , use it as an "impact value." The impact values can be used to assign weights to the selected seed words so that highly correlated words would have a higher probability in the prior that we pass to topic modeling. We now show some sample results generated by this approach to illustrate the applications that it can potentially support. First, we show a sample of causal topics discovered from a news data set when using two different stock time series as context in Figure 19.17. The text data set here is the New York Times news articles in the time period of June 2000 through December 2011. The time series used is the stock prices of two companies, American Airlines (AAMRQ) and Apple Inc. (AAPL). If we are to use a topic model to mine the news data set to discover topics, we would obtain topics that are neutral to both American Airlines and Apple.  We would like to see whether we can discover biased topics toward either American Airlines or Apple when we use their corresponding time series as context. The results here show that the iterative causal topic modeling approach indeed generates topics that are more tuned toward each stock time series. Specifically, on the left column, we see topics highly relevant to American Airlines, including, e.g., a topic about airport and airlines, and another about terrorism (the topic is relevant because of the September 11th terrorist attack in 2001, which negatively impacted American Airlines), though the correlation of other topics with American Airlines stock is not obvious. In contrast, on the right column, we see topics that are clearly more related to AAPL, including a topic about computer technology, and another about the Web and Internet. While not all topics can be easily interpreted, it is clear the use of the time series as context has impacted the topics discovered and enabled discovery of specific topics that are intuitively related to the time series. These topics can serve as entry points for analysts to further look into the details for any potential causal relations between topics and time series. These results also clearly suggest the important role that humans must play in any real application of causal topic discovery, but these topics can be immediately used as features in a predictive model (for predicting stock prices). It is reasonable to assume that some of these topics would make better features than simple features such as n-grams or ordinary topics discovered from the collection without considering the time series context. 