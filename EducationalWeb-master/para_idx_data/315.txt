When we use a generative model to solve a problem, we start with thinking about what kind of data we need to model and from what perspective. Our data would "look" differently if we use a different perspective. For example, we may view a document simply as a set of words without considering the frequencies of words, which would lead to a bit vector representation as we discussed in the context of the vector space retrieval model. Such a representation would need a different generative model than if we view the document as a bag of words where we care about the different frequencies of words. In topic analysis, the frequencies of words can help distinguish subtle semantic variations, so we generally should retain the word frequencies. Once we decide on a perspective to view the data, we will design a specific model for generating the data from the desired perspective, i.e., model the data based on the representation of the data reflecting the desired perspective. The choice of a particular model partly depends on our domain knowledge about the data and partly depends on what kind of knowledge we would like to discover. The target knowledge would determine what parameters we would include in the model since we want our parameters to denote the knowledge interesting to us (after we estimate the values of these parameters). Here we are interested in discovering a topic represented as a word distribution, so a natural choice of model would be a unigram language model, as in Section 3.4. After we specify the model, we can formally write down the likelihood function, i.e., the probability of the data given the assumed model. This is generally a function of the (unknown) parameters, and the value of the function would vary according to the values of the parameters. Thus, we can attempt to find the parameter values that would maximize the value of this function (the likelihood) given data from the model. Such a way of obtaining parameters is called the Maximum Likelihood Estimate (MLE) as we've discussed previously. Sometimes, it is desirable to also incorporate some additional knowledge (a prior belief) about the parameters that we may have about a particular application. We can do this by using Bayesian estimation of parameters, which seeks a compromise of maximizing the probability of the observed data (maximum likelihood) and being consistent with the prior belief that we impose. In any case, once we have a generative model, we would be able to fit such a model to our data and obtain the parameter values that can best explain the data. These parameter values can then be taken as the output of our mining process. Let's follow these steps to design the simplest topic model for discovering a topic from one document; we will examine many more complicated cases later. The model is shown in Figure 17.10 where we see that we have decided to view a 342 Chapter 17 Topic Analysis Figure 17.10 Unigram language model for discovering one topic. 