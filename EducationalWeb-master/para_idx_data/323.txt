In order to understand some interesting behaviors of mixture models, we take a look at a very simple case, as illustrated in Figure 17.19. Although the example is very simple, the observed patterns here actually are applicable to mixture models in general. Let's assume that the probability of choosing each of the two models is exactly the same. That is, we will flip a fair coin to decide which model to use. Furthermore, we will assume that there are precisely two words in our vocabulary: the and text. Obviously this is a naive oversimplification of the actual text, but it's useful to examine the behavior in such a special case. We further assume that the background model gives probability of 0.9 to the and 0.1 to text. We can write down the likelihood function in such a case as shown in Figure 17.19. The probability of the two-word document is simply the product of the probability of each word, which is itself a sum of the probability of generating the word with each of the two distributions. Since we already know all the parameters except for the θ d , the likelihood function has just two unknown variables, p(the | θ d ) and p(text | θ d ). Our goal of computing the maximum likelihood estimate is to find out for what values of these two probabilities the likelihood function would reach its maximum. Now the problem has become one to optimize a very simple expression with two variables as shown in Figure 17.20. Note that the two probabilities must sum to one, so we have to respect this constraint. If there were no constraint, we would have been able to set both probabilities to their maximum value (which would be 1.0) to maximize the likelihood expression. However, we can't do this because we can't give both words a probability of one, or otherwise they would sum to 2.0. How should we allocate the probability between the two words? As we shift probability mass from one word to the other, it would clearly affect the value of the likelihood function. Imagine we start with an even allocation between the and text, i.e., each would have a probability of 0.5. We can then imagine we could gradually move some probability mass from the to text or vice versa. How would such a change affect the likelihood function value? If you examine the formula carefully, you might intuitively feel that we want to set the probability of text to be somewhat larger than the, and this intuition can indeed be supported by a mathematical fact: when the sum of two variables is a constant, their product would reach the maximum when the two variables have the same value. In our case, the sum of the two terms in the product is Plugging in the constraint p(text | θ d ) + p(the | θ d ) = 1, we can easily obtain the solution p(text | θ d ) = 0.9 and p(the | θ d ) = 0.1. Therefore, the probability of text is indeed much larger than the probability of the, effectively factoring out this common word. Note that this is not the case when we have just one distribution where the has a much higher probability than text. The effect of reducing the estimated probability of the is clearly due to the use of the background model, which assigned very high probability to the and low probability to text. Looking into the process of reaching this solution, we see that the reason why text has a higher probability than the is because its corresponding probability by the background model p(text | θ B ) is smaller than that of the; had the background model given the a smaller probability than text, our solution would give the a 356 Chapter 17 Topic Analysis higher probability than text in order to ensure that the overall probability given by the two models working together is the same for text and the. Thus, the ML estimate tends to give a word a higher probability if the background model gives it a smaller probability, or more generally, if one distribution has given word w 1 a higher probability than w 2 , then the other distribution would give word w 2 a higher probability than word w 1 so that the combined probability of w 1 given by the two distributions working together would be the same as that of w 2 . In other words, the two distributions tend to give high probabilities to different words as if they try to avoid giving the high probability to the same word. In such a two-component mixture model, we see that the two distributions will be collaborating to maximize the probability of the observed data, but they are also competing on the words in the sense that they would tend to "bet" high probabilities on different words to gain advantages in this competition. In order to make their combined probability equal (so as to maximize the product in the likelihood function), the probability assigned by θ d must be higher for a word that has a smaller probability given by the background model θ B . The general behavior we have observed here about a mixture model is that if one distribution assigns a higher probability to one word than another, the other distribution would tend to do the opposite; it would discourage other distributions to do the same. This also means that by using a background model that is fixed to assigning high probabilities to common (stop) words, we can indeed encourage the unknown topical word distribution to assign smaller probabilities for such common words so as to put more probability mass on those content words that cannot be explained well by the background model. Let's look at another behavior of the mixture model in Figure 17.21 by examining the response of the estimated probabilities to the data frequencies. In Figure 17.21, we have shown a scenario where we've added more words to the document, specifically, more the's to the document. What would happen to the estimated p(w | θ) if we keep adding more and more the's to the document? As we add more words to the document, we would need to multiply the likelihood function by additional terms to account for the additional occurrences. In this case, since all the additional terms are the, we simply need to multiply by the term representing the probability of the. This obviously changes the likelihood function, and thus also the solution of the ML estimation. How exactly would the additional terms accounting for multiple occurrences of the change the ML estimate? The solution we derived earlier, p(text | θ d ) = 0.9, is no longer optimal. How should we modify this solution to make it optimal for the new likelihood function? One way to address this question is to take away some probability mass from one word and add the probability mass to the other word, which would ensure that they sum to one. The question is, of course, which word to have a reduced probability and which word to have a larger probability. Should we make the probability of the larger or that of text larger? If you look at the formula for a moment, you might notice that the new likelihood function (which is our objective function for optimization) is influenced more by the than text, so any reduction of probability of the would cause more decrease of the likelihood than the reduction of probability of text. Indeed, it would make sense to take away some probability from text, which only affects one term, and add the extra probability to the, which would benefit more terms in the likelihood function (since the occurred many times), thus generating an overall effect of increasing the value of the likelihood function. In other words, because the is repeated many times in the likelihood function, if we increase its probability a little bit, it will have substantial positive impact on the likelihood function, whereas a slight decrease of probability of text will have a relatively small negative impact because it occurred just once. The analysis above reveals another behavior of the ML estimate of a mixture model: high frequency words in the observed text would tend to have high probabilities in all the distributions. Such a behavior should not be surprising at all because-after all-we are maximizing the likelihood of the data, so the more a 358 Chapter 17 Topic Analysis word occurs, the higher its overall probability should be. This is, in fact, a very general phenomenon of all the maximum likelihood estimators. In our special case, if a word occurs more frequently in the observed text data, it would also encourage the unknown distribution θ d to assign a somewhat higher probability to this word. We can also use this example to examine the impact of p(θ B ), the probability of choosing the background model. We've been so far assuming that each model is equally likely, i.e., p(θ B ) = 0.5. But, you can again look at this likelihood function shown in Figure 17.21 and try to picture what would happen to the likelihood function if we increase the probability of choosing the background model. It is not hard to notice that if p(θ B ) > 0.5 is set to a very large value, then all the terms representing the probability of the would be even larger because the background has a very high probability for the (0.9), and the coefficient in front of 0.9, which was 0.5, would be even larger. The consequence is that it is now less important for θ d to increase the probability mass for the even when we add more and more occurrences of the to the document. This is because the overall probability of the is already very large (due to the large p(θ B ) and large p(the | θ B )), and the impact of increasing p(the | θ d ) is regulated by the coefficient p(θ d ) which would be small if we make p(θ B ) very large. It would be more beneficial for θ d to ensure p(text | θ d ) to be high since text does not get any help from the background model, and it must rely on θ d to assign a high probability. While high frequency words tend to get higher probabilities in the estimated p(w | θ d ), the degree of increase of probability due to the increased counts of a word observed in the document is regularized by p(θ d ) (or equivalently p(θ B )). The smaller p(θ d ) is, the less important for θ d to respond to the increase of counts of a word in the data. In general, the more likely a component is being chosen in a mixture model, the more important it is for the component model to assign higher probability values to these frequent words. To summarize, we discussed the mixture model, the estimation problem of the mixture model, and some general behaviors of the maximum likelihood estimator. First, every component model attempts to assign high probabilities to high frequent words in the data so as to collaboratively maximize the likelihood. Second, different component models tend to bet high probabilities on different words in order to avoid the "competition," or waste of probability. This would allow them to collaborate more efficiently to maximize the likelihood. Third, the probability of choosing each component regulates the collaboration and the competition between component models. It would allow some component models to respond more to the change of frequency of a word in the data. We also discussed the special case of fixing one component to a background word distribution, which can be estimated based on a large collection of English documents using the simplest single 