Term Cosine similarity to "france" vectors corresponding to the two words, w 1 and w 2 . With such a model, we can then try to find the vector representation for all the words that would maximize the probability of using each word to predict all other words in a small window of words surrounding the word. In effect, we would want the vectors representing two semantically related words, which tend to co-occur together in a window, to be more similar so as to generate a higher value when taking their dot product. Google's implementation of skip-gram, called word2vec [Mikolov et al. 2013] is perhaps the most well-known software in this area. They showed that performing vector addition on terms in vector space yielded interesting results. For example, adding the vectors for Germany and capital resulted in a vector very close to the vector Berlin. Figure 14.8 shows example output from using this tool. Although similar results can also be obtained by using heuristic paradigmatic relation discovery (e.g., using the methods we described in Chapter 13) and the n-gram class language model, word embedding provides a very promising new alternative that can potentially open up many interesting new applications of text mining due to its flexibility in formulating the objective functions to be optimized and the fact that the vector representation is systematically learned through optimizing an explicitly defined objective function. One disadvantage of word embedding, at least in its current form, is that the elements in the vector representation of a word are not meaningful and cannot be easily interpreted intuitively. As a result, the utility of these word vectors has so far been mostly limited to computation of word similarities, which can also obtained by using many other methods. 