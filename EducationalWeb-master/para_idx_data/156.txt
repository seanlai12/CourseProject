The Cranfield evaluation methodology was developed in the 1960s and is a strategy for laboratory testing of system components. It's actually a methodology that has been very useful not only for search engine evaluation, but also for evaluating virtually all kinds of empirical tasks. For example, in image processing or other fields where the problem is empirically defined we typically would need to use a method such as this. The basic idea of this approach is to build reusable test collections and define measures using these collections. Once such a test collection is built, it can be used again and again to test different algorithms or ideas. Using these test collections, we will define measures that allow us to quantify the performance of a system or algorithm. The assembled test collection of documents is similar to a real document collection in a search application. We can also have a sample set of queries or topics that simulate the user's information need. Then, we need to have relevance judg-9.1 Introduction 169 ments. These are judgments of which documents should be returned for which queries. Ideally, they have to be made by users who formulated the queries because those are the people that know exactly what the documents (search results) would be used for. Finally, we have to have measures to quantify how well a system's result matches the ideal ranked list that would be constructed based on users' relevance judgements. This methodology is very useful for evaluating retrieval algorithms because the test set can be reused many times. It also provides a fair comparison for all the methods, since the evaluation is exactly the same for each one. That is, we have the same criteria, same corpus, and same relevance judgements to compare the different algorithms. This allows us to compare a new algorithm with an old algorithm that was invented many years ago by using the same approach. In Figure 9.1, we illustrate how the Cranfield evaluation methodology works. As mentioned, we need a set of queries that are shown here. We have Q 1 , Q 2 , and so on. We also need the document collection, D 1 , D 2 , . . ., and on the far right side of the figure, we have the relevance judgments which are plus or minus annotations on each document specifying whether it is relevant (plus) or not relevant (minus). Essentially, these are binary judgments of documents with respect to a specific query since there are only two levels of relevance. For example, D 1 and D 2 are judged as being relevant to Q 1 . D 3 is judged as non-relevant with respect to Q 1 . Which is better? How to quantify? Relevance judgments 