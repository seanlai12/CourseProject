large loss is incurred if we deliver a bad one. The system in this case would be very reluctant to deliver many documents, and has to be absolutely sure that it's a relevant one. In short, the utility function has to be designed based on a specific application preference, potentially different for different users. The three basic components in content-based filtering are the following. Initialization module. Gets the system started based only on a very limited text description, or very few examples, from the user. Decision module. Given a text document and a profile description of the user, decide whether the document should be delivered or not. Learning module. Learn from limited user relevance judgments on the delivered documents. (If we don't deliver a document to the user, we'd never know whether the user likes it or not.) All these modules would have to be optimized to maximize the utility function U. To solve these problems, we will talk about how to extend a retrieval system for information filtering. First, we can reuse retrieval techniques to do scoring; we know how to score documents against queries and measure the similarity between a profile text description and a document. We can use a score threshold θ for the filtering decision. If score(d) > θ, we say document d is relevant and we are going to deliver it to the user. Of course, we still need to learn from the history, and for this we can use the traditional feedback techniques to learn to improve scoring, such as Rocchio. What we don't know how to do yet is learn how to set θ. We need to set it initially and then we have to learn how to update it over time as more documents are delivered to the user and we have more information. Figure 11.3 shows what the system might look like if we generalized a vectorspace model for filtering problems. The document vector could be fed into a scoring module, which already exists in a search engine that implements the vector-space model, where the profile will be treated as a query. The profile vector can be matched with the document vector to generate the score. This score will be fed into a thresholding module that would say yes or no depending on the current value of θ . The evaluation would be based on the utility for the filtering results. If it says yes, the document will be sent to the user, and then the user could give some feedback. The feedback information would be used to both adjust the threshold and change the vector representation. In this sense, vector learning is essentially the same as query modification or feedback in search. The threshold learning is a new component that we need to talk a little bit more about. No judgments are available for these documents Figure 11.4 Information available to the content-based recommender system. There are some interesting challenges in threshold learning. Figure 11.4 depicts the type of data that you can collect in the filtering system. We have the scores and the status of relevance. The first document has a score 36.5 and it's relevant. The second one is not relevant. We have many documents for which we don't know the status, since their scores are less than θ and are not shown to the user for judging. Thus, the judged documents are not a random sample; it's biased or censored data, which creates some difficulty for learning an optimal θ. Secondly, there are in general very little labeled data and very few relevant data, which make it challenging 11.1 Content-based Recommendation 227 for machine learning approaches, which require a large amount of training data. In the extreme case at the beginning, we don't even have any labeled data at all, but the system still has to make a decision. This issue is called the exploration-exploitation tradeoff. This means we want to explore the document space to see if the user might be interested in the documents that we have not yet labeled, but we don't want to show the user too many nonrelevant documents or they will be unsatisfied with the system. So how do we do that? We could lower the threshold a little bit and deliver some near misses to the user to see what their response to this extra document is. This is a tradeoff because on one hand, you want to explore, but on the other hand, you don't want to explore too much since you would over-deliver non-relevant information. Exploitation means you would take advantage of the information learned about the user. Say you know the user is interested in this particular topic, so you don't want to deviate that much. However, if you don't deviate at all, then you don't explore at all, and you might miss the opportunity to learn another interest of the user. Clearly, this is a dilemma and a difficult problem to solve. Why don't we just use the empirical utility optimization strategy to optimize U? The problem is that this strategy is used to optimize the threshold based on historical data. That is, you can compute the utility on the training data for each candidate score threshold, keeping track of the highest utility observed given a θ . This doesn't account for the exploration that we just mentioned, and there is also the difficulty of biased training samples. In general, we can only get an upper bound for the true optimal threshold because the threshold might be lower than we found; it's possible that some of the discarded items might actually be interesting to the user. So how do we solve this problem? We can lower the threshold to explore a little bit. We'll discuss one particular approach called beta-gamma threshold learning [Zhai et al. 1998]. The basic idea of the beta-gamma threshold learning algorithm is as follows. Given a ranked list of all the documents in the training database sorted by their scores on the x-axis, their relevance, and a specific utility U, we can plot the utility value at each different cutoff position θ. Each cutoff position corresponds to a score threshold. Figure 11.5 shows this configuration and how a choice of α determines a cutoff point between the optimal and the zero utility points, and how β and γ help us to adjust α dynamically according to the number of judged examples in the training database. The optimal point θ opt is the point when we would achieve the maximum utility if we had chosen this threshold. The θ zero threshold is the zero utility threshold. Between these two θ values give us a safe point to explore the potential cutoff values. As one can see from the formula, the threshold will be 228 Chapter 11 Recommender Systems Encourage exploration up to θ zero The more examples, the less exploration (closer to θ optimal ) Cutoff position (descending order of doc sources) Figure 11.5 Beta-gamma threshold learning to set the optimal value of θ . just the interpolation of the zero utility threshold and the optimal threshold via the interpolation parameter α. Now the question is how we should set α and deviate from the optimal utility point. This can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore only up to the θ zero point (which is still a safe point), but not necessarily reach all the way to it. Rather, we're going to use other parameters to further define α's value given some additional information. The β parameter controls the deviation from θ opt , which can be based on our previously observed documents (i.e., the training data). What's more interesting is the γ parameter which controls the influence of the number of examples in the training data set, N . As N becomes greater, it encourages less exploration. In other words, when N is very small, the algorithm will try to explore more, meaning that if we have seen only a few examples, we're not sure whether we have exhausted the space of interest. But, as we observe many data points from the user, we feel that we probably don't have to explore as much. This gives us a dynamic strategy for exploration: the more examples we have seen, the less exploration we are going to do, so the threshold will be closer to θ opt . This approach has worked well in some empirical studies, particularly on the TREC filtering tasks. It's also convenient that it welcomes any arbitrary utility function with an appropriate lower bound. It explicitly addresses the explorationexploration tradeoff, and uses θ zero as a safeguard. That is, we're never going to explore further than the zero utility point. If you take the analogy of gambling, you 11.2 Collaborative Filtering 229 don't want to risk losing money, so it's a "safe" strategy in that sense. The problem is, of course, that this approach is purely heuristic and the zero utility lower bound is often too conservative in practice. There are more advanced machine learning projects that have been proposed for solving these problems; it is actually a very active research area. 