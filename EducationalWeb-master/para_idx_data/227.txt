In order to compute mutual information, we often use a different form of mutual information that we can mathematically rewrite as which is in the context of KL-divergence (see Appendix C). The numerator of the fraction is the observed joint distribution and the denominator is the expected joint distribution if they are independent. KL-divergence quantifies the difference between these two distributions. That is, it measures the divergence of the actual joint distribution from the expected distribution under an independence assumption. The larger the divergence is, the higher the mutual information would be. Continuing to inspect this formulation of mutual information, we see that it is also summed over many combinations of different values of the two random variables. Inside the sum, we are doing a comparison between the two joint distributions. Again, the numerator has the actually observed joint distribution of the two random variables while the denominator can be interpreted as the expected joint distribution of the two random variables. If the two random variables are independent, their joint distribution is equal to the product of the two probabilities, so this comparison will tell us whether the two variables are indeed independent. If they are indeed independent, then we would expect that the numerator and denominator are the same. If the numerator is different from the denominator, that would mean the two variables are not independent and their difference can measure the strength of their association. The sum is simply to take all of the combinations of the values of these two random variables into consideration. In our case, each random variable can choose one of the two values, zero or one, so we have four combinations. If we look at this form of mutual information, it shows that the mutual information measures the divergence of the actual joint distribution from the expected distribution under the independence assumption. The larger this divergence is, the higher the mutual information would be. Let's further look at exactly what probabilities are involved in the mutual information formula displayed in Figure 13.11. First, we have to calculate the probabilities corresponding to the presence or absence of each word. For w 1 , we have two probabilities shown here. They should sum to one, because a word can either be present or absent in the segment, and similarly for the second word, we also have two probabilities representing presence 