where c(w, D) is the count of word w in D and |D| is the length of D, or total number of words in D. Such an estimate is optimal in the sense that it would maximize the probability of the observed data, but whether it is really optimal for an application is still questionable. For example, if our goal is to estimate the language model in the mind of an author of a research article, and we use the maximum likelihood estimator to estimate the model based only on the abstract of a paper, then it is clearly nonoptimal since the estimated model would assign zero probability to any unseen words in the abstract, which would make the whole article have a zero probability unless it only uses words in the abstract. Note that, in general, the maximum likelihood estimate would assign zero probability to any unseen token or event in the observed data; this is so because assigning a non-zero probability to such a token or event would take away probability mass that could have been assigned to an observed word (since all probabilities must sum to 1), thus reducing the likelihood of the observed data. We will discuss various techniques for improving the maximum likelihood estimator later by using techniques called smoothing. Although extremely simple, a unigram language model is already very useful for text analysis. For example, Figure 3.6 shows three different unigram language models estimated on three different text data samples, i.e., a general English text database, a computer science research article database, and a text mining research paper. In general, the words with the highest probabilities in all the three models are those functional words in English because such words are frequently used in any text. After going further down on the list of words, one would see more content-carrying and topical words. Such content words would differ dramatically depending on the data to be used for the estimation, and thus can be used to discriminate the topics in different text samples. Unigram language models can also be used to perform semantic analysis of word relations. For example, we can use them to find what words are semantically associated with a word like computer. The main idea for doing this is to see what other words tend to co-occur with the word computer. Specifically, we can first obtain a sample of documents (or sentences) where computer is mentioned. We can then estimate a language model based on this sample to obtain p(w | computer). This model tells us which words occur frequently in the context of "computer." However, the most frequent words according to this model would likely be functional words in English or words that are simply common in the data, but have no strong association with computer. To filter out such common words, we need a model for such words which can then tell us what words should be filtered. It is easy to see that the general English language model (i.e., a background language model) would serve  the purpose well. So we can use the background language model to normalize the model p(w | computer) and obtain a probability ratio for each word. Words with high ratio values can then be assumed to be semantically associated with computer since they tend to occur frequently in its context, but not frequently in general. This is illustrated in Figure 3.7. More applications of language models in text information systems will be further discussed as their specific applications appear in later chapters. For example, we can represent both documents and queries as being generated from some language model. Given this background however, the reader should have sufficient information to understand the future chapters dealing with this powerful statistical tool. 