stemmed words (words that have been transformed into a basic root form) are a viable option so all forms of the same word are treated as one, and can be matched as one term. We also need to perform stop word removal; this removes some very common words that don't carry any content such as the, a, or of . We could use phrases or even latent semantic analysis, which characterizes documents by which cluster words belong to. We can also use smaller units, like character n-grams, which are sequences of n characters, as dimensions. In practice, researchers have found that the bag-of-words representation with phrases (or "bag-of-phrases") is the most effective representation. It's also efficient so this is still by far the most popular document representation method and it's used in all the major search engines. Sometimes we need to employ language-specific and domain-specific representation. This is actually very important as we might have variations of the terms that prevent us from matching them with each other even though they mean the same thing. Take Chinese, for example. We first need to segment text to obtain word boundaries because it's originally just a sequence of characters. A word might correspond to one character or two characters or even three characters. It's easier in English when we have a space to separate the words, but in some other languages we may need to do some natural language processing to determine word boundaries. There is also possibility to improve the similarity function. So far, we've used the dot product, but there are other measures. We could compute the cosine of the angle between two vectors, or we can use a Euclidean distance measure. The dot product still seems the best and one of the reasons is because it's very general; in fact, it's sufficiently general. If you consider the possibilities of doing weighting in different ways, cosine measure can be regarded as the dot product of two normalized vectors. That means we first normalize each vector, and then we take the dot product. That would be equivalent to the cosine measure. We mentioned that BM25 seems to be one of the most effective formulas-but there has also been further development in improving BM25, although none of these works have changed the BM25 fundamentally. In one line of work, people have derived BM25-F. Here, F stands for field, and this is BM25 for documents with structure. For example, you might consider the title field, the abstract field, the body of the research article, or even anchor text (on web pages). These can all be combined with an appropriate weight on different fields to help improve scoring for each document. Essentially, this formulation applies BM25 on each field, and then combines the scores, but keeps global (i.e., across all fields) frequency counts. This has the advantage of avoiding over-counting the first occurrence of the term. Recall that in the sublinear transformation of TF, the first occurrence is very important 110 Chapter 6 Retrieval Models and contributes a large weight. If we do that for all the fields, then the same term might have gained a large advantage in every field. When we just combine counts on each separate field, the extra occurrences will not be counted as fresh first occurrences. This method has worked very well for scoring structured documents. More details can be found in Robertson et al. [2004]. Another line of extension is called BM25+. Here, researchers have addressed the problem of over-penalization of long documents by BM25. To address this problem, the fix is actually quite simple. We can simply add a small constant to the TF normalization formula. But what's interesting is that we can analytically prove that by doing such a small modification, we will fix the problem of over-penalization of long documents by the original BM25. Thus, the new formula called BM25+ is empirically and analytically shown to be better than BM25 [Lv and Zhai 2011]. 