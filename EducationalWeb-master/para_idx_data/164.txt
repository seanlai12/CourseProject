document and it is relevant. What about the recall? Note that we are assuming that there are ten relevant documents for this query in the collection so it's one out of ten. What if the user stops at the second position? The precision is the same since both D 1 and D 2 are relevant: 100%, or two out of two. The recall is two out of ten, or 20%. If the user stops at the third position, we have an interesting case because we don't have any additional relevant documents, so the recall does not change. However, the precision is lower because we have two out of three relevant documents. The recall won't change until we see another relevant document. In this case, that point is at D 5 . There, the recall has increased to three out of ten and the precision is three out of five. As you can see, if we keep doing this, we can also get to D 8 and have a precision of four out of eight, because there are eight documents and four of them are relevant. There, the recall is four out of ten. When can we get a recall of five out of ten? In this list, we don't have it. For convenience, we often assume that the precision is zero in a situation like this. This is a pessimistic assumption since the actual precision would be higher, but we make this assumption in order to have an easy way to compute another measure called average precision, that we will discuss soon. Note that we've made some assumptions that are clearly not accurate. But, this is okay for the relative comparison of two text retrieval methods. As long as the deviation is not biased toward any particular retrieval method, the measure is acceptable since we can still accurately tell which method works better. This is the most important point to keep in mind: when you compare different algorithms, the key is to avoid any bias toward a particular method. As long as you can avoid that, it's perfectly fine to do a transformation of these measures that preserves the order. Since we can get a lot of precision-recall numbers at different positions, we can plot a curve; this is what's shown on the right side of Figure 9.4. On the x-axis are the recall values. On the y-axis are the precision values. We plot precisionrecall numbers so that we display at what recall we can obtain a certain precision. Furthermore, we can link these points to form a curve. As you see in the figure, we assumed all the precision values at the high-level recalls are zero. Although the real curves will not be exactly like this, it doesn't matter that much for comparing two methods whether we get the exact precision values here or not. In Figure 9.5, we compare two systems by plotting their PR-curves on the same graph. System A is shown in red and system B is shown in blue. Which one is better? On the left, system A is clearly better since for the same level of recall, the precision value by system A is better than system B. In general, the higher the curve is, the better. The problem is that we might see a case like the right graph-this actually happens quite often where the two curves cross each other. In this case, which one is better? This is a real problem that you might actually have to face. Suppose you build a search engine and you have an old algorithm that's shown here in blue as system B. Then, you have come up with a new idea and test it with results shown in red as system A. The question is, is your new method better than the old method? Or, more practically, do you have to replace the algorithm that you're already using in your search engine with another new algorithm? If you make the replacement, the search engine would behave like system A here, whereas if you don't do that, it will be like system B. Now, some users might like system A, while other users might like system B. So what's the difference here? Well, the difference is just that-in the low level of recall, system B is better, but in the high recall region, system A is better. That means it depends on whether the user cares about high recall, or low recall with high precision. Imagine someone is just going to check out what's happening today and wants to find out something relevant in the news. Which system is better for that task? In this case, system B is better because the user is unlikely to examine many results, i.e., the user doesn't care about high recall, only the first few results being useful. On the other hand, if a user wants to determine whether an idea has been thought of before, they will want to emphasize high recall so that they see as many relevant documents as possible and don't miss the chance to find the idea. Therefore, those users would favor system A. But this brings us back to the original question: which one is better? Again, this actually depends on the users, or more precisely, the users' task. You may not necessarily be able to come up with one number that would accurately depict the performance. You have to look at the overall picture. Despite this, it can be 9.3 Evaluation of a Ranked List 177 beneficial to have one number to compare the systems so that we can easily make a lot of different system comparisons; we need a number to summarize the range of precision-recall values. One way is to look at the area underneath the curve-the average precision. Basically, we're going to take a look at every different recall point and consider the precision. The precisions we add up correspond to retrieving the first relevant document, the second, and so on. In the example in the figure, we missed many relevant documents so in all of these cases we assume that they have zero precision. Finally, we take the average and divide it by ten, which is the total number of relevant documents in the collection. Note that we're not dividing this sum by four (which is the number of retrieved relevant documents). Dividing by four is a common mistake; this favors a system that would retrieve very few documents, making the denominator very small. In the correct formula, the denominator is ten, the total number of relevant documents. This will allow us to compute the area under the PR-curve, combining recall and precision. Mathematically, we can define average precision on a ranked list L where |L| = n as where p(i) denotes the precision at rank i of the documents in L, and Rel is the set of all relevant documents in the collection. If D i is not relevant, we would ignore the contribution from this rank by setting p(i) = 0. If D i is relevant, to obtain p(i) we divide the number of relevant documents we've seen so far by the current position in the list (which is i). If the first relevant document is at the second rank, then p(2) = 1 2 . If the third relevant document is at the seventh rank, then p(7) = 3 7 . Let's use this formula to calculate the average precision of the documents returned in Figure 9.4. Figure 9.6 shows the calculation. This measure is sensitive to a small change in position of a relevant document. If we move the third or fourth relevant document up, it would increase the averages. Conversely, if we move any relevant document down, then it would decrease. Therefore, this is a good measure because it's sensitive to the ranking of each individual relevant document. It can distinguish small differences between two ranked lists, and that's exactly what we want. In contrast, if we look at the precision at ten documents it's easy to see that it's four out of ten. That precision is very meaningful because it tells us what a user would see from their perspective. But, if we use this measure to compare two or more systems, it wouldn't be as effective since precision alone is not sensitive to where these four relevant documents are ranked in the list. If they are moved around the top ten spots, the precision at ten remains the same. In contrast, average precision is a much better measure since subtle differences in rank affect the overall score. 