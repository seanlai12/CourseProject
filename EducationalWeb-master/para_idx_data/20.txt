With the statistical foundation from the previous sections, we can now start to see how we might apply a probabilistic model to text analysis. In general, in text processing, we would be interested in a probabilistic model for text data, which defines distributions over sequences of words. Such a model is often called statistical language model, or a generative model for text data (i.e., a probabilistic model that can be used for sampling sequences of words). As we started to explain previously, we usually treat the sample space as V , the set of all observed words in our corpus. That is, we define probability distributions over words from our dataset, which are essentially multinomial distributions if we do not consider the order of words. While there are many more sophisticated models for text data (see, e.g., Jelinek [1997]), this simplest model (often called unigram language model) is already very useful for a number of tasks in text data management and analysis due to the fact that the words in our vocabulary are very well designed meaningful basic units for human communications. For now, we can discuss the general framework in which statistical models are "learned." Learning a model means estimating its parameters. In the case of a distribution over words, we have one parameter for each element in V . The workflow looks like the following. 1. Define the model. 