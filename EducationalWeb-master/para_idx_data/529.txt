In this example, c 1 = 1 2 × 3 + 0 2 × 4 + 0.48 2 × 9 + 0.42 2 × 14 + 0.41 2 × 18 + 0.47 2 × 21 1 2 + 0 2 + 0.48 2 + 0.42 2 + 0.41 2 + 0.47 2 , 1 2 × 3 + 0 2 × 10 + 0.48 2 × 6 + 0.42 2 × 8 + 0.41 2 × 11 + 0.47 2 × 7 1 2 + 0 2 + 0.48 2 + 0.42 2 + 0.41 2 + 0.47 2 = (8.47, 5.12) and c 2 = 0 2 × 3 + 1 2 × 4 + 0.52 2 × 9 + 0.58 2 × 14 + 0.59 2 × 18 + 0.53 2 × 21 0 2 + 1 2 + 0.52 2 + 0.58 2 + 0.59 2 + 0.53 2 , 0 2 × 3 + 1 2 × 10 + 0.52 2 × 6 + 0.58 2 × 8 + 0.59 2 × 11 + 0.53 2 × 7 0 2 + 1 2 + 0.52 2 + 0.58 2 + 0.59 2 + 0.53 2 = (10.42, 8.99). We repeat the iterations, where each iteration contains an E-step and an M-step. Table 11.3 shows the results from the first three iterations. The algorithm stops when the cluster centers converge or the change is small enough. "How can we apply the EM algorithm to compute probabilistic model-based clustering?" Let's use a univariate Gaussian mixture model (Example 11.6) to illustrate. Example 11.8 Using the EM algorithm for mixture models. Given a set of objects, O = {o 1 , . . . , o n }, we want to mine a set of parameters, = { 1 , . . . , k }, such that P(O| ) in Eq. (11.11) is maximized, where j = (µ j , σ j ) are the mean and standard deviation, respectively, of the jth univariate Gaussian distribution, (1 ≤ j ≤ k). We can apply the EM algorithm. We assign random values to parameters as the initial values. We then iteratively conduct the E-step and the M-step as follows until the parameters converge or the change is sufficiently small. In the E-step, for each object, o i ∈ O (1 ≤ i ≤ n), we calculate the probability that o i belongs to each distribution, that is, . (11.13) In the M-step, we adjust the parameters so that the expected likelihood P(O| ) in Eq. (11.11) is maximized. This can be achieved by setting and . (11.15) In many applications, probabilistic model-based clustering has been shown to be effective because it is more general than partitioning methods and fuzzy clustering methods. A distinct advantage is that appropriate statistical models can be used to capture latent clusters. The EM algorithm is commonly used to handle many learning problems in data mining and statistics due to its simplicity. Note that, in general, the EM algorithm may not converge to the optimal solution. It may instead converge to a local maximum. Many heuristics have been explored to avoid this. For example, we could run the EM process multiple times using different random initial values. Furthermore, the EM algorithm can be very costly if the number of distributions is large or the data set contains very few observed data points. 