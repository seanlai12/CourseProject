In this section, we will discuss ordinal logistic regression for sentiment analysis. A typical sentiment classification problem is related to rating prediction because we often try to predict sentiment value on some scale, e.g., positive to negative with other labels in between. We have an opinionated text document d as input, and we want to generate as output a rating in the range of 1 through k. Since it's a discrete rating, this could be treated as a categorization problem (finding which is the correct of k categories). Unfortunately, such a solution would not consider the order and dependency of the categories. Intuitively, the features that can distinguish rating 2 from 1 may be similar to those that can distinguish k from k − 1. For example, positive words generally suggest a higher rating. When we train a categorization problem by treating these categories as independent, we would not capture this. One approach that addresses this issue is ordinal logistic regression. Let's first think about how we use logistic regression for binary sentiment (which is a binary categorization problem). Suppose we just wanted to distinguish positive from negative. The predictors (features) are represented as X, and we can output a score based on the log probability ratio:  There are M features all together and each feature value x i is a real number. As usual, these features can be a representation of a text document. X is a binary response variable 0 or 1, where 1 means X is positive and 0 means X is negative. Of course, this is then a standard two category categorization problem and we can apply logistic regression. You may recall from Chapter 10 that in logistic regression, we assume the log probability that Y = 1 is a linear function of the features. This would allow us to also write p(Y = 1 | X) as a transformed form of the linear function of the features. The β i 's are parameters. This is a direct application of logistic regression for binary categorization. If we have multiple categories or multiple levels, we will adapt the binary logistic regression problem to solve this multilevel rating prediction, as illustrated in Figure 18.5. The idea is that we can introduce multiple binary classifiers; in each case we ask the classifier to predict whether the rating is j or above. So, when Y j = 1, it means the rating is j or above. When it's 0, that means the rating is lower than j . If we want to predict a rating in the range of 1 to k, we first have one classifier to distinguish k versus the others. Then, we're going to have another classifier to distinguish k − 1 from the rest. In the end, we need a classifier to distinguish between 2 and 1 which altogether gives us k − 1 classifiers.  With this modification, each classifier needs a different set of parameters, yielding many more parameters overall. We will index the logistic regression classifiers by an index j , which corresponds to a rating level. This is to make the notation more consistent with what we show in the ordinal logistic regression. So, we now have k − 1 regular logistic regression classifiers, each with its own set of parameters. With this approach, we can now predict ratings, as shown in Figure 18.6. After we have separately trained these k − 1 logistic regression classifiers, we can take a new instance and then invoke classifiers sequentially to make the decision. First, we look at the classifier that corresponds to the rating level k. This classifier will tell us whether this object should have a rating of k or not. If the probability according to this logistic regression classifier is larger than 0.5, we're going to say yes, the rating is k. If it's less than 0.5, we need to invoke the next classifier, which tells us whether it's at least k − 1. We continue to invoke the classifiers until we hit the end when we need to decide whether it's 2 or 1. Unfortunately, such a strategy is not an optimal way of solving this problem. Specifically, there are two issues with this approach. The first problem is that there are simply too many parameters. For each classifier, we have M + 1 parameters with k − 1 classifiers all together, so the total number of parameters is (k − 1) . (M + 1). When a classifier has many parameters, we would in general need more training data to help us decide the optimal parameters of such a complex model. The second problem is that these k − 1 classifiers are not really independent. We know that, in general, words that are positive would make the rating higher for any of these classifiers, so we should be able to take advantage of this fact. This is 