it's going to rank d 2 above all the other documents because in all the cases, given q 1 and d 2 , R = 1. With volumes of clickthrough data, a search engine can learn to improve its results. This is a simple example that shows that with even a small number of entries, we can already estimate some probabilities. These probabilities would give us some sense about which document might be more useful to a user for this query. Of course, the problem is that we don't observe all the queries and all of the documents and all the relevance values; there will be many unseen documents. In general, we can only collect data from the documents that we have shown to the users. In fact, there are even more unseen queries because you cannot predict what queries will be typed in by users. Obviously, this approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic idea of the probabilistic retrieval model. What do we do in such a case when we have a lot of unseen documents and unseen queries? The solution is that we have to approximate in some way. In the particular case called the query likelihood retrieval model, we just approximate this by another conditional probability, p(q | d , R = 1) [Lafferty and Zhai 2003]. We assume that the user likes the document because we have seen that the user clicked on this document, and we are interested in all these cases when a user liked this particular document and want to see what kind of queries they have used. Note that we have made an interesting assumption here: we assume that a user formulates the query based on an imaginary relevant document. If you just look at this as a conditional probability, it's not obvious we are making this assumption. We have to somehow be able to estimate this conditional probability without relying on the big table from Figure 6.19. Otherwise, we would have similar problems as before. By making this assumption, we have some way to bypass the big table. Let's look at how this new model works for our example. We ask the following question: which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query? We quantify this probability as a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the user's mind. We compute all these query likelihood probabilities-that is, the likelihood of the query given each document. Once we have these values, we can then rank these documents. To summarize, the general idea of modeling relevance in the probabilistic retrieval model is to assume that we introduce a binary random variable R and let the scoring function be defined based on the conditional probability p(R = 1 | d , q). We also talked about approximating this by using query likelihood. This means we have a ranking function that's based on a probability of a query given the document. p(q = "presidential campaign"|d = ) … news of presidential campaign … presidential candidate … "presidential" This probability should be interpreted as the probability that a user who likes document d would pose query q. Now the question, of course, is how do we compute this conditional probability? We will discuss this in detail in the next section. 