… news of presidential campaign … presidential candidate … p(q|d 4 = p(q = "presidential campaign"|d) = * c("presidential", d) -|d| … news about organic food campaign … … news of presidential campaign … Figure 6.21 Computing the probability of a query given a document using the query likelihood formulation. We've made the assumption that each query word is independent and that each word is obtained from the imagined ideal document satisfying the user's information need. Let's see how this works exactly. Since we are computing a query likelihood, then the total probability is the probability of this particular query, which is a sequence of words. Since we make the assumption that each word is generated independently, the probability of the query is just a product of the probability of each query word, where the probability of each word is just the relative frequency of the word in the document. For example, the probability of presidential given the document would be just the count of presidential in the document divided by the total number of words in the document (i.e., the document length). We now have an actual formula for retrieval that we can use to rank documents. Let's take a look at some example documents from Figure 6.21. Suppose now the query is presidential campaign. To score these documents, we just count how many times we have seen presidential and how many times we have seen campaign. We've seen presidential two times in d 4 , so that's 2 |d 4 | . We also multiply by 1 |d 4 | for the probability of campaign. Similarly, we can calculate probabilities for the other two documents d 3 and d 2 . If we assume d 3 and d 4 have about the same length, then it looks like we will rank d 4 above d 3 , which is above d 2 . As we would expect, it looks like this formulation captures the TF heuristic from the vector space models. However, if we try a different query like this one, presidential campaign update, then we might see a problem. Consider the word update: none of the documents contain this word. According to our assumption that a user would pick a word from a document to generate a query, the probability of obtaining a word like update would be zero. Clearly, this causes a problem because it would cause all these documents to have zero probability of generating this query. While it's fine to have a zero probability for d 2 which is not relevant, it's not okay to have zero probability for d 3 and d 4 because now we no longer can distinguish them. In fact, we can't even distinguish them from d 2 . Clearly, that's not desirable. When one has such a result, we should think about what has caused this problem, examining what assumptions have been made as we derive this ranking function. We have made an assumption that every query word must be drawn from the document in the user's mind-in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document. So let's consider an improved model. Instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document language model as depicted in Figure 6.22. Here, we assume that this document is generated by using this unigram language model, which doesn't necessarily assign zero probability for the word update. In fact, we assume this model does not assign zero probability for any word. If we're thinking this way, then the generative process is a bit different: the user has this model (distribution of words) in mind instead of a particular ideal document, although the model still has to be estimated based on the documents in our corpus. The user can generate the query using a similar process. They may pick a word such as presidential and another word such as campaign. The difference is that now we can pick a word like update even though it doesn't occur in the document. This "c am p ai gn " " p r e s i d e n t i a l " p(q = "presidential campaign"|d = ) … news of presidential campaign … presidential candidate … "presidential" campaign update " u p d a t e " … presidential 0.2 campaign 0.1 news 0.01 candidate 0.02 … update 0.00001 … Figure 6.22 Computing the probability of a query given a document using a document language model. 