Longer than avdl use the average document length as a pivot, or reference point. That means we will assume that for the average length documents, the score is about right (a normalizer would be one). If a document is longer than the average document length, then there will be some penalization. If it's shorter than the average document length, there's even some reward. The x-axis represents the length of a document. On the y-axis we show the normalizer, i.e., the pivoted length normalization. The formula for the normalizer is an interpolation of one and the normalized document lengths, controlled by a parameter b. When we first divide the length of the document by the average document length, this not only gives us some sense about how this document is compared with the average document length, but also gives us the benefit of not worrying about the unit of length. This normalizer has an interesting property; first, we see that if we set the parameter b to zero, then the normalizer value would be one, indicating no length normalization at all. If we set b to a nonzero value, then the value would be higher for documents that are longer than the average document length, whereas the value of the normalizer will be smaller for shorter documents. In this sense we see there's a penalization for long documents and a reward for short documents. The degree of penalization is controlled by b. By adjusting b (which varies from zero to one), we can control the degree of length normalization. If we plug this length normalization factor into the Pivoted length normalization VSM vector space model ranking functions that we have already examined, we will end up with state-of-the-art retrieval models, some of which are shown in Figure 6.18. Let's take a look at each of them. The first one is called pivoted length normalization. We see that it's basically the TF-IDF weighting model that we have discussed. The IDF component appears in the last term. There is also a query TF component, and in the middle there is normalized TF. For this, we have the double logarithm as we discussed before; this is to achieve a sublinear transformation. We also put a document length normalizer in the denominator of the TF formula, which causes a penalty for long documents, since the larger the denominator is, the smaller the TF weight is. The document length normalization is controlled by the parameter b. The next formula is called Okapi BM25, or just BM25. It's similar to the pivoted length normalization formula in that it has an IDF component and a query TF component. In the middle, the normalization is a little bit different; we have a sublinear transformation with an upper bound. There is a length normalization factor here as well. It achieves a similar effect as discussed before, since we put the normalizer in the denominator. Thus, again, if a document is longer, the term weight will be smaller. We have now reached one of the best-known retrieval functions by thinking logically about how to represent a document and by slowly tweaking formulas and considering our initial assumptions. 