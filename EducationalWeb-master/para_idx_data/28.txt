In an Information Retrieval course, there are 78 computer science majors, 21 electrical and computer engineering majors, and 10 library and information science majors. Two students are randomly selected from the course. What is the probability that they are from the same department? What is the probability that they are from different departments? 2.3. Use Bayes' rule to solve the following problem. One third of the time, Milo takes the bus to work and the other times he takes the train. The bus is less reliable, so he gets to work on time only 50% of the time. If taking the train, he is on time 90% of the time. Given that he was on time on a particular day, what is the probability that Milo took the bus? 2.4. In a game based on a deck of 52 cards, a single card is drawn. Depending on the type of card, a certain value is either won or lost. If the card is one of the four aces, $10 is won. If the card is one of the four kings, $5 is won. If the card is one of the eleven diamonds that is not a king or ace, $2 is won. Otherwise, $1 is lost. What are the expected winnings or losings after drawing a single card? (Would you play?) 2.5. Consider the game outlined in the previous question. Imagine that two aces were drawn, leaving 50 cards remaining. What is the expected value of the next draw? 2.6. In the information theory section, we defined three random variables X, Y , and Z when discussing entropy. We compared H (Y ) with H (Z). How does H (X) compare to the other two entropies? 2.7. In the information theory section, we compared the entropy of the word the to that of the word unicorn. In general, what types of words have a high entropy and what types of words have a low entropy? As an example, consider a corpus of ten 38 Chapter 2 Background documents where the occurs in all documents, unicorn appears in five documents, and Mercury appears in one document. What would be the entropy value of each? 2.8. Brainstorm some different features that may be good for the sentiment classification task outlined in this chapter. What are the strengths and weaknesses of such features? 2.9. Consider the following scenario. You are writing facial recognition software that determines whether there is a face in a given image. You have a collection of 100, 000 images with the correct answer and need to determine if there are faces in new, unseen images. Answer the following questions. (a) Is this supervised learning or unsupervised learning? (b) What are the labels or values we are predicting? (c) Is this binary classification or multiclass classification? (Or neither?) (d) Is this a regression problem? (e) What are the features that could be used? 2.10. Consider the following scenario. You are writing essay grading software that takes in a student essay and produces a score from 0-100%. To design this system, you are given essays from the past year which have been graded by humans. Your task is to use the system with the current year's essays as input. Answer the same questions as in Exercise 2.9. 2.11. Consider the following scenario. You are writing a tool that determines whether a given web page is one of personal home page, links to a personal home page, or neither of the above. To help you in your task, you are given 5, 000, 000 pages that are already labeled. Answer the same questions as in Exercise 2.9. In this chapter, we introduce basic concepts in text data understanding through natural language processing (NLP). NLP is concerned with developing computational techniques to enable a computer to understand the meaning of natural language text. NLP is a foundation of text information systems because how effective a TIS is in helping users access and analyze text data is largely determined by how well the system can understand the content of text data. Content analysis is thus logically the first step in text data analysis and management. While a human can instantly understand a sentence in their native language, it is quite challenging for a computer to make sense of one. In general, this may involve the following tasks. Lexical analysis. The purpose of lexical analysis is to figure out what the basic meaningful units in a language are (e.g., words in English) and determine the meaning of each word. In English, it is rather easy to determine the boundaries of words since they are separated by spaces, but it is non-trivial to find word boundaries in some other languages such as Chinese where there is no clear delimiter to separate words. Syntactic analysis. The purpose of syntactic analysis is to determine how words are related with each other in a sentence, thus revealing the syntactic structure of a sentence. Semantic analysis. The purpose of semantic analysis is to determine the meaning of a sentence. This typically involves the computation of meaning of a whole sentence or a larger unit based on the meanings of words and their syntactic structure. Pragmatic analysis. The purpose of pragmatic analysis is to determine meaning in context, e.g., to infer the speech acts of language. Natural language is used by humans to communicate with each other. A deeper understanding 40 Chapter 3 Text Data Understanding of natural language than semantic analysis is thus to further understand the purpose in communication. Discourse analysis. Discourse analysis is needed when a large chunk of text with multiple sentences is to be analyzed; in such a case, the connections between these sentences must be considered and the analysis of an individual sentence must be placed in the appropriate context involving other sentences. In Figure 3.1, we show what is involved in understanding a very simple English sentence "A dog is chasing a boy on the playground." The lexical analysis in this case involves determining the syntactic categories (parts of speech) of all the words (for example, dog is a noun and chasing is a verb). Syntactic analysis is to determine that a and boy form a noun phrase. So do the and playground, and on the playground is a prepositional phrase. Semantic analysis is to map noun phrases to entities and verb phrases to relations so as to obtain a formal representation of the meaning of the sentence. For example, the noun phrase a boy can be mapped to a semantic entity denoting a boy (i.e., b1), and a dog to an entity denoting a dog (i.e., d1). The verb phrase can be mapped to a relation predicate chasing(d1,b1,p1) as shown in the figure. Note that with this level of understanding, one may also infer additional information based on any relevant common sense knowledge. For example, if we assume that if someone is being chased, he or she may be scared, we could infer that the boy being chased (b1) may be scared. Finally, pragmatic analysis might further reveal that the person who said this sentence might intend to request an action, such as reminding the owner of the dog to take the dog back. While it is possible to derive a clear semantic representation for a simple sentence like the one shown in Figure 3.1, it is in general very challenging to do this kind of analysis for unrestricted natural language text. The main reason for this difficulty is because natural language is designed to make human communication efficient; this is in contrast with a programming language which is designed to facilitate computer understanding. Specifically, there are two reasons why NLP is very difficult. (1) We omit a lot of "common sense" knowledge in natural language communication because we assume the hearer or reader possesses such knowledge (thus there's no need to explicitly communicate it). (2) We keep a lot of ambiguities, which we assume the hearer/reader knows how to resolve (thus there's no need to waste words to clarify them). As a result, natural language text is full of ambiguity, and resolving ambiguity would generally involve reasoning with a large amount of common-sense knowledge, which is a general difficult challenge in artificial intel- 