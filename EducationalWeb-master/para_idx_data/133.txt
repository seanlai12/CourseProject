Modern search engines are designed to be able to index data that is much larger than the amount of system memory. For example, a Wikipedia database dump is about 40 GB of uncompressed text. At the time of writing this book, this is much larger than the amount of memory in common personal systems, although it is quite a common dataset for computer science researchers. TREC research datasets may even be as large as several terabytes. This doesn't even take into account realworld production systems such as Google that index the entire Web. This requires us to design indexing systems that only load portions of the raw corpus in memory at one time. Furthermore, when running queries on our indexed files, we want to ensure that we can return the necessary term statistics fast enough to ensure a usable search engine. Scanning over every document in the corpus to match terms in the query will not be sufficient, even for relatively small corpora. An inverted index is the main data structure used in a search engine. It allows for quick lookup of documents that contain any given term. The relevant data structures include (1) the lexicon (a lookup table of term-specific information, such as document frequency and where in the postings file to access the per-document term counts) and (2) the postings file (mapping from any term integer ID to a list of document IDs and frequency information of the term in those documents).  In order to support "proximity heuristics" (rewarding matching terms that are together), it is also common to store the position of each term occurrence. Such position information can be used to check whether all the query terms are matched within a certain window of text, e.g., it can be used to check whether a phrase is matched. This information is stored in the postings file since it is documentspecific. Figure 8.1 shows a representation of the lexicon and postings files. The arrows in the image are actually integer offsets that represent bit or byte indices into the postings file. 