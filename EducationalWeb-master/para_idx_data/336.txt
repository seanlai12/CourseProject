Topic analysis evaluation has similar difficulties to information retrieval evaluation. In both cases, there is usually not one true answer, and evaluation metrics heavily depend on the human issuing judgements. What defines a topic? We addressed this issue the best we could when defining the models, but the challenging nature of such a seemingly straightforward question complicates the eventual evaluation task. Log-likelihood and model perplexity are two common evaluation measures used by language models, and they can be applied for topic analysis in the same way. Both are predictive measures, meaning that held-out data is presented to the model and the model is applied to this new information, calculating its likelihood. If the model generalizes well to this new data (by assigning it a high likelihood or low perplexity), then the model is assumed to be sufficient. In Chapter 13, we mentioned Chang et al. [2009]. Human judges responded to intrusion detection scenarios to measure the coherency of the topic-word distributions. A second test that we didn't cover in the word association evaluation is the document-topic distribution evaluation. This test can measure the coherency of topics discovered from documents through the previously used intrusion test. The setup is as follows: given a document d from the collection the top three topics are chosen; call these most likely topics θ 1 , θ 2 , and θ 3 . An additional lowprobability topic θ u is also selected, and displayed along with the top three topics. 