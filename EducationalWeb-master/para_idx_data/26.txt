takes a real value, the problem is often called a regression problem. Both forms of the problem can also be called prediction when our goal is mainly to infer the unknown y for a given x; the term "prediction" is especially meaningful when y is some property of a future event. In text-based applications, both forms may occur, although the classification problem is far more common, in which case the problem is also called text categorization or text classification. We dedicate a chapter to this topic later in the book (Chapter 15). The regression problem may occur when we use text data to predict another non-text variable such as sentiment rating or stock prices; both cases are also discussed later. In classification as well as regression, the (input) data instance x is often represented as a feature vector where each feature provides a potential clue about which y value is most likely the value of f (x). What the computer learns from the training data is an optimal way to combine these features with weights on them to indicate their importance and their influence on the final function value y. "Optimal" here simply means that the prediction error on the training data is minimum, i.e., the predictedŷ values are maximally consistent with the true y values in the training data. More formally, let our collection of objects be X such that x i ∈ X is a feature vector that represents object i. A feature is an attribute of an object that describes it in some way. For example, if the objects are news articles, one feature could be whether the word good occurred in the article. All these different features are part of a document's feature vector, which is used to represent the document. In our cases, the feature vector will usually have to do with the words that appear in the document. We also have Y, which is the set of possible labels for each object. Thus, y i may be sports in our news article classification setup and y j could be politics. A classifier is a function f ( . ) that takes a feature vector as input and outputs a predicted labelŷ ∈ Y. Thus, we could have f (x i ) = sports, meaningŷ = sports. If the true y is also sports, the classifier was correct in its prediction. Notice how we can only evaluate a classification algorithm if we know the true labels of the data. In fact, we will have to use the true labels in order to learn a good function f ( . ) to take unseen feature vectors and classify them. For this reason, when studying machine learning algorithms, we often split our corpus X into two parts: training data and testing data. The training portion is used to build the classifier, and the testing portion is used to evaluate the performance (e.g., determine how many correct labels were predicted). In applications, the training data are generally all the labelled examples that we can generate, and the test cases are the data points, to which we would like to apply our machine learning program. But what does the function f ( . ) actually do? Consider a very simple example that determines whether a news article has positive or negative sentiment, i.e., Y = {positive, negative}: positive if x's count for the term good is greater than 1 negative otherwise. Of course, this example is overly simplified, but it does demonstrate the basic idea of a classifier: it takes a feature vector as input and outputs a class label. Based on the training data, the classifier may have determined that positive sentiment articles contain the term good more than once; therefore, this knowledge is encoded in the function. In Chapter 15, we will investigate some specific algorithms for creating the function f ( . ) based on the training data. Other topics such as feedback for information retrieval (Chapter 7) and sentiment analysis (Chapter 18) make use of classifiers, or resemble them. For this reason, it's good to know what machine learning is and what kinds of problems it can solve. In contrast to supervised learning, in unsupervised learning we only have the data instances X without knowing Y. In such a case, obviously we cannot really know how to compute y based on an x. However, we may still learn latent properties or structures of X. Since there is no human effort involved, such an approach is called unsupervised. For example, the computer can learn that some data instances are very similar, and the whole dataset can be represented by three major clusters of data instances such that in each cluster, the data instances are all very similar. This is essentially the clustering technique that we will discuss in Chapter 14. Another form of unsupervised learning is to design probabilistic models to model the data (called "generative models") where we can embed interesting parameters that denote knowledge that we would like to discover from the data. By fitting the model to our data, we can estimate the parameter values that can best explain the data, and treat the obtained parameter values as the knowledge discovered from the data. Applications of such an approach in analyzing latent topics in text are discussed in detail in Chapter 17. 