ing general conversations, but it may be inaccurate for describing conversations happening at a mathematics conference, where the sequence The equation has a solution may occur more frequently than Today is Wednesday. Given a language model, we can sample word sequences according to the distribution to obtain a text sample. In this sense, we may use such a model to "generate" text. Thus, a language model is also often called a generative model for text. Why is a language model useful? A general answer is that it provides a principled way to quantify the uncertainties associated with the use of natural language. More specifically, it allows us to answer many interesting questions related to text analysis and information retrieval. The following are some examples of questions that a language model can help answer. . Given that we see John and feels, how likely will we see happy as opposed to habit as the next word? Answering this question can help speech recognition as happy and habit have very similar acoustic signals, but a language model can easily suggest that John feels happy is far more likely than John feels habit. . Given that we observe baseball three times and game once in a news article, how likely is it about the topic "sports"? This will obviously directly help text categorization and information retrieval tasks. . Given that a user is interested in sports news, how likely would it be for the user to use baseball in a query? This is directly related to information retrieval. If we enumerate all the possible sequences of words and give a probability to each sequence, the model would be too complex to estimate because the number of parameters is potentially infinite since we have a potentially infinite number of word sequences. That is, we would never have enough data to estimate these parameters. Thus, we have to make assumptions to simplify the model. The simplest language model is the unigram language model in which we assume that a word sequence results from generating each word independently. Thus, the probability of a sequence of words would be equal to the product of the probability of each word. Formally, let V be the set of words in the vocabulary, and w 1 , . . . , w n a word sequence, where w i ∈ V is a word. We have Given a unigram language model θ, we have as many parameters as the words in the vocabulary, and they satisfy the constraint w∈V p(w) = 1. Such a model essentially specifies a multinomial distribution over all the words.  Given a language model θ , in general, the probabilities of generating two different documents D 1 and D 2 would be different, i.e., p(D 1 | θ) = p(D 2 | θ). What kind of documents would have higher probabilities? Intuitively it would be those documents that contain many occurrences of the high probability words according to p(w | θ). In this sense, the high probability words of θ can indicate the topic captured by θ. For example, the two unigram language models illustrated in Figure 3.5 suggest a topic about "text mining" and a topic about "health", respectively. Intuitively, if D is a text mining paper, we would expect p(D | θ 1 ) > p(D | θ 2 ), while if D is a blog article discussing diet control, we would expect the opposite: p(D | θ 1 ) < p(D | θ 2 ). We can also expect p(D | θ 1 ) > p(D | θ 1 ) and p(D | θ 2 ) < p(D | θ 2 ). Now suppose we have observed a document D (e.g., a short abstract of a text mining paper) which is assumed to be generated using a unigram language model θ, and we would like to infer the underlying model θ (i.e., estimate the probabilities of each word w, p(w | θ)) based on the observed D. This is a standard problem in statistics called parameter estimation and can be solved using many different methods. One popular method is the maximum likelihood (ML) estimator, which seeks a modelθ that would give the observed data the highest likelihood (i.e., best explain the data):θ = arg max θ p(D | θ). (3.2) It is easy to show that the ML estimate of a unigram language model gives each word a probability equal to its relative frequency in D. That is, 