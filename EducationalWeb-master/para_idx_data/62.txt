{I took a : 1, took a vacation : 1, a vacation to : 1, . . .} As we will see in this text, the unigram words document representation is of utmost importance for text representation. This vector of counts representation does have a downside though: we lost the order of the words. This representation is also known as "bag-of-words," since we only know the counts of each word, and no longer know the context or position. This unigram counting scheme can be used with POS tags or any other type of token derived from a document. Use the following three commands to do an n-gram frequency analysis on a document, for n ∈ [1, 3]. ./profile config.toml two-cities.txt --freq-unigram ./profile config.toml two-cities.txt --freq-bigram ./profile config.toml two-cities.txt --freq-trigram This will give the output file two-cities.freq.1.txt for the option --frequnigram and so on. What makes the output reasonably clear? Think back to stop words and stemming. Removing stop words gets rid of the noisy high-frequency words that don't give any information about the content of the document. Stemming will aggregate inflected words into a single count. This means the partial vector {run : 4, running : 2, runs : 3} would instead be represented as {run : 9}. Not only does this make it easier for humans to interpret the frequency analysis, but it can improve text mining algorithms, too! 4.6. Zipf's Law. In English, the top four most frequent words are about 10-15% of all word occurrences. The top 50 words are 35-40% of word occurrences. In fact, there is a similar trend in any human language. Think back to the stop words. These are the most frequent words, and make up a majority of text. At the same time, many words may only appear once in a given document. We can plot the rank of a word on the x axis, and the frequency count on the y axis. Such a graph can give us an idea of the word distribution in a given document or collection. In Figure 4.2, we counted unigram words from another Dickens book, Oliver Twist. The plot on the left is a normal x ∼ y plot and the one on the right is a log x ∼ log y plot.  Zipf's law describes the shape of these plots. What do you think Zipf's law states? The shape of these plots allows us to apply certain techniques to take advantage of the word distribution in natural language. 