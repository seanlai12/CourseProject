This formula considers both scenarios of the value of eats and captures the conditional entropy regardless of whether eats is equal to 1 or 0 (present or absent). We define the conditional entropy of meat given eats as the following expected entropy of meat for both values of eat: p(X eats = u)H (X meat | X eats = u). (13.1) In general, for any discrete random variables X and Y , we have the conditional entropy is no larger than the entropy of the variable X; that is, This is an upper bound for the conditional entropy. The inequality states that we can only reduce uncertainty by adding more information, which makes sense. As we know more information, it should always help us make the prediction and can't hurt the prediction in any case. This conditional entropy gives us one way to measure the association of two words because it tells us to what extent we can predict one word given that we know the presence or absence of another word. Before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case of the conditional entropy of a word given itself: H (X meat | X meat ). This means we know where meat occurs in the sentence, and we hope to predict whether the meat occurs in the sentence. This is zero because once we know whether the word occurs in the segment, we'll already know the answer of the prediction! That also happens to be when this conditional entropy reaches the minimum. Let's look at some other cases. One is knowing the and trying to predict meat. Another is the case of knowing eats and trying to predict meat. We can ask the question: which is smaller, H (X meat | X the ) or H (X meat | X eats )? We know that smaller entropy means it is easier to predict. In the first case, the doesn't really tell us much about meat; knowing the occurrence of the doesn't really help us reduce entropy that much, so it stays fairly close to the original entropy of meat. In the case of eats, since eats is related to meat, knowing presence or absence of eats would help us predict whether meat occurs. Thus, it reduces the entropy of meat. For this reason, we expect the second term H (X meat | X eats ) to have a smaller entropy, which means there is a stronger association between these two words. This suggests that when you use conditional entropy for mining syntagmatic relations, the algorithm would look as follows. 1. For each word w 1 , enumerate all other words w 2 from the corpus. 2. Compute H (X w1 | X w2 ). Sort all candidates in ascending order of the conditional entropy. 3. Take the top-ranked candidate words as words that have potential syntagmatic relations with w 1 . Note that we need to use a threshold to extract the top words; this can be the number of top candidates to take or a value cutoff for the conditional entropy. This would allow us to mine the most strongly correlated words with a particular word w 1 . But, this algorithm does not help us mine the strongest k syntagmatic relations from the entire collection. In order to do that, we have to ensure that these conditional entropies are comparable across different words. In this case of discovering the syntagmatic relations for a target word like w 1 , we only need to compare the conditional entropies for w 1 given different words. The conditional entropy of w 1 given w 2 and the conditional entropy of w 1 given w 3 are comparable because they all measure how hard it is to predict the w 1 . However, if we try to predict a different word other than w 1 , we will get a different upper bound for the entropy calculation. This means we cannot really compare conditional entropies across words. The next section shows how we can use mutual information to solve this problem. 