the other is the probability of actually observing the data point from that selected component model. Their product gives the probability of observing the data point when it is generated using the corresponding component model, which is why the sum would give the total probability of observing the data point regardless which component model has been used to generate the data point. As will be seen later, more sophisticated topic models tend to use more than two components, and their probability of generating a word would be of the same form as we see here except that there are more than two products in the sum (more precisely as many products as the number of component models). Once we write down the likelihood function for one word, it is very easy to see that as a whole, the mixture model can be regarded as a single word distribution defined in a somewhat complicated way. That is, it also gives us a probability distribution over words as defined above. Thus, conceptually the mixture model is yet another generative model that also generates a sequence of words by generating each word independently. This is the same as the case of a simple unigram language model, which defines a distribution over words by explicitly specifying the probability of each word. The main idea of a mixture model is to group multiple distributions together as one model, as shown in Figure 17.15, where we draw a box to "encapsulate" the two distributions to form a single generative model. When viewing the whole box as one model, we can easily see that it's just like any other generative model that would give us the probability of each word. However, how this probability is determined in such a mixture model is quite different from when we have just one unigram language model. It's often useful to examine some special cases of a model as such an exercise can help interpret the model intuitively and reveal relations between simpler models and a more complicated model. In this case, we can examine what would happen if we set the probability of choosing the background component model to zero. It is easy to see that in such a case, the term corresponding to the background model would disappear from the sum, and the mixture model would degenerate to the special case of just one distribution characterizing the topic to be discovered. In this sense, the mixture model is more general than the previous model where we have just one distribution, which can be covered as a special case. Naturally, our reason for using a mixture model is to enforce a non-zero probability of choosing the background language model so that it can help explain the common words in the data and allow our topic word distribution to be more concentrated on content words. Once we write down the likelihood function, the next question is how to estimate the parameters. As in the case of the single unigram language model, we can use any method (e.g., the maximum likelihood estimator) to estimate the parameters, which can then be regarded as the knowledge that we discover from the text.   