In fact, this constant just ensures that given the α and β parameters, the Beta distribution still integrates to one over its support. As you probably recall, this is a necessity for a probability distribution. Mathematically, we can write this as Note that the sum over the support of x is the reciprocal of that constant. If we divide by it (multiply by reciprocal), we will get one as desired: If you're proficient in calculus (or know how to use Wolfram Alpha or similar), you can confirm this fact for yourself. One more note on the Beta distribution: its expected value is We'll see how this can be useful in a minute. Let's finally rewrite our estimate of p(θ | D). The data we have observed is H , T . Additionally, we are using the two hyperparameters α and β for our Beta distribution prior. They're called hyperparameters because they are parameters for our prior distribution. But this is itself a Beta distribution! Namely, Finally, we can get our Bayesian parameter estimation. Unlike maximum likelihood estimation (MLE), where we have the parameter that maximizes our data, we integrate over all possible θ , and find its expected value given the data, E[θ | D]. In this case, our "data", is the flip results and our hyperparameters α and β: We won't go into detail with solving the integral since that isn't our focus. What we do see, though, is our final result. This result is general for any Bayesian estimate of a binomial parameter with a Beta prior.  