Another useful concept is mutual information defined on two random variables, I (X; Y ), which is defined as the reduction of entropy of X due to knowledge about another random variable Y , i.e., I (X; Y ) = H (X) âˆ’ H (X | Y ). (2.15) It can be shown that mutual information can be equivalently written as . (2.16) It is easy to see that I (X; Y ) tends to be large if X and Y are correlated, whereas I (X; Y ) would be small if X and Y are not so related; indeed, in the extreme case when X and Y are completely independent, there would be no reduction of entropy, and thus H (X) = H (X | Y ), and I (X; Y ) = 0. However, if X is completely determined by Y , then H (X | Y ) = 0, thus I (X; Y ) = H (X). Intuitively, mutual information can measure the correlation of two random variables. Clearly as a correlation measure on X and Y , mutual information is symmetric. Applications of these basic concepts, including entropy, conditional entropy, and mutual information will be further discussed later in this book. 