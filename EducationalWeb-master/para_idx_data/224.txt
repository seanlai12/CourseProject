The main issue with conditional entropy is that its values are not comparable across different words, making it difficult to find the most highly correlated words in an entire corpus. To address this problem, we can use mutual information. In particular, the mutual information of X and Y , denoted I (X; Y ), is the reduction in entropy of X obtained from knowing Y . Specifically, the question we are interested in here is how much of a reduction in entropy of X can we obtain by knowing Y . Mathematically, mutual information can be defined as (13.3) 