In vector space retrieval models, we use similarity as a notion of relevance, assuming that the relevance of a document with respect to a query is correlated with the similarity between the query and the document. Naturally, that implies that the query and document must be represented in the same way, and in this case, we represent them as vectors in a high dimensional vector space. The dimensions are defined by words, concepts, or terms. We generally need to use multiple heuristics to design a ranking function; we gave some examples which show the need for several heuristics, which include: . TF (term frequency) weighting and sublinear transformation; . IDF (inverse document frequency) weighting; and . document length normalization. These three are the most important heuristics to ensure such a general ranking function works well for all kinds of tasks. Finally, BM25 and pivoted length normalization seem to be the most effective VS formulas. While there has been some work done in improving these two powerful measures, their main idea remains the same. In the next section, we will discuss an alternative approach to the vector space representation. 