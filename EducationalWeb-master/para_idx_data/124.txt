As depicted in the language model on the right in Figure 7.6, the high-scoring words are actually common words like the. This isn't very good for feedback, because we will be adding many such words to our query when we interpolate with our original query language model. Clearly, we need to get rid of these stop words. In fact, we have already seen one way to do that, by using a background language model while learning word associations in Chapter 2. Instead, we're going to talk about another approach which is more principled. What we can do is to assume that those unwanted words are from the background language model. If we use a maximum likelihood estimate, a single model would have been forced to assign high probabilities to a word like the because it occurs so frequently. In order to reduce its probability in this model, we have to have another model to explain such a common word. It is appropriate to use the background language model to achieve this goal because this model will assign high probabilities to these common words. We assume the machine that generated these words would work as follows. Imagine we flip a coin to decide what distribution to use (topic words or background words). With the probability of λ ∈ [0, 1] the coin shows up as heads and then we're going to use the background language model. Once we know we will use the background LM, we can then sample a word from that model. Alternatively, with probability 1 − λ, we decide to use an unknown topic model to generate a word. This is a mixture model because there are two distributions that are mixed together, and we actually don't know when each distribution is used. We can treat this feedback  mixture model as a single distribution in that we can still ask it to generate words, and it will still give us a word in a random way (according to the underlying models). Which word will show up depends on both the topic distribution and background distribution. In addition, it would also depend on the mixing parameter λ; if λ is high, it's going to prefer the background distribution. Conversely, if λ is very small, we're going to use only our topic words. Once we're thinking this way, we can do exactly the same as what we did before by using MLE to adjust this model and set the parameters to best explain the data. The difference, however, is that we are not asking the unknown topic model alone to explain all the words; rather, we're going to ask the whole mixture model to explain the data. As a result, it doesn't have to assign high probabilities to words like the, which is exactly what we want. It would then assign high probabilities to other words that are common in the topic distribution but not having high probability in the background distribution. As a result, this topic model must assign high probabilities to the words common in the feedback documents yet not common across the whole collection. Mathematically, we have to compute the log likelihood of the feedback documents F with another parameter λ, which denotes noise in the feedback documents. We assume it will be fixed to some value. Assuming it's fixed, then 