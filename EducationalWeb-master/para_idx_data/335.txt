Core assumption in all topic models Added by LDA PLSA component Figure 17.37 Likelihood function of PLSA and LDA. Although the likelihood function of LDA is more complicated than PLSA, we can still use the MLE to estimate its parameters, α and β: (ˆ α,ˆ β) = arg max α, β log p(C | α, β). (17 .7) Naturally, the computation required to solve such an optimization problem is more complicated than LDA. It is now easy to see that LDA has only k + M parameters, far fewer than PLSA. However, the cost is that the interesting output that we would like to generate in topic analysis, i.e., the k word distributions {θ i } characterizing all the topics in a collection, and the topic coverage distribution {π d , j } for each document, is unfortunately, no longer immediately available to us after we estimate all the parameters. Indeed, as usually happens in Bayesian inference, to obtain values of such latent variables in LDA, we must rely on posterior inference. That is, we must compute 17.6 Evaluating Topic Analysis 383 p({θ i }, {π d , j } | C , α, β) as follows by using Bayes' Rule: This gives us a posterior distribution over all the possible values of these interesting variables, from which we can then further obtain a point estimate or compute other interesting properties that depend on the distribution. The computation process is once again complicated due to the integrals involved in some of the probabilities. Many different inference algorithms have been proposed. A very popular and efficient approach is collapsed Gibbs sampling, which works in a very similar way to the EM algorithm of PLSA. Empirically, LDA and PLSA have been shown to work similarly on various tasks when using such a model to learn a low-dimensional semantic representation of documents (by using π d , j to represent a document in the k-dimensional space). The learned word distributions also tend to look very similar. 