In order to assess whether this simplest vector space model actually works well, let's look at the example in Figure 6.4. This figure shows some sample documents and a simple query. The query is news about presidential campaign. For this example, we will examine five documents from the corpus that cover different terms in the query. You may realize that some documents are probably relevant and others probably not relevant. If we ask you to rank these documents, how would you rank them? Your answer (as the user) is the ideal ranking, R (q). Most users would agree that d 4 and d 3 are probably better than  the others since these two really cover the query well. They match news, presidential, and campaign, so they should be ranked on top. The other three, d 1 , d 2 , and d 5 , are non-relevant. Let's see if our vector space model could do the same or could do something close to our ideal ranking. First, think about how we actually use this model to score documents. In Figure 6.5, we show two documents, d 1 and d 3 , and we have the query here also. In the vector space model, we want to first compute the vectors for these documents and the query. The query has four words, so for these four words, there would be a one and for the rest there will be zeros. Document d 1 has two ones, news and about, while the rest of the dimensions are zeros. Now that we have the two vectors, we can compute the similarity with the dot product by multiplying the corresponding elements in each vector. Each pair of vectors forms a product, which represents the similarity between the two items. We actually don't have to care about the zeroes in each vector since any product with one will be zero. So, when we take a sum over all these pairs, we're just counting how many pairs of ones there are. In this case, we have seen two, so the result will be two. That means this number is the value of this scoring function; it's simply the count of how many unique query terms are matched in the document. This is how we interpret the score. Now we can also take a look at d 3 . In this case, you can see the result is three because d 3 matched the three distinct query words news, presidential, and campaign, whereas d 1 only matched two. Based on this, d 3 is ranked above d 1 . That looks pretty good. However, if we examine this model in detail, we will find some problems.  In Figure 6.6, we show all the scores for these five documents. The bit vector scoring function counts the number of unique query terms matched in each document. If a document matches more unique query terms, then the document will be assumed to be more relevant; that seems to make sense. The only problem is that there are three documents, d 2 , d 3 , and d 4 , that are tied with a score of three. Upon closer inspection, it seems that d 4 should be right above d 3 since d 3 only mentioned presidential once while d 4 mentioned it many more times. Another problem is that d 2 and d 3 also have the same score since for d 2 , news, about, and campaign were matched. In d 3 , it matched news, presidential, and campaign. Intuitively, d 3 is more relevant and should be scored higher than d 2 . Matching presidential is more important than matching about even though about and presidential are both in the query. But this model doesn't do that, and that means we have to solve these problems. To summarize, we talked about how to instantiate a vector space model. We need to do three things: 1. define the dimensions (the concept of what a document is); 2. decide how to place documents and queries as vectors in the vector space; and 3. define the similarity between two vectors. Based on this idea, we discussed a very simple way to instantiate the vector space model. Indeed, it's probably the simplest vector space model that we can derive. We used each word to define a dimension, with a zero-one bit vector to represent a document or a query. In this case, we only care about word presence or absence, ignoring the frequency. For a similarity measure, we used the dot product 