We can also use this approach to discover syntagmatic relations. When we represent a term vector to represent a context with a term vector we would likely see some terms have higher weights and other terms have lower weights. Depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with a candidate word in the context. The idea is to use the converted representation of the context to see which terms are scored high. If a term has high weight, then that term might be more strongly related to the candidate word. We have each x i defined as a normalized weight of BM25. This weight alone reflects how frequently the w i occurs in the context. We can't simply say a frequent term in the context would be correlated with the candidate word because many common words like the will occur frequently in the context. However, if we apply IDF weighting, we can then re-weight these terms based on IDF. That means the words that are common, like the, will get penalized. Now, the highest-weighted terms will not be those common terms because they have lower IDFs. Instead, the highly weighted terms would be the terms that are frequently in the context but not frequent in the collection. Clearly, these are the words that tend to occur in the context of the candidate word. For this reason, the highly weighted terms in this 260 Chapter 13 Word Association Mining idea of a weighted vector can also be assumed to be candidates for syntagmatic relations. Of course, this is only a byproduct of our approach for discovering paradigmatic relations. In the next section, we'll talk more about how to discover syntagmatic relations in particular. This discussion clearly shows the relation between discovering the two relations. Indeed, these two word relations may be discovered in a joint manner by leveraging such associations. This also shows some interesting connections between the discovery of syntagmatic relations and paradigmatic relations. Specifically, words that are paradigmatically related tend to have a syntagmatic relation with the same word. To summarize, the main idea of computing paradigmatic relations is to collect the context of a candidate word to form a pseudo document which is typically represented as a bag of words. We then compute the similarity of the corresponding context documents of two candidate words; highly similar word pairs have the highest paradigmatic relations, i.e., the words that share similar contexts. There are many different ways to implement this general idea, but we just talked about a few of the approaches. Specifically, we talked about using text retrieval models to help us design an effective similarity function to compute the paradigmatic relations. More specifically, we used BM25 TF and IDF weighting to discover paradigmatic relations. Finally, syntagmatic relations can also be discovered as a byproduct when we discover paradigmatic relations. 