designed to denote the knowledge we would like to discover. How exactly we should fit the model to the data or infer the parameter values based on the data is often a standard problem in statistics, and there are many different ways to do this as we discussed briefly in Chapter 2. Following the idea of using a generative model to solve the specific problem of discovering topics and topic coverages from text data, we see that our generative model needs to contain all the k word distributions representing the topics and the topic coverage distributions for all the documents, which is all the output we intend to compute in our problem setup. Thus, there will be many parameters in the model. First, we have |V | parameters for the probabilities of words in each word distribution, so we have in total |V |k word probability parameters. Second, for each document, we have k values of π , so we have in total Nk topic coverage probability parameters. Thus, we have in total |V |k + Nk parameters. Given that we have constraints on both θ and π , however, the number of free parameters is smaller at (|V | − 1)k + N(k − 1); in each word distribution, we only need to specify |V | − 1 probabilities and for each document, we only need to specify k − 1 probabilities. Once we set up the model, we can fit its parameters to our data. That means we can estimate the parameters or infer the parameters based on the data. In other words, we would like to adjust these parameter values until we give our data set maximum probability. Like we just mentioned, depending on the parameter values, some data points will have higher probabilities than others. What we're interested in is what parameter values will give our data the highest probability. In Figure 17.8, we illustrate , the parameters, as a one-dimensional variable. It's oversimplification, obviously, but it suffices to show the idea. The y axis shows the probability of the data. This probability obviously depends on this setting of , so that's why it varies as you change 's value in order to find * , the parameter settings that maximize the probability of the observed data. Such a search yields our estimate of the model parameters. These parameters are precisely what we hoped to discover from the text data, so we view them as the output of our data mining or topic analysis algorithm. This is the general idea of using a generative model for text mining. We design a model with some parameter values to describe the data as well as we can. After we have fit the data, we learn parameter values. We treat the learned parameters as the discovered knowledge from text data. 