pens that there are a few outliers from a different class close to our query, it will be classified incorrectly. There are several variations on the basic k-NN framework. For one, we can weight the votes of the neighbors based on distance to the query in weighted k-nearest neighbors. That is, a closer neighbor to the query would have more influence, or a higher-weighted vote. A simple weighting scheme would be to multiply each neighbor's vote by 1 d , where d is the distance to the query. Thus, more distant neighbors have less of an impact on the predicted label. Another variant is the nearest-centroid classifier. In this algorithm, instead of using individual documents as neighbors, we consider the centroid of each class label (see Chapter 14 for more information on centroids and clustering). Here, if we have n classes, we simply see which of the n is closest to the query. The centroid of each class label may be thought of as a prototype, or ideal representation of a document from that class. We also receive a performance benefit, since we only need to do n similarity comparisons as opposed to a full search engine query over all the training documents. 