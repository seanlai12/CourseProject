Know nothing about the segment H(X meat ) = -p(X meat = 0) log 2 p(X meat = 0)p(X meat = 1) log 2 p(X meat = 1) H(X meat |X eats = 1) = -p(X meat = 0|X eats = 1) log 2 p(X meat = 0|X eats = 1) H(X meat |X eats = 0) can be defined similarly -p(X meat = 1|X eats = 1) log 2 p(X meat = 1|X eats = 1) p(X meat = 1|X eats = 1) p(X meat = 0|X eats = 1) Know "eats" is present (X eats = 1) Figure 13.9 Illustration of conditional entropy. Now, we'll address a different scenario where we assume that we know something about the random variable. That is, suppose we know that eats occurred in the segment. How would that help us predict the presence or absence of a word like meat? If we frame this question using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about meat. In other words, can we reduce the entropy of the random variable corresponding to the presence or absence of meat? What if we know of the absence of eats? Would that also help us predict the presence or absence of meat? These questions can be addressed by using conditional entropy. To explain this concept, let's first look at the scenario we had before, when we know nothing about the segment. We have probabilities indicating whether a word occurs or doesn't occur in the segment. We have an entropy function that looks like the one in Figure 13.9. Suppose we know eats is present, which means we know the value of X eats . That fact changes all these probabilities to conditional probabilities. We look at the presence or absence of meat, given that we know eats occurred in the context. That is, we have p(X meat | X eats = 1). If we replace these probabilities with their corresponding conditional probabilities in the entropy function, we'll get the conditional entropy (conditioned on the presence of eats). This is essentially the same entropy function as before, except that all the probabilities now have a condition. This then tells us the entropy of meat after we have known eats occurs in the segment. Of course, we can also define this conditional entropy for the scenario where we don't see eats. Now, putting these different scenarios together, we have the complete definition of conditional entropy: 