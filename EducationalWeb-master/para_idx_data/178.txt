Map(K, V) Output Input <Bye,1> <Hadoop,1> and then just call a Collect function, which means it would then send the word and the counter to the collector. The collector would then try to sort all these key value pairs from different map functions. The programmer specifies this function as a way to process each part of the data. Of course, the second line will be handled by a different instance of the map function, which will produce a similar output. As mentioned, the collector will do the internal grouping or sorting. At this stage, you can see we have collected multiple pairs. Each pair is a word and its count in the line. Once we see all these pairs, then we can sort them based on the key, which is the word. Each word now is attached to a number of values, i.e., a number of counts. These counts represent the occurrences of this word in different lines. These new (key , value) pairs will then be fed into a reduce function. Figure 10.4 shows how the reduce function finishes the job of counting the total occurrences of this word. It already has these partial counts, so all it needs to do is simply add them up. We have a counter and then iterate over all the words that we see in this array, shown in pseudocode at the bottom of the figure. Finally, we output the key and the total count, which is precisely what we want as the output of this whole program. As we can see, this is already very similar to building an inverted index; the output here is indexed by a word, and we have a dictionary of the vocabulary. What's missing is the document IDs and the specific frequency counts of words in each particular document. We can modify this slightly to actually build an inverted index in parallel.  Let's modify our word-counting example to create an inverted index. Figure 10.5 illustrates this example. Now, we assume the input to map function is a (key , value) pair where the key is a document ID and the value denotes the string content of all the words in that document. The map function will do something very similar to what we have seen in the previous example: it simply groups all the counts of this word in this document together, generating new pairs. In the new pairs, each key is a word and the value is the count of this word in this document followed by the document ID. Later, in the inverted index, we would like to keep this document ID information, so the map function keeps track of it. After the map function, there is a sorting mechanism that would group the same words together and feed this data into the reduce function. We see the reduce function's input looks like an inverted index entry. It's just the word and all the documents that contain the word and the frequency of the word in those documents. All we need to do is simply to concatenate them into a continuous chunk of data, and this can be then stored on the filesystem. The reduce function is going to do very minimal work. Algorithm 10.1 can be used for this inverted index construction. Algorithm 10.1, adapted from Lin and Dyer [2010], describes the map and reduce functions. A programmer would specify these two functions to run on top of a MapReduce cluster. As described before, map counts the occurrences of a word using an associative array (dictionary), and outputs all the counts together with the document ID. The reduce function simply concatenates all the input that it   has been given and as single entry for this document ID key. Despite its simplicity, this MapReduce function allows us to construct an inverted index at a very large scale. Data can be processed by different machines and the programmer doesn't have to take care of the details. This is how we can do parallel index construction for web search. To summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques. Mainly, we have to store the index on multiple machines, and this is usually done by using a distributed file system like the GFS. Second, it requires creating the index in parallel because it's so large. This is done by using the MapReduce framework. It's important to note that the both the GFS and MapReduce framework are very general, so they can also support many other applications aside from indexing. 