Document LM Query q = "data mining algorithms" p("data mining alg"|d 1 ) = p("data"|d 1 ) × p("mining"|d 1 ) × p("alg"|d 1 ) p("data mining alg"|d 2 ) = p("data"|d 2 ) × p("mining"|d 2 ) × p("alg"|d 2 )  would fix our problem with zero probabilities and it's also reasonable because we're now thinking of what the user is looking for in a more general way, via a unigram language model instead of a single fixed document. In Figure 6.23, we show two possible language models based on documents d 1 and d 2 , and a query data mining algorithms. By making an independence assumption, we could have p(q | d) as a product of the probability of each query word in each document's language model. We score these two documents and then rank them based on the probabilities we calculate. Let's formally state our scoring process for query likelihood. A query q contains the words q = w 1 , w 2 , . . . , w n such that |q| = n. The scoring or ranking function is then the probability that we observe q given that a user is thinking of a particular document d. This is the product of probabilities of all individual words, which is based on the independence assumption mentioned before: (6.4) 