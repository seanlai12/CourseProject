For example, if a user has an interest in information retrieval, they might favor papers published in SIGIR. If users favor SIGIR papers, then they might have an interest in IR. The text content of items doesn't matter! This is in sharp contrast to the previous section, where we looked at item similarity through content-based filtering. Here, we will infer an individual's interest based on other similar users. The general idea is displayed in Figure 11.6: given a user u, we will rank other users based on similarity, u 1 , . . . , u m . We then predict user preferences based on the preferences of these m other users. The preference is on a common set of items o 1 , . . . , o n . If we arrange the users and objects into a matrix X, we can consider the user u i and the object o j as the point (u i , o j ) in the matrix. If we have a judgment by that user for that object, the element in that position would be the user rating X ij . Again, note that the exact content of each item doesn't matter at all. We only consider the relationship between the users and the items. This makes this approach very general since it can be applied to any items-not just text documents. Those items could be movies or products and the users could give ratings (e.g.) one through five. Some users have watched movies and rated them, but most movies for a given user are unrated (since it's unlikely that a user has examined all items in the object space). Thus, many item entries have unknown values and it is the job of collaborative filtering to infer the value of a element in this matrix based on other known values. One other assumption we have to make is that there are a sufficiently large number of user preferences available to us. For example, we need an appreciable number of ratings by users for movies that indicate their preferences for those particular movies. If we don't have sufficient data, there will be a data sparsity problem and that's often called the cold start problem. We assume an unknown function f ( . , . ), that maps a user and object to a rating. In the matrix X, we have observed there are some output values of this function and we want to infer the value of this function for other pairs that don't have values. This is very similar to other machine learning problems, where we would know the values of the function on some training data and we hope to predict the values of this function on some unseen test data. As usual, there are many approaches to solving this problem. In fact, there is a major conference specifically dedicated to this problem. We will discuss what is called a memory-based approach. When we consider a particular user, we're going to try to retrieve the relevant (i.e., similar) users to the current user. Then, we use those users to predict the preference of the current user. Let n i be the average rating of all objects by user u i . We need n i so we can normalize the ratings of objects by this user by subtracting the average rating from all the ratings. This is necessary so that the ratings from different users will be comparable; some users might be more generous and generally give higher ratings 11.2 Collaborative Filtering 231 while others might be more critical and have a lower average rating. So, their ratings can not be directly compared with each other or aggregated together, which is why we first normalize. Let u a be the user that we are interested in recommending items to (the "active" user). In particular, we are interested in recommending o j to u a . The idea here is to look at whether similar users to this user have liked this object or not. Mathematically, the predicted rating of this user on this object is a combination of the normalized ratings of different users. We're picking a sum of all the users, but not all users contribute equally to the average; each user's weight controls the influence of a user on the prediction. Naturally, the weight is related to the similarity between u a and a particular user, u i . The more similar they are, the more contribution we would like user u i to make in predicting the preference of u a . We have the following formulas. First, using the normalized ratings we can write the predicted normalized ratinĝ where w( . , . ) is the similarity function and k is the normalizer that ensuresV aj ∈ [0, 1]. Once we have the predicted normalized rating, we transform it into the rating range that u a uses: (11.5) If we want to write a program to implement this collaborative filtering, we still face the problem of determining the weighting function. Once we know this, then the formula is very easy to implement. Specific definitions of the weighting function define the different interpretations of the collaborative filtering rating estimate. As you may imagine, there are many possibilities of similarity functions. One popular approach is the Pearson Correlation Coefficient: 