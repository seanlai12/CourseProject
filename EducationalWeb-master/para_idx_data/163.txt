In the previous section, we only considered whether a relevant document appeared in the results or not-a binary measure. In this section, we will see how we can take each document's position into account when assigning an evaluation score. We saw that precision and recall are the two basic ways to quantitatively measure the performance of a search result. But, as we talked about in depth in Chapter 5, the text retrieval problem is a ranking problem, not a classification one. Thus, we need to evaluate the quality of a ranked list as opposed to whether a relevant document was returned anywhere in the results. How can we use precision and recall to evaluate a ranked list? Naturally, we will have to look at precision and recall at different cutoffs since a ranked list of relevant documents is determined by where the user stops browsing. If we assume the user sequentially browses the list of results, the user would stop at some point. That point would determine the size of the set. Therefore, that's the most important cutoff that we have to consider when we compute the precision-recall. Without knowing where exactly the user would stop, we have to consider all the possible positions where they might stop. A precision-recall curve does exactly this, as illustrated in Figure 9.4 What if the user stops at the first document? What's the precision-recall at this point? Since D 1 is relevant, the precision is one out of one since we have one   