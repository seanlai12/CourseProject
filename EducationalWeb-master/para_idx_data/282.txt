Algorithm 15.6 Perceptron Testinĝ As the algorithm shows, training of the perceptron classifier consists of continuously updating the weights vector based on its performance in classifying known examples. In the case where y i andŷ have the same sign (classified correctly), the weights are unchanged. In the case where y i <ŷ, the object should have been classified as −1 so weight is subtracted from each active feature index in w. In the opposite case (y i >ŷ), weight is added to each active feature in w. By "active feature," we mean features that are present in the current example x; only features x ij > 0 will contribute to the update in w. Eventually, the change in w will be small after some number of iterations, signifying that the algorithm has found the best accuracy it could. The final w vector is saved as the model, and it can be used to classify unseen documents. What if we need to support multiclass classification? Not all classification problems fit nicely into two categories. Fortunately, there are two common methods for using multiple binary classifiers to create one multiclass categorization method on k classes. One-vs-all (OVA) trains one classifier per class (for k total classifiers). Each classifier is trained to predict +1 for its respective class and −1 for all other classes. With this scheme, there may be ambiguities if multiple classifiers predict +1 at test time. Because of this, linear classifiers that are able to give a confidence score as a prediction are used. A confidence score such as +0.588 or +1.045 represents the +1 label, but the latter is "more confident" than the former, so the class that the algorithm predicting +1.045 would be chosen. All-vs-all (AVA) trains k(k−1) 2 classifiers to distinguish between all pairs of k classes. The class with the most +1 predictions is chosen as the final answer. Again, confidence-based scoring may be used to add votes into totals for each class label. 