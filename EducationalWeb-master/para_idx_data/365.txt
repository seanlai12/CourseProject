context for analyzing the text data. The output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series. That is, whenever the topic is mentioned frequently in the text stream, the time series variable tends to have higher (or lower) values. We call these topics causal topics since they can potentially explain the cause of fluctuation of the time series and offer insights for humans to further analyze the topics for better understanding of the time series. They can also be useful features for predicting time series. Intuitively, the output is similar to what we generate by using a topic model, but with an important difference. In regular topic modeling, our goal is to discover topics that best explain the content in the text data, but in our setup of discovering causal topics, the topics to be discovered should not only be semantically meaningful and coherent (as in the case of regular topic modeling), but also be correlated with the external time series. To solve this problem, a natural idea is to apply a model such as CPLSA to our text stream so as to discover a number of topics along with their coverage over time. This would allow us to obtain a time series for each topic representing its coverage in the text such as the temporal trends shown in Figure 19.7. We can then choose the topics from this set that have the strongest correlation with the external time series. However, this approach is not optimal because the content of the topics would have been discovered solely based on the text data (e.g., maximizing the likelihood function) without consideration of the time series at all. Indeed, the discovered topics would tend to be the major topics that explain the text data well (as they should be), but they are not necessarily correlated with time series. Even if we choose the best ones from them, the most correlated topics might still have a low correlation, and thus not be very useful from the perspective of discovering causal topics. One way to improve this simple approach is to use time series context to not only select the topics with the highest correlations with the time series, but also influence the content of topics. One approach is called Iterative Causal Topic Modeling, shown in Figure 19.15. The idea of this approach is to do an iterative adjustment of topics discovered by topic models using time series to induce a prior. Specifically, as shown in Figure 19.15, we first take the text stream as input and apply regular topic modeling to generate a number of topics (four shown here). Next, we use the external time series to assess which topic is more causally related (correlated) with the external time series by using a causality measure such as Granger Test. For example, in this figure, topic 1 and topic 4 may be more correlated than topic 2 and topic 3. The simple approach that we discussed earlier would have just stopped here and taken topics 1 and 4 as potential causal topics. However, here we go further to improve them by zooming into the word level to further identify the words that are most strongly correlated with the time series. Specifically, we can look into each word in the top ranked words for each topic (those with highest probabilities), and compute the correlation of each word with the time series. This would allow us to further separate those words into three groups: strongly positively correlated words; strongly negatively correlated words; and weakly correlated words. The first two groups can then each be regarded as seeds to define two new subtopics that can be expected to be positively and negatively correlated with the time series, respectively. The figure shows a potential split of topic 1 into two such potentially more correlated subtopics: one with w 1 and w 3 (positive) and one with w 2 and w 4 (negative). However, these two subtopics may not necessarily be coherent semantically. To improve the coherence, the algorithm would not take these directly as topics, but rather feed them as a prior to the topic model so as to steer the topic model toward discovering topics matching these two subtopics. Thus, we can expect the topics discovered by the topic model in the next iteration to be more correlated with the time series than the original topics discovered from the previous iteration. Once we discover a new generation of topics, we can repeat the process to analyze the words in correlated topics and generate another set of seed topics, which would then be fed into the topic model again as prior.  The whole process is seen as a heuristic way of optimizing causality and coherence, which is precisely our goal in discovery of causal topics. When applying the topic model, we ensure the semantic coherence in the discovered topics, but when splitting a topic into positively and negatively subtopics, we improve the correlation with the time series, essentially iteratively improving both coherence and correlation (causality), as illustrated in Figure 19.16. Here we see that the pure topic models will be very good at maximizing topic coherence, thus scoring high on the x-axis, meaning the discovered topics will all be meaningful. If we only use a causality test or correlation measure, then we would generate a set of words that are strongly correlated with the time series, thus scoring high on the y-axis (causality), but they aren't necessarily coherent semantically. Our goal is to have a causal topic that scores high, in both topic coherence and correlation. The approach discussed above can be regarded as an alternate way to maximize both axes. When we apply the topic models we're maximizing the coherence, while when we decompose the topic model words into sets of words that are very strongly correlated with the time series, we would select the most strongly correlated words with the time series. Thus we are, in effect, pushing the model back to the causal dimension to make it better in causal scoring. When we apply the selected words as a prior to guide topic models in topic discovery, we again go back to optimize the coherence. Eventually, such an iterative process can be expected to reach a compromise of semantic coherence and strong correlation with time series. 