Clearly, if we find a θ (n+1) such that Q(θ (n+1) ; θ (n) ) > Q(θ (n) ; θ (n) ), then we will also have L(θ (n+1) ) > L(θ (n) ). Thus, the general procedure of the EM algorithm is the following. 1. Initialize θ (0) randomly or heuristically according to any prior knowledge about where the optimal parameter value might be. 2. Iteratively improve the estimate of θ by alternating between the following twosteps: 1. the E-step (expectation): Compute Q(θ ; θ (n) ), and 2. the M-step (maximization): Re-estimate θ by maximizing the Qfunction: θ (n+1) = argmax θ Q(θ; θ (n) ). 3. Stop when the likelihood L(θ ) converges. As mentioned earlier, the complete likelihood L c (θ ) is much easier to maximize as the values of the hidden variable are assumed to be known. This is why the Q-function, which is an expectation of L c (θ ), is often much easier to maximize than the original likelihood function. In cases when there does not exist a natural latent variable, we often introduce a hidden variable so that the complete likelihood function is easy to maximize. The major computation to be carried out in the E-step is to compute p(H | X, θ (n) ), which is sometimes very complicated. In our case, this is simple: And, of course, p(z ij = 0 | F , θ (n) F ) = 1 − p(z ij = 1 | F , θ (n) F ). Note that, in general, z ij may depend on all the words in F . In our model, however, it only depends on the corresponding word d ij . The M-step involves maximizing the Q-function. This may sometimes be quite complex as well. But, again, in our case, we can find an analytical solution. In order to achieve this, we use the Lagrange multiplier method since we have the following constraint on the parameter variables {p(w | θ F )} w∈V , where V is our vocabulary: w∈V p(w | θ F ) = 1. We thus consider the following auxiliary function: and take its derivative with respect to each parameter variable p(w | θ F ) (B.6) Setting this derivative to zero and solving the equation for p(w | θ F ), we obtain . (B.8) Note that we changed the notation so that the sum over each word position in document d i is now a sum over all the distinct words in the vocabulary. This is possible, because p(z ij | F , θ (n) F ) depends only on the corresponding word d ij . Using word w, rather then the word occurrence d ij , to index z, we have . (B.9) We therefore have the following EM updating formulas for our simple mixture model: B.5 The General Procedure of EM 471 Note that we never need to explicitly compute the Q-function; instead, we compute the distribution of the hidden variable z and then directly obtain the new parameter values that will maximize the Q-function.  