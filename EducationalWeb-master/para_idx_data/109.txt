because that's the coefficient in front of the probability of the word given by the collection language model. The second smoothing method we will discuss is called Dirichlet prior smoothing, or Bayesian smoothing. Again, we face the problem of zero probability for words like network. Just like Jelinek-Mercer smoothing, we'll use the collection language model, but in this case we're going to combine it with the MLE esimate in a somewhat different way. The formula first can be seen as an interpolation of the MLE probability and the collection language model as before. Instead, however, α d is not simply a fixed λ, but a dynamic coefficient which takes μ > 0 as a parameter. Based on Figure 6.27, we can see if we set μ to a constant, the effect is that a long document would actually get a smaller coefficient here. Thus, a long document would have less smoothing as we would expect, so this seems to make more sense than fixed-coefficient smoothing. The two coefficients |d| |d|+μ and μ |d|+μ would still sum to one, giving us a valid probability model. This smoothing can be understood as a dynamic coefficient interpolation. Another way to understand this formula-which is even easier to remember-is to rewrite this smoothing method in this form:  Here, we can easily see what change we have made to the MLE. In this form, we see that we add a count of μ . p(w | C) to every word, which is proportional to the probability of w in the entire corpus. We pretend every word w has μ . p(w | C) additional pseudocounts. Since we add this extra probability mass in the numerator, we have to re-normalize in order to have a valid probability distribution. Since w∈V p(w | C) = 1, we can add a μ in the denominator, which is the total number of pseudocounts we added for each w in the numerator. Let's also take a look at this specific example again. For the word text, we will have ten counts that we actually observe but we also added some pseudocounts which are proportional to the probability of text in the entire corpus. Say we set μ = 3000, meaning we will add 3000 extra word counts into our smoothed model. We want some portion of the 3000 counts to be allocated to text; since p(text | C) = 0.001, we'll assign 0.001 . 3000 counts to that word. The same goes for the word network; for d, we observe zero counts, but also add μ . p(network | C) extra pseudocounts for our smoothed probability. In Dirichlet prior smoothing, α d will actually depend on the current document being scored, since |d| is used in the smoothed probability. In the Jelinek-Mercer linear interpolation, α d = λ, which is a constant. For Dirichlet prior, we have α d = μ |d|+μ , which is the interpolation coefficient applied to the collection language model. For a slightly more detailed derivation of these variables, the reader may consult Appendix A. Now that we have defined p seen and α d for both smoothing methods, let's plug these variables in the original smoothed query likelihood retrieval function. Let's start with Jelinek-Mercer smoothing: Then, plugging this into the entire query likelihood retrieval formula, we get We ignore the |q| log α d additive term (derived in the previous section) since α d = λ does not depend on the current document being scored. We'll end up having a ranking function that is strikingly similar to a vector space model since it is a sum over all the matched query terms. The value of the logarithm term is nonnegative. We see very clearly the TF weighting in the numerator, which is scaled sublinearly. We also see the IDF-like weighting, which is the p(w | C) term in the denominator; the more frequent the term is in the entire collection, the more 6.4 Probabilistic Retrieval Models 127 discounted the numerator will be. Finally, we can see the |d| in the denominator is a form of document length normalization, since as |d| grows, the overall term weight would decrease, suggesting that the impact of α d in this case is clearly to penalize a long document. The second fraction can also be considered as the ratio of two probabilities; if the ratio is greater than one, it means the probability of w in d is greater than appearing by chance in the background. If the ratio is less than one, the chance of seeing w in d is actually less likely than observing it in the collection. What's also important to note is that we received this weighting function automatically by making various assumptions, whereas in the vector space model, we had to go through those heuristic design choices in order to get this. These are the advantages of using this kind of probabilistic reasoning where we have made explicit assumptions. We know precisely why we have a logarithm here, and precisely why we have these probabilities. We have a formula that makes sense and does TF-IDF weighting and document length normalization. Let's look at the complete function for Dirichlet prior smoothing now. We know what p seen is and we know that α d = μ |d|+μ : . (6.12) We can now substitute this into the complete formula: The form of the function looks very similar to the Jelinek-Mercer scoring function. We compute a ratio that is sublinearly scaled by a non-negative logarithm. Both TF and IDF are computed in almost the exact same way. The difference here is that Dirichlet prior smoothing can capture document length normalization differently than Jelinek-Mercer smoothing. Here, we have retained the |q| log α d term since α d depends on the document, namely |d|. If |d| is large, then less extra mass is added onto the final score; if |d| is small, more extra mass is added to the score, effectively rewarding a short document. To summarize this section, we've talked about two smoothing methods: Jelinek-Mercer, which is doing the fixed coefficient linear interpolation, and Dirichlet prior, 128 Chapter 6 Retrieval Models which adds pseudo counts proportional to the probability of the current word in the background collection. In most cases we can see, by using these smoothing methods, we will be able to reach a retrieval function where the assumptions are clearly articulated, making them less heuristic than some of the vector space models. Even though we didn't explicitly set out to define the popular VS heuristics, in the end we naturally arrived at TF-IDF weighting and document length normalization, perhaps justifying their inclusion in the VS models. Each of these functions also has a smoothing parameter (λ or μ) with an intuitive meaning. Still, we need to set these smoothing parameters or estimate them in some way. Overall, this shows that by using a probabilistic model, we follow very different strategies than the vector space model. Yet in the end, we end up with retrieval functions that look very similar to the vector space model. Some advantages here are having assumptions clearly stated and a final form dictated by a probabilistic model. This section also concludes our discussion of the query likelihood probabilistic retrieval models. Let's recall what assumptions we have made in order to derive the functions that we have seen the following. 