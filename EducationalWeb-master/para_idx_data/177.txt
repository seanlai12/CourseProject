In this section, we will discuss how to create a web-scale index. After our crawler delivers gigabytes or terabytes of data, the next step is to use the indexer to create the inverted index. In general, we can use the standard information retrieval techniques for creating the index, but there are new challenges that we have to solve for web scale indexing. The two main challenges are scalability and efficiency. The index will be so large that it cannot actually fit into any single machine or single disk, so we have to store the data on multiple machines. Also, because the data is so large, it's beneficial to process the data in parallel so that we can produce the index quickly. To address these challenges, Google has made a number of innovations. One is the Google File System, which is a general distributed file system that can help programmers manage files stored on a cluster of machines. The second is MapReduce, which is a general software framework for supporting parallel computation. Hadoop is the most well known open source implementation of MapReduce, now used in many applications. Figure 10.1 shows the architecture of the Google File System (GFS). It uses a very simple centralized management mechanism to manage all the specific locations of files. That is, it maintains a file namespace and lookup table to know where exactly each file is actually stored. The application client talks to the GFS master node, which obtains specific locations of the files to process. This filesystem stores its files on machines in fixed-size chunks; each data file is separated into many 64 MB chunks. These chunks are replicated to ensure reliability. All of these details are something that the programmer doesn't have to worry about, and it's all taken care of by this filesystem. From the application perspective, the programmer would see a normal file. The program doesn't have to know where exactly it's stored, and can just invoke high level operators to process the file. Another feature is that the data transfer is directly between application and chunk servers, so it's efficient in this sense as well. On top of the GFS, Google proposed MapReduce as a general framework for parallel programming. This supports tasks like building an inverted index. Like GFS,  this framework hides low level features from the programmer. As a result, the programmer can make minimum effort to create an application that can be run on a large cluster in parallel. Some of the low-level details hidden in the framework are communications, load balancing, and task execution. Fault tolerance is also built in; if one server goes down, some tasks may not be finished. Here, the MapReduce mechanism would know that the task has not been completed and would automatically dispatch the task on other servers that can do the job. Again, the programmer doesn't have to worry about this. In MapReduce, the input data are separated into a number of (key, value) pairs. What exactly the value is will depend on the data. Each pair will be then sent to a map function which the programmer writes. The map function will then process these (key, value) pairs and generate a number of other (key, value) pairs. Of course, the new key is usually different from the old key that's given to map as input. All the outputs of all the calls to map are collected and sorted based on the key. The result is that all the values that are associated with the same key will be grouped together. For each unique key we now have a set of values that are attached to this key. This is the data that is sent to the reduce function. Each reduce instance will handle a different key. This function processes its input, which is a key and a set of values, to produce another set of (key , value) pairs as the output. This is the general framework of MapReduce. Now, the programmer only needs to write the map function and the reduce function. Everything else is taken care of by the MapReduce framework. With such a framework, the input data can be partitioned into multiple parts which are processed in parallel first by map, and then processed again in parallel once we reach the reduce stage. Figure 10.3 shows an example of word counting. The input is files containing tokenized words and the output that we want to generate is the number of occurrences of each word. This kind of counting would be useful to assess the popularity of a word in a large collection or achieving an effect of IDF weighting for search. So, how can we solve this problem? One natural thought is that this task can be done in parallel by simply counting different parts of the file in parallel and combining all the counts. That's precisely the idea of what we can do with MapReduce: we can parallelize lines in this input file. More specifically, we can assume the input to each map function is a (key , value) pair that represents the line number and the string on that line. The first line is the pair (1, Hello World Bye World). This pair will be sent to a map function that counts the words in this line. In this case, there are only four words and each word gets a count of one. The map pseudocode shown at the bottom of the figure is quite simple. It simply needs to iterate over all the words in this line, 