From the last section, we showed how to smooth the query likelihood retrieval model with the collection language model. We end up having a retrieval function that looks like the following: We can see it's a sum of all the matched query terms, and inside the sum it's a count of terms in the query with some weight for the term in the document. We saw in the previous section how TF and IDF are captured in this sum. We also mentioned how the second term α d can be used for document length normalization. If we wanted to implement this function using a programming language, we'd still need to figure out a few variables; in particular, we're going to need to know how to estimate the probability of a word and how to set α d . In order to answer these questions, we have to think about specific smoothing methods, where we define p seen and α d . We're going to talk about two different smoothing methods. The first is a linear interpolation with a fixed mixing coefficient. This is also called Jelinek-Mercer smoothing. The idea is actually quite simple. Figure 6.26 shows how we estimate the document language model by using MLE. That gives us word counts normalized by the total number of words in the document. The idea of using this method is to maximize the probability of the observed text. As a result, if a word like network is not observed in the text, it's going to get zero probability. The idea of smoothing is to rely on the collection reference model where this word is not going to have a zero probability, helping us decide what non-zero probability should be assigned to such a word. In Jelinek-Mercer smoothing, we do a linear interpolation between the maximum likelihood estimate and the collection language model. This is controlled by the smoothing parameter λ ∈ [0, 1]. Thus, λ is a smoothing parameter for this particular smoothing method. The larger λ is, the more smoothing we have, putting more weight on the background probabilities. By mixing the two distributions together, we achieve the goal of assigning non-zero probability to unseen words in the document that we're currently scoring. So let's see how it works for some of the words here. For example, if we compute the smoothed probability for the word text, we get the MLE estimate in the document interpolated with the background probability. Since text appears ten times in d and |d| = 100, our MLE estimate is 10 100 . In the background, we have p(text | C) = 0.001, giving our smoothed probability of In Figure 6.26 we also consider the word network, which does not appear in d. In this case, the MLE estimate is zero, and its smoothed probability is 0 + λ . p(w | C) = λ . 0.001. You can see now that α d in this smoothing method is just λ Document d Total #words = 100   