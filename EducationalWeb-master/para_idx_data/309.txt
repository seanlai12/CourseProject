The simplest, natural way to define a topic is just as a term. A term can be a word or a phrase. For example, we may have terms like sports, travel, or science to denote three separate topics covered in text data, as shown in Figure 17.4. If we define a topic in this way, we can then analyze the coverage of such topics in each document based on the occurrences of these topical terms. A possible scenario may look like what's shown in Figure 17.4: 30% of the content of Doc 1 is about sports, and 12% is about travel, etc. We might also discover Doc 2 does not cover sports at all. So the coverage π 21 is zero. Recall that we have two tasks. One is to discover the topics and the other is to analyze coverage. To solve the first problem, we need to mine k topical terms from a collection. There are many different ways to do that. One natural way is to first parse the text data in the collection to obtain candidate terms. Here candidate terms can be words or phrases. The simplest case is to just take each word as a term. These words then become candidate topics. Next, we will need to design a scoring function to quantify how good each term is as a topic. There are many things that we can consider when designing such a function with a main basis being the statistics of terms. Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. That would mean we want to favor a frequent term. However, if we simply use the frequency to design the scoring function, then the highest scored terms would be general terms or function words like the or a. Those terms occur 17.1 Topics as Terms 333 very frequently in English, so we also want to avoid having such words on the top. That is, we would like to favor terms that are fairly frequent but not too frequent. A specific approach to achieving our goal is to use TF-IDF weighting discussed in some previous chapters of the book on retrieval models and word association discovery. An advantage of using such a statistical approach to define a scoring function is that the scoring function would be very general and can be applied to any natural language, any text. Of course, when we apply such an approach to a particular problem, we should always try to leverage some domain-specific heuristics. For example, in news we might favor title words because the authors tend to use the title to describe the topic of an article. If we're dealing with tweets, we could also favor hashtags, which are invented to denote topics. After we have designed the scoring function, we can discover the k topical terms by simply picking the k terms with the highest scores. We might encounter a situation where the highest scored terms are all very similar. That is, they are semantically similar, or closely related, or even synonyms. This is not desirable since we also want to have a good coverage over all the content in the collection, meaning that we would like to remove redundancy. One way to do that is to use a greedy algorithm, called Maximal Marginal Relevance (MMR) re-ranking. The idea is to go down the list based on our scoring function and select k topical terms. The first term, of course, will be picked. When we pick the next term, we will look at what terms have already been picked and try to avoid picking a term that's too similar. So while we are considering the ranking of a term in the list, we are also considering the redundancy of the candidate term with respect to the terms that we already picked. With appropriate thresholding, we can then get a balance of redundancy removal and picking terms with high scores. The MMR technique is described in more detail in Chapter 16. After we obtain k topical terms to denote our topics, the next question is how to compute the coverage of each topic in each document, π ij . One solution is to simply count occurrences of each topical term as shown in Figure 17.5. So, for example, sports might have occurred four times in document d i , and travel occurred twice. We can then just normalize these counts as our estimate of the coverage probability for each topic. The normalization is to ensure that the coverage of each topic in the document would add to one, thus forming a distribution over the topics for each document to characterize coverage. As always, when we think about an idea for solving a problem, we have to ask the following questions: how effective is the solution? Is this the best way of solving problem? In general, we have to do some empirical evaluation by using actual data sets and to see how well it works. However, it is often also instructive to analyze "Sports" count("sports", d i ) = 4 count("travel", d i ) = 2 count("science", Figure 17.5 Computing topic coverage when a topic is a term. 2. "Star" can be ambiguous (e.g., star in the sky). some specific examples. So now let's examine the simple approach we have been discussing with a sample document in Figure 17.6. Here we have a text document that's about an NBA basketball game. In terms of the content, it's about sports, but if we simply count these words that represent our topics, we will find that the word sports actually did not occur in the article, even though the content is about sports. Since the count of sports is zero, the coverage of sports would be estimated as zero. We may note that the term science also did not occur in the document, and so its estimate is also zero, which is intuitively what we want since the document is not about science. However, giving a zero probability to sports certainly is a problem because we know the content is about sports. What's worse, the term travel actually occurred in the document, so when we estimate 