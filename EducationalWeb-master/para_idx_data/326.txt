For now, let's assume we have a tentative estimate of all the parameters. How can we infer which of the two distributions a word has been generated from? Consider a specific word such as text. Is it more likely from θ d or θ B ? To answer this question, we compute the conditional probability p(θ d | text). The value of p(θ d | text) would depend on two factors. . How often is θ d (as opposed to θ B ) used to generate a word in general? This probability is given by p(θ d ). If p(θ d ) is high, then we'd expect p(θ d | text) to be high. . If θ d is indeed chosen to generate a word, how likely would we observe text? This probability is given by p(w | θ d ). If p(text | θ d ) is high, then we'd also expect p(θ d | text) to be high. Our intuition can be rigorously captured by using Bayes' rule to infer p(θ d | text), where we essentially compare the product p(θ d )p(text | θ d ) with the product p(θ B ) p(text | θ B ) to see whether text is more likely generated from θ d or from θ B . This is illustrated in Figure 17.23. The Bayesian inference involved here is a typical one where we have some prior about how likely each of these two distributions is used to generate any word (i.e., p(θ d ) and p(θ B )). These are prior because they encode our belief about which distribution before we even observe the word text; a prior that has very high p(θ d ) would encourage us to lean toward guessing θ d for any word. Such a prior is then updated by incorporating the data likelihood p(text | θ d ) and p(text | θ B ) so that we would favor a distribution that gives text a higher probability. In the example shown in Figure 17.23, our prior says that each of the two models is equally likely; thus, it is a non-informative prior (one with no bias). As a result, our inference of which distribution has been used to generate a word would solely be based on p(w | θ d ) and p(w | θ B ). Since p(text | θ d ) is much larger than p(text | θ B ), we can conclude that θ d is much more likely the distribution that has been used to generate text. In general, our prior may be biased toward a particular distribution. Indeed, a heavily biased prior can even dominate over the data likelihood to essentially dictate the decision. For example, imagine our prior says p(θ B ) = 0.99999999, then our inference result would say that text is more likely generated by θ B than by θ d even though p(text | θ d ) is much higher than p(text | θ B ), due to the very strong prior. Bayes' Rule provides us a principled way of combining the prior and data likelihood. In Figure 17.23, we introduced a binary latent variable z here to denote whether the word is from the background or the topic. When z is 0, it means it's from the topic, θ d ; when it's 1, it means it's from the background, θ B . The posterior probability p(z = 0 | w = text) formally captures our guess about which distribution has been used to generate the word text, and it is seen to be proportional to the product of the prior p(θ d ) and the likelihood p(text | θ d ), which is intuitively very meaningful since in order to generate text from θ d , we must first choose θ d (as opposed to θ B ), which is captured by p(θ d ), and then obtain word text from the selected θ d , which is captured by p(w | θ d ). Understanding how to make such a Bayesian inference of which distribution has been used to generate a word based on a set of tentative parameter values is very crucial for understanding the EM algorithm. This is essentially the E-step of the EM algorithm where we use Bayes' rule to partition data and allocate all the data points among all the component models in the mixture model. Note that the E-step essentially helped us figure out which words have been generated from θ d (and equivalently, which words have been generated from θ B ) except that it does not completely allocate a word to θ d (or θ B ), but splits a word in between the two distributions. That is, p(z = 0 | text) tells us what percent of the count of text should be allocated to θ d , and thus contribute to the estimate of θ d . This way, we will be able to collect all the counts allocated to θ d , and renormalize them to obtain a potentially improved estimate of p(w | θ d ), which is our goal. This step of re-estimating parameters based on the results from the E-step is called the M-step. 