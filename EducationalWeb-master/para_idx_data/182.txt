The vector p equals the transpose of a matrix multiplied by p again. The transposed matrix is in fact the sum from 1 to N written in matrix form. Recall from linear algebra that this is precisely the equation for an eigenvector. Thus, this equation can be solved by using an iterative algorithm. In this iterative algorithm, called power iteration, we simply start with a random p. We then repeatedly update p by multiplying the transposed matrix expression by p. Let's look at a concrete example: set α = 0.2. This means that there is a 20% chance of randomly jumping to a page on the entire web and an 80% chance of randomly following a link from the current page. We have the original transition matrix M as before that encodes the actual links in the graph. Then, we have this uniform smoothing transition matrix I representing random jumping. We combine them together with interpolation via α to form another matrix we call A: The PageRank algorithm will randomly initialize p first, and then iteratively update it by using matrix multiplication. If we rewrite this matrix multiplication in terms of just A, we'll get the following: ⎡  If you want to compute the updated value for d 1 , you multiply the top row in A by the column vector of PageRank scores from the previous iteration. This is how we update the vector; we started with some initial values and iteratively multiply the matrices together, which generates a new set of scores. We repeat this multiplication until the values in p converge. From linear algebra, we know that since there are no zero values in the matrix, such iteration is guaranteed to converge. At that point we will have the PageRank scores for all the pages. Interestingly, this update formula can be interpreted as propagating scores across the graph. We can imagine we have values initialized on each of these pages, and if you look at the equation, we combine the scores of the pages that would lead to reaching a page. That is, we'll look at all the pages that are pointing to a page and combine their scores with the propagated score in order to get the next score for the current document. We repeat this for all documents, which transfers probability mass across the network. In practice, the calculation of the PageRank score is actually quite efficient because the matrices are sparse-that means that if there isn't a link into the current page, we don't have to worry about it in the calculation. It's also possible to normalize the equation, and that will give a somewhat different form, although the relative ranking of pages will not change. The normalization is to address the potential problem of zero outlinks. In that case, the probabilities of reaching the next page from the current page will not sum to 1 because we have lost some probability mass when we assume that there's some probability that the surfer will try to follow links (although in this case there are no links to follow!). There are many extensions to PageRank. One extension is to do query-specific PageRank, also called Personalized PageRank. For example, in this topic-specific PageRank, we can simply assume when the surfer gets bored, they won't randomly jump into any page on the web. Instead, they jump to only those pages that are relevant to the query. For example, if the query is about sports, then we could assume that when we do random jumping, we randomly jump to a sports page. By doing this, our PageRank scores align with sports. Therefore, if you know the current query is about sports, we can use this specialized PageRank score to rank the results. Clearly, this would be better than using a generic PageRank score for the entire web. PageRank is a general algorithm that can be used in many other applications such as network analysis, particularly in social networks. We can imagine if you compute a person's PageRank score on a social network (where a link indicates a friendship relation), you'll get some meaningful scores for people. 