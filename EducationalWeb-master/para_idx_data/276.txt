k-NN is a learning algorithm that directly uses our inverted index and search engine. Unlike the next two algorithms we will discuss, there is no explicit training step; all we need to do is index the training documents. This makes k-NN a lazy learner or instance-based classifier. As shown in the training and testing algorithms, the basic idea behind k-NN is to find the most similar documents to the query document, and use the most common class label of the similar documents. The assumption is that similar documents will have the same class label. Figure 15 similar documents are blue. In the case of a tie, the highest ranking document of the class with a tie would be chosen. k-NN can be applied to any distance measure and any document representation. With only some slight modifications, we can directly use this classification method with an existing inverted index. A forward index is not required. Despite these advantages, there are some downsides as well. For one, finding the nearest neighbors requires performing a search engine query for each testing instance. While this is a heavily optimized operation, it will still be significantly slower than other machine learning algorithms in test time. As we'll see, the other two algorithms perform simple vector operations on the query vector as opposed to querying the inverted index. However, these other algorithms have a much longer training time than k-NN-this is the tradeoff. One more important point is the chosen label for k-NN is highly dependant on only the k neighbors; on the other hand, the other two algorithms take all training examples in account. In this way, k-NN is sensitive to the local structure of the feature space that the top k documents occupy. If it so hap- 