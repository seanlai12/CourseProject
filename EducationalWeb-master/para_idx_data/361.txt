• Probabilistic topic modeling as optimization: maximize likelihood * = arg max p(TextData| ) • Main idea: network imposes restraints on model parameters -The text at two adjacent nodes of the network tends to cover similar topics -Topic distributions are smoothed over adjacent nodes -Add network-induced regularizers to the likelihood objective function knowledge or any preferences dictated by the user or application. This would lead to a modification to the original optimization problem such that the likelihood is no longer the only objective to optimize. Following this thinking, the main idea of performing joint analysis of text and associated network context is to use the network to impose some constraints on the model parameters. For example, the text at adjacent nodes of the network can be assumed to cover similar topics. Indeed, in many cases, they do tend to cover similar topics; two authors collaborating with each other tend to publish papers on similar topics. Such a constraint or preference would essentially attempt to smooth the topic distributions on the graph or network so that adjacent nodes will have very similar topic distributions. This means we expect them to share a common distribution on the topics, or have just slight variations of the topic coverage distribution. We add a network-induced regularizer to the likelihood objective function, as shown in Figure 19.10. That is, instead of optimizing the probability p(TextData | ), we will optimize another function f , which combines the likelihood function p(TextData | ) with a regularizer r( , Network) defined based on whatever preferences we can derive from the network context. When optimizing the new objective function f , we would seek a compromise between parameter values that maximize the likelihood and those that satisfy our regularization constraints or preferences. Thus we may also view the impact of the network context as imposing some prior on the model parameters if we view the new optimization problem conceptually as Bayesian inference of parameter values, even though we do not have any explicitly defined prior distribution of parameters. Note that such an idea of regularizing likelihood function is quite general; indeed, the probabilistic model can be any generative model for text (such as a language model), and the network can be also any network or graph that connects the text objects that we hope to analyze. The regularizer can also be any regularizer that we would like to use to capture different heuristics suitable for a particular application; it may even be a combination of multiple regularizers. Finally, the function f can also vary, allowing for many different ways to combine the likelihood function with the regularizers. Another variation is to specify separate constraints that must be satisfied based on network context, making a constrained optimization problem. Although the idea is quite general, in practice, the challenge often lies in how to instantiate such a general idea with specific regularizers so as to make the optimization problem remain tractable. Below we introduce a specific instantiation called NetPLSA (shown in Figure 19.11), which is an extension of PLSA to incorporate network context by implementing the heuristic that the neighbors on the network must have similar topic distributions. As shown in Figure 19.11, the new modified objective function is a weighted sum of the standard PLSA likelihood function and a regularizer where the parameter λ ∈ [0, 1] controls the weight on the regularizer. Clearly, if λ = 0, the model reduces to the standard PLSA. In the regularizer, we see that the main constraint is the  