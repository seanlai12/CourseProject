As mentioned previously, document clustering groups documents together into clusters. We can categorize document clustering methods into two categories. Similarity-based clustering . These clustering algorithms need a similarity function to work. Any two objects have the potential to be similar, depending on how they are viewed. Therefore, a user must define the similarity in some way. Agglomerative clustering is a "bottom up" approach, also called hierarchical clustering. In this approach, we gradually merge similar objects to generate clusters. Divisive clustering is a "top down" approach. In this approach, we gradually divide the whole set of objects into smaller clusters. For both above methods, each document can only belong to one cluster. This is a "hard assignment," unlike the clusters we receive from a model-based method. Model-based techniques design a probabilistic model to capture the latent structure of data (i.e., features in documents), and fit the model to data to obtain clusters. Typically, this is an example of soft clustering, since one object can be in multiple clusters (with a certain probability). There will be much more discussion on this in the topic analysis chapter. Term clustering has applications in query expansion. It allows similar terms to be added to the query, increasing the possible number of documents matched from the index. It also allows humans to understand advanced features more easily if there are many hundreds or thousands of them, or if they are hard to conceptualize. Later, we will first discuss term clustering using semantic relatedness via topic language models mentioned in Chapter 2. Then, we explore a simple probabilistic technique called pointwise mutual information. We briefly mention a hierarchical technique for term clustering called Brown clustering. This technique has similarity to the agglomerative document clustering along with the probabilistic nature of model-based methods. We finish term clustering with an explanation of word vectors, a context-based word representation that should remind you of the information retrieval problem setup. As we will see in Chapter 17, output from a model-based topic analysis additionally gives us groups of similar words (in fact, these are the "topics"). Thus, topic analysis delivers both term and document clusters to the user. Although with an unsupervised clustering algorithm, we generally do not provide any prior expectations as to what our clusters may contain, it is also possible to provide some supervision in the form of requiring two objects to be in the same cluster or not to be in the same cluster. Such supervision is useful when we have some knowledge about the clustering problem that we would like to incorporate into a clustering algorithm and allows users to "steer" the clustering in a flexible way. A user can also control the number of clusters by setting the number of clusters desired beforehand, or the user may leave it to the algorithm to determine what a natural breakdown of our objects is, in which case the number of clusters is usually optimized based on some statistical measures such as how well the data can be explained by a certain number of clusters. Most clustering output does not give labels for the clusters found; it's up to the user to examine the groups of terms or documents and mentally assign a label such as "biology" or "architecture." However, there are also approaches to automate assignment of a label to a text cluster where a label is often a phrase or multiple phrases [Mei et al. 2007b]. This labeling task can be regarded as a form of text summarization which we will further discuss in Chapter 16. Finally, a brief note on the implementation of clustering algorithms. As with the rest of the chapters in this part of the book, we will see that the information retrieval techniques that we discussed in Part II are often also very useful for implementing many other algorithms for text analysis, including clustering. For example, in the case of document clustering, we may assume we already have a forward index of tokenized documents according to some feature representation. Leveraging the 14.2 Document Clustering 279 data structures already in place for supporting search is especially desirable in a unified software system for supporting both text data access and text analysis. The clustering techniques we discuss are general, so they can be potentially used for clustering many different types of objects, including, e.g., unigram words, bigram words, trigram POS-tags, or syntactic tree features. All the clustering algorithms need are a term vocabulary represented as term IDs. The clustering algorithms only care about term occurrences and probabilities, not what they actually represent. Thus-with the same clustering algorithm-we can cluster documents by their word usage or by similar stylistic patterns represented as grammatical parse tree segments. For term clustering, we may not use an index, but we do also assume that each sentence or document is tokenized and term IDs are assigned. 