Note that the second term on the right-hand side of the formula is a querydependent constant, or more specifically, the entropy of the query model θ Q . It can be ignored for the purpose of ranking documents. In general, the computation of the above formula involves a sum over all the words that have a non-zero probability according to p(w | θ Q ). However, when θ D is based on certain general smoothing method, the computation would only involve a sum over those that both have a non-zero probability according to p(w | θ Q ) and occur in document d. Such a sum can be computed much more efficiently with an inverted index. We now explain this in detail. The general smoothing scheme we assume is the following: where p s (w | d) is the smoothed probability of a word seen in the document, p(w | C) is the collection language model, and α d is a coefficient controlling the probability mass assigned to unseen words, so that all probabilities sum to one. In general, Thus, individual smoothing methods essentially differ in their choice of p s (w | d). The collection language model p(w | C) is typically estimated by c(w, C) , or a smoothed version , where V is an estimated vocabulary size (e.g., the total number of distinct words in the collection). One advantage of the smoothed version is that it would never give a zero probability to any term, but in terms of retrieval performance, there will not be any significant difference in these two versions, since w c(w , C) is often significantly larger than V . It can be shown that with such a smoothing scheme, the KL-divergence scoring formula is essentially (the two sides are equivalent for ranking documents) Note that the scoring is now based on a sum over all the terms that both have a non-zero probability according to p(w | θ Q ) and occur in the document, i.e., all "matched" terms. 