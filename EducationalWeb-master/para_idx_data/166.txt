Recall our discussion about the F 1 score. In this situation, is an arithmetic mean of average precisions acceptable? We concluded before that the arithmetic mean of precision and recall was not as good as the harmonic mean. Here, we have a similar situation: we should think about the alternative ways of aggregating the average precisions. Another way is the geometric mean; using the geometric mean to consolidate the average precisions is called geometric mean average precision, or gMAP for short. We define it below mathematically as 3) or in log space as Imagine you are again testing a new algorithm. You've tested multiple topics (queries) and have the average precision for each topic. You wish to consider the overall performance, but which strategy would you use? Can you think of scenarios where using one of them would make a difference? That is, is there a situation where one measure would give different rankings of the two methods? Similar to our argument about F 1 , we realize in the arithmetic mean the sum is dominated by large values. Here, a large value means that the query is relatively easy. On the other hand, gMAP tends to be affected more by low values-those are the queries that don't have good performance (the average precision is low). If you wish to improve the search engine for those difficult queries, then gMAP would be preferred. If you just want to improve over all kinds of queries, then perhaps MAP would be preferred. So again, the answer depends on your users' tasks and preferences. Which measure is most likely going to represent your users' needs? As a special case of the mean average precision, we can also think about the case where there is precisely one relevant document in the entire collection. This actually happens quite often, for example, in what's called a known item search, where you know a target page such as Amazon or Facebook. Or in another application such as question answering, there is only one answer. In this scenario, if you rank the answers, then your goal is to rank that one particular answer on top. In this case, the average precision will boil down to the reciprocal rank. That is, 1 r where r is the position (rank) of the single relevant document. If that document is ranked on the very top, then the reciprocal rank would be 1 1 = 1. If it's ranked at the second 180 Chapter 9 Search Engine Evaluation position, then it's 1 2 and so on. This means we can also take an average of all the reciprocal ranks over a set of topics, which gives us the mean reciprocal rank (MRR). It's a very popular measure for known item search or any problem where you have just one relevant item. We can see this r is quite meaningful; it indicates how much effort a user would have to make in order to find that one relevant document. If it's ranked on the top it's low effort; if it's ranked at 100 then you actually have to (presumably) sift through 100 documents in order to find it. Thus, r is also a meaningful measure and the reciprocal rank will take the reciprocal of r instead of using it directly. The usual question also applies here: Why not just simply use r? If you were to design a ratio to measure the performance of a system where there is only one relevant item, you might have thought about using r directly as the measure. After all, that measures the user's effort, right? But, think about if you take an average of this over a large number of topics. Again, it would make a difference. For one single topic, using r or using 1 r wouldn't make any difference. A larger r corresponds to a small 1 r . The difference appears when there are many topics. Just like MAP, this sum will be dominated by large values of r. So what are those values? Those are basically large values that indicate lower ranked results. That means the relevant items rank very low down on the list. The average would then be dominated by the relevant documents that are ranked in the lower portion of the list. From a user's perspective we care more about the highly ranked documents, so by taking this transformation by using reciprocal rank we emphasize more on the difference on the top. Think about the difference between rank one and rank two and the difference between rank 100 and 1000 using each method. Is one more preferable than the other? In summary, we showed that the precision-recall curve can characterize the overall accuracy of a ranked list. We emphasized that the actual utility of a ranked list depends on how many top ranked results a user would examine; some users will examine more than others. Average precision is a standard measure for comparing two ranking methods; it combines precision and recall while being sensitive to the rank of every relevant document. We concluded this section with three methods to summarize multiple average precision values: MAP, gMAP, and MRR. 