This can be accomplished with α = 4, β = 1, or α = 16, β = 4, or even α = 0.4, β = 0.1. But what is the difference? Figure A.1 shows a comparison of the Beta distribution with varying parameters. It's also important to remember that a draw from a Beta prior θ ∼ Beta(α, β) gives us a distribution. Even though it's a single value on the range [0, 1], we are still using the prior to produce a probability distribution. Perhaps we'd like to choose a unimodal Beta prior, with a mean 0.8. As we can see from Figure A.1, the higher we set α and β, the sharper the peak at 0.8 will be. Looking at our parameter estimation, we can imagine the hyperparameters as pseudo counts-counts from the outcome of experiments already performed. The higher the hyperparameters are, the more pseudo counts we have, which means our prior is "stronger." As the total number of experiments increases, the sum H + T also increases, which means we have less dependence on our priors. Initially, though, when H + T is relatively low, our prior plays a stronger role in the estimation of θ . As we all know, a small number of flips will not give an accurate estimate of the true θ -we'd like to see what our estimate becomes as our number of flips approaches infinity (or some "large enough" value). In this sense, our prior also smooths our estimation. Rather than the estimate fluctuating greatly initially, it could stay relatively smooth if we have a decent prior. If our prior turns out to be incorrect, eventually the observed data will overshadow the pseudo counts from the hyperparameters anyway, since α and β are held constant. 