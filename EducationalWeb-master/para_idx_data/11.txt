As we will see later in this chapter and in many other chapters, probabilistic or statistical models play a very important role in text mining algorithms. This section gives every reader a sufficient background and vocabulary to understand these probabilistic and statistical approaches covered in the later chapters of the book. A probability distribution is a way to assign likelihood to an event in some probability space . As an example, let our probability space be a six-sided die. Each side has a different color. Thus, = {red, orange, yellow, green, blue, purple} and an event is the act of rolling the die and observing a color. We can quantify the uncertainty of rolling the die by declaring a probability distribution over all possible events. Assuming we have a fair die, the probability of rolling any specific color is 1 6 , or about 16%. We can represent our probability distribution as a collection of probabilities such as where the first index corresponds to p(red) = 1 6 , the second index corresponds to p(orange) = 1 6 , and so on. But what if we had an unfair die? We could use a different probability distribution θ to model events concerning it: θ = 1 3 , 1 3 , 1 12 , 1 12 , 1 12 , 1 12 . In this case, red and orange are assumed to be rolled more often than the other colors. Be careful to note the difference between the sample space and the defined probability model θ used to quantify its uncertainty. In our text mining tasks, we usually try to estimate θ given some knowledge about . The different methods to estimate θ will determine how accurate or useful the probabilistic model is. Consider the following notation: x ∼ θ . We read this as x is drawn from theta, or the random variable x is drawn from the probability distribution θ . The random variable x takes on each value from with a certain probability defined by θ . For example, if we had x ∼ θ , then there is a 2 3 chance that x is either red or orange. In our text application tasks, we usually have as V , the vocabulary of some text corpus. For example, the vocabulary could be V = {a, and, apple, . . . , zap, zirconium, zoo} and we could model the text data with a probability distribution θ . Thus, if we have some word w we can write p(w | θ) (read as the probability of w given θ). If w is the word data, we might have p(w = data | θ) = 0.003 or equivalently p θ (w = data) = 0.003. In our examples, we have only considered discrete probability distributions. That is, our models only assign probabilities for a finite (discrete) set of outcomes. In reality, there are also continuous probability distributions, where there are an infinite number of "events" that are not countable. For example, the normal (or Gaussian) distribution is a continuous probability distribution that assigns realvalued probabilities according to some parameters. We will discuss continuous distributions as necessary in later chapters. For now, though, it's sufficient to understand that we can use a discrete probability distribution to model the probability of observing a single word in a vocabulary V . 