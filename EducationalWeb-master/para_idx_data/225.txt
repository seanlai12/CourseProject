Mutual information is always non-negative. This is easy to understand because the original entropy is always not going to be lower than the (possibly) reduced conditional entropy. In other words, the conditional entropy will never exceed the original entropy; knowing some information can always help us potentially, but will not hurt us in predicting X. Another property is that mutual information is symmetric: I (X; Y ) = I (Y ; X). A third property is that it reaches its minimum, zero, if and only if the two random variables are completely independent. That means knowing one of them does not tell us anything about the other. When we fix X to rank different Y s using conditional entropy, we would get the same order as ranking based on mutual information. Thus, ranking based on mutual entropy is exactly the same as ranking based on the conditional entropy of X given Y , but the mutual information allows us to compare different pairs of X and Y . That is why mutual information is more general and more useful. Let's examine the intuition of using mutual information for syntagmatic relation mining in Figure 13.10. The question we ask is: whenever eats occurs, what other words also tend to occur? This question can be framed as a mutual information question; that is, which words have high mutual information with eats? So, we need to compute the mutual information between eats and other words. For example, we know the mutual information between eats and meat, which is the same as between meat and eats because the mutual information is symmetric. This is expected to be higher than the mutual information between eats and the, because knowing the does not really help us predict the other word. You also can easily see that the mutual information between a word and itself is the largest, which is equal to the entropy of the word. In that case, the reduction is maximum because knowing one allows us to predict the other completely. In other words, the conditional entropy is zero which means mutual information reaches its maximum. 