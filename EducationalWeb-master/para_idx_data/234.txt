The idea here is that if it's easy to spot the intruder, the top k words form a coherent group, meaning that they are a very good word association group. In L 1 , it's quite obvious that apple is the intruder since it doesn't fit in with the other words in the list. Thus, the remaining words form a good word association list. In L 2 , we can't really tell which word is the intruder, meaning the word association algorithm used to create the k candidates in L 2 is not as good as the one used to generate L 1 . Performing this type of experiment over many different words in the vocabulary is a good (yet expensive) way to strictly evaluate the word associations. We say this method is expensive since it requires many human judgements. Finally, it's important to consider the time-accuracy tradeoff of using such a tool in word association mining. Imagine the scenario where we have a baseline system with a MAP of 0.89 on some dataset. If we use query expansion via word association mining, we can get a statistically significantly higher MAP of 0.90. However, this doesn't take into account the preprocessing time of mining the word associations. In this example, the query time is not affected because the word association mining takes place beforehand offline, but it still is a non-negligible cost. The application manager would have to decide whether an increase in MAP of 0.01 is worth the effort of implementing, running, and maintaining the query expansion program. This is actually quite a realistic and general issue whenever new technology is proposed to replace or extend an existing one. As a data scientist, it is often part of the job to Exercises 273 convince others that such modifications are useful and worthwhile to the overall system. Manning and Sch√ºtze [1999] has two useful relevant chapters on the discovery of word associations: Chapter 5 (Collocations) and Chapter 8 (Lexical Acquisition). An early reference on the use of mutual information for discovering word associations is Church and Hanks [1990]. Both paradigmatic and syntagmatic relations can also be discovered using random walks defined on word adjacency graphs, and a unified framework for modeling both kinds of word associations was proposed in Jiang and Zhai [2014]. Non-compositional phrases (also called lexical atoms) such as hot dog can also be discovered using similar heuristics to what we have discussed in this chapter (see Zhai 1997, Lin 1999. Another approach to word association discovery is the n-gram class language model [Brown et al. 1992]. Recently, word embedding techniques (e.g., word2vec; Mikolov et al. 2013) have shown great promise for learning a vector representation of a word that can further enable computation of similarity between two words, thus directly supporting paradigmatic relation discovery. Both the n-gram class language model and word2vec are briefly discussed in the context of term clustering in Chapter 14.  