In our forward index, we'd store {1, 1, 1, 1, 1, 2, 1, 1} x j = {0, 0, 0, 1, 0, 0, 2, 0}, so x ik is the k th term in the i th document. We also have Y, which is a vector of labels for each document. Thus y i may be sports in our news article classification setup and y j could be politics. A classifier is a function f ( . ) that takes a document vector as input and outputs a predicted labelŷ ∈ Y. Thus we could have f (x i ) = sports. In this case,ŷ = sports and the true y is also sports; the classifier was correct in its prediction. Notice how we can only evaluate a classification algorithm if we know the true labels of the data. In fact, we will have to use the true labels in order to learn a good function f ( . ) to take unseen document vectors and classify them. For this reason, we often split our corpus X into two parts: training data and testing data. The training portion is used to build the classifier, and the testing portion is used to evaluate the performance (e.g., seeing how many correct labels were predicted). But what does the function f ( . ) actually do? Consider this very simple example that determines whether a news article has positive or negative sentiment, i.e., Y = {positive, negative}: positive if x's count for the term good is greater than 1 negative otherwise. Of course, this example is overly simplified, but it does demonstrate the basic idea of a classifier: it takes a document vector as input and outputs a class label. Based on the training data, the classifier may have determined that positive sentiment articles contain the term good more than once; therefore, this knowledge is encoded in the function. Later in this chapter, we will investigate some specific algorithms for creating the function f ( . ) based on the training data. It's also important to note that these learning algorithms come in several different flavors. In binary classification there are only two categories. Depending on the type of classifier, it may only support distinguishing between two different classes. Multiclass classification can support an arbitrary number of labels. As we will see, it's possible to combine multiple binary classifiers to create a multiclass classifier. Regression is a very related problem to classification; it assigns real-valued scores on some range as opposed to discrete labels. For example, a regression problem could be to predict the amount of rainfall for a particular day given rainfall data for previous years. The outputŷ would be a number ≥ 0, perhaps representing rainfall 304 Chapter 15 Text Categorization in inches. On the other hand, the classification variant could predict whether there would be rainfall or not, Y = {yes, no}. 