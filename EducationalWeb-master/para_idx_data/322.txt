In this section, we will discuss how to estimate the parameters of a mixture model. We will start with the simplest scenario where one component (the background) is already completely known, and the topic choice distribution has an equal probability of choosing either the background or the topic word distribution. Our goal is to estimate the unknown topic word distribution where we hope to not see common words with high probabilities. A main assumption is that those common words are generated using the background model, while the more discriminative content-bearing words are generated using the (unknown) topic word distribution,  as illustrated in Figure 17.17. This is also the scenario that we used to motivate the use of the mixture model. Figure 17.18 illustrates such a scenario. In this scenario, the only parameters unknown would be the topic word distribution p(w | θ d ). Thus, we have exactly the same number of parameters to estimate as in the case of a single unigram language model. Note that this is an example of customizing a general probabilistic model so that we can embed an unknown variable that we are interested in computing, while simplifying other parts of the model based on certain assumptions that we can make about them. That is, we assume that we have knowledge about other variables. Setting the background model to a fixed word distribution based on the maximum likelihood estimate of a unigram language model of a large sample of English text is not only feasible, but also desirable since our goal of designing such a generative model is to factor out the common words from the topic word distribution to be estimated. Feeding the model with a known background word distribution is a powerful technique to inject our knowledge about what words are counted as noise (stop words in this case). Similarly, the parameter p(θ B ) can also be set based on our desired percentage of common words to factor out; the larger p(θ B ) is set, the more common words would be removed from the topic word distribution. It's easy to see that if p(θ B ) = 0, then we would not be able to remove any common words as the model degenerates to the simple case of using just one distribution (to explain all the words). Note that we could have assumed that both θ B and θ d are unknown, and we can also estimate both by using the maximum likelihood estimation, but in such a case, we would no longer be able to guarantee that we will obtain a distribution θ B that 17.3 Mining One Topic from Text 353  assigns high probabilities to common words. For our application scenario (i.e., factoring out common words), it is more appropriate to pre-set the background word distribution to bias the model toward allocating the common words to the background word distribution, and thus allow the topic word distribution to focus more on the content words as we will further explain. If we view the mixture model in Figure 17.18 as a black box, we would notice that it actually now has exactly the same number of parameters (indeed, the same parameters) as the simplest single unigram language model. However, the mixture model gives us a different likelihood function which intuitively requires θ d to work together optimally with the fixed background model θ B to best explain the observed document. It might not be obvious why the constraint of "working together" with the given background model would have the effect of factoring out the common words from θ d as it would require understanding the behavior of parameter estimation in the case of a mixture model, which we explain in the next section. 