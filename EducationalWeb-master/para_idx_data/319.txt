assign high probabilities to such content-bearing words rather than the common function words in English. The assumed process for generating a word with such a mixture model is just slightly different from the generation process of our simplest unigram language. Since we now have two distributions, we have to decide which distribution to use when we generate the word, but each word will still be a sample from one of the two distributions. The text data are still generated in the same way, by generating one word at a time. More specifically, when we generate a word, we first decide which of the two distributions to use. This is controlled by a new probability distribution over the choices of the component models to use (two choices in our case), including specifically the probability of θ d (using the unknown topic model) and the probability of θ B (using the known background model). Thus, p(θ d ) + p(θ B ) = 1. In the figure, we see that both p(θ d ) and p(θ B ) are set to 0.5. This means that we can imagine flipping a fair coin to decide which distribution to use, although in general these probabilities don't have to be equal; one topic could be more likely than another. The process of generating a word from such a mixture model is as follows. First, we flip a biased coin which would show up as heads with probability p(θ d ) (and thus as tails with probability p(θ B ) = 1 − p(θ d )) to decide which word distribution to use. If the coin shows up as heads, we would use θ d ; otherwise, θ B . We will use the chosen word distribution to generate a word. This means if we are to use θ d , we would sample a word using p(w | θ d ), otherwise using p(w | θ B ), as illustrated in Figure 17.13. We now have a generative model that has some uncertainty associated with the use of which word distribution to generate a word. If we treat the whole generative model as a black box, the model would behave very similarly to our simplest topic model where we only use one word distribution in that the model would specify a distribution over sequences of words. We can thus examine the probability of observing any particular word from such a mixture model, and compute the probability of observing a sequence of words. Let's assume that we have a mixture model as shown in Figure 17.13 and consider two specific words, the and text. What's the probability of observing a word like the from the mixture model? Note that there are two ways to generate the, so the probability is intuitively a sum of the probability of observing the in each case. What's the probability of observing the being generated using the background model? In order for the to be generated in this way, we must have first chosen to use the background model, and then obtained the word the when sampling 348 Chapter 17 Topic Analysis a word from the background language model p(w | θ B ). Thus, the probability of observing the from the background model is p(θ B )p(the | θ B ), and the probability of observing the from the mixture model regardless of which distribution we use would be p(θ B )p(the | θ B ) + p(θ d )p(the | θ d ), as shown in Figure 17.14, where we also show how to compute the probability of text. It is not hard to generalize the calculation to compute the probability of observing any word w from such a mixture model, which would be (17.4) The sum is over the two different ways to generate the word, corresponding to using each of the two distributions. Each term in the sum captures the probability of observing the word from one of the two distributions. For example, p(θ B )p(w | θ B ) gives the probability of observing word w from the background language model. The product is due to the fact that in order to observe word w, we must have (1) decided to use the background distribution (which has the probability of p(θ B )), and (2) obtained word w from the distribution θ B (which has the probability of p(w | θ B )). Both events must happen in order to observe word w from the background distribution, thus we multiply their probabilities to obtain the probability of observing w from the background distribution. Similarly, p(θ d )p(w | θ d ) gives the probability of observing word w from the topic word distribution. Adding them together gives us the total probability of observing w regardless which distribution has actually been used to generate the word. Such a form of likelihood actually reflects some general characteristics of the likelihood function of any mixture model. First, the probability of observing a data point from a mixture model is a sum over different ways of generating the word, each corresponding to using a different component model in the mixture model. Second, each term in the sum is a product of two probabilities: one is the probability of selecting the component model corresponding to the term, while 