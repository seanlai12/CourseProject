Choose an instance of C j according to its probability density function, f j . The data generation process here is the basic assumption in mixture models. Formally, a mixture model assumes that a set of observed objects is a mixture of instances from multiple probabilistic clusters. Conceptually, each observed object is generated independently by two steps: first choosing a probabilistic cluster according to the probabilities of the clusters, and then choosing a sample according to the probability density function of the chosen cluster. Given data set, D, and k, the number of clusters required, the task of probabilistic model-based cluster analysis is to infer a set of k probabilistic clusters that is most likely to generate D using this data generation process. An important question remaining is how we can measure the likelihood that a set of k probabilistic clusters and their probabilities will generate an observed data set. Consider a set, C, of k probabilistic clusters, C 1 , . . . , C k , with probability density functions f 1 , . . . , f k , respectively, and their probabilities, ω 1 , . . . , ω k . For an object, o, the probability that o is generated by cluster C j (1 ≤ j ≤ k) is given by P(o|C j ) = ω j f j (o). Therefore, the probability that o is generated by the set C of clusters is (11.5) Since the objects are assumed to have been generated independently, for a data set, D = {o 1 , . . . , o n }, of n objects, we have Now, it is clear that the task of probabilistic model-based cluster analysis on a data set, D, is to find a set C of k probabilistic clusters such that P(D|C) is maximized. Maximizing P(D|C) is often intractable because, in general, the probability density function of a cluster can take an arbitrarily complicated form. To make probabilistic model-based clusters computationally feasible, we often compromise by assuming that the probability density functions are parameterized distributions. Formally, let o 1 , . . . , o n be the n observed objects, and 1 , . . . , k be the parameters of the k distributions, denoted by O = {o 1 , . . . , o n } and = { 1 , . . . , k }, respectively. Then, for any object, o i ∈ O (1 ≤ i ≤ n), Eq. (11.5) can be rewritten as where P j (o i | j ) is the probability that o i is generated from the jth distribution using parameter j . Consequently, Eq. (11.6) can be rewritten as Using the parameterized probability distribution models, the task of probabilistic model-based cluster analysis is to infer a set of parameters, , that maximizes Eq. (11.8). Example 11.6 Univariate Gaussian mixture model. Let's use univariate Gaussian distributions as an example. That is, we assume that the probability density function of each cluster follows a 1-D Gaussian distribution. Suppose there are k clusters. The two parameters for the probability density function of each cluster are center, µ j , and standard deviation, σ j (1 ≤ j ≤ k). We denote the parameters as j = (µ j , σ j ) and = { 1 , . . .  (11.9) Assuming that each cluster has the same probability, that is ω 1 = ω 2 = · · · = ω k = 1 k , and plugging Eq. (11.9) into Eq. (11.7), we have (11.10) Applying Eq. (11.8), we have (11.11) The task of probabilistic model-based cluster analysis using a univariate Gaussian mixture model is to infer such that Eq. (11.11) is maximized. 11.1 Probabilistic Model-Based Clustering 505 