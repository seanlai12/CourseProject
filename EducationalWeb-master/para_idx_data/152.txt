The double barrel LRU cache was originally used in the popular Lucene search engine. 2 It is a simplified approximation of the LRU cache. Figure 8.4 shows a DBLRU cache with size six. The DBLRU cache is just two hash tables named primary and secondary. The algorithm is as follows, assuming we want to retrieve the postings list for term ID x. . First, search primary for term ID x; if it exists, return it. . If it's not in primary, search secondary. . If it's in secondary, delete it. Insert it in primary and return it. If this causes primary to reach the maximum size, clear the entire contents of secondary. Then, swap the two hash tables. . If it's not in secondary, retrieve it from disk and insert it into secondary. This cache has a rough hierarchy of usage: primary contains elements that are more frequently accessed than secondary. So when the cache fills, the secondary table is emptied to free memory. While the temporal accuracy of the DBLRU cache is not as precise as the LRU cache, it is a much simpler setup, which translates to faster access times. As usual, there is a tradeoff between the speed and accuracy of these two caches. A classic reference book for the implementation of search engines is the book Managing Gigabytes [Witten et al. 1999]. Other books on information retrieval, such as Introduction to Information Retrieval [Manning et al. 2008], and Search Engines: Information Retrieval in Practice [Croft et al. 2009], and Information Retrieval: Implementing and Evaluating Search Engines [BÃ¼ttcher et al. 2010] also have an excellent coverage of implementations of search engines. This chapter focuses on the evaluation of text retrieval (TR) systems. In the previous chapter, we talked about a number of different TR methods and ranking functions, but how do we know which one works the best? In order to answer this question, we have to compare them, and that means we'll have to evaluate these retrieval methods. This is the main focus of this chapter. We start out with the methodology behind evaluation. Then, we compare the retrieval of sets with the retrieval of ranked lists as well as judgements with multiple levels of relevance. We end with practical issues in evaluation, followed by exercises. 