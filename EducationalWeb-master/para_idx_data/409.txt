This appendix is a more detailed discussion of the KL-divergence function and its relation to Dirichlet prior smoothing in the generalized query likelihood smoothing framework. We briefly touched upon KL-divergence in Chapter 7 and Chapter 13. As we have seen, given two probability mass functions p(x) and q(x), D(p q), the Kullback-Leibler divergence (or relative entropy) between p and q is defined as It is easy to show that D(p q) is always non-negative and is zero if and only if p = q. Even though it is not a true distance between distributions (because it is not symmetric and does not satisfy the triangle inequality), it is still often useful to think of the KL-divergence as a "distance" between distributions [Cover and Thomas 1991]. 