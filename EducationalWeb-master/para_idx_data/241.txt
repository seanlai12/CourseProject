In this section, we examine similarity-based document clustering through two methods: agglomerative clustering and divisive clustering. As these are both similarity-based clustering methods, a similarity measure is required. In case a refresh of similarity measures is required, we suggest the reader consult Chapter 6. In particular, the similarity algorithms we use for clustering need to be symmetric; that is, sim(d 1 , d 2 ) must be equal to sim(d 2 , d 1 ). Furthermore, our similarity algorithm must be normalized on some range. Usually, this range is [0, 1]. These constraints ensure that we can fairly compare similarity scores of different pairs of objects. Most retrieval formulas we have seen-such as BM25, pivoted length normalization, and query likelihood methods-are asymmetric since they treat the query differently from the current document being scored. Whissell and Clarke [2013] explore symmetric versions of popular retrieval formulas and they show that they are quite effective. Despite the fact that default query-document similarity measures are not used for clustering, it is possible to use (for example) Okapi BM25 term weighting in document vectors which are then scored with a simple symmetric similarity algorithm like cosine similarity. Recall that cosine similarity is defined as Since all term weights in our document vector representation are positive, the cosine similarity score ranges from [0, 1]. As mentioned, the term weights may be raw counts, TF-IDF, or anything else the user could imagine. The cosine similarity captures the cosine of the angle between the two document vectors plotted in 280 Chapter 14 Text Clustering their high-dimensional space; the larger the angle, the more dissimilar the documents are. Another common similarity metric is Jaccard similarity. This metric is a set similarity; that is, it only captures the presence and absence of terms with no regard to magnitude. It is defined as follows: where X and Y represent the set of elements in the document vector x and y, respectively. In plain English, it captures the ratio of shared objects and total objects in both sets. For a more in-depth study of similarity measures and their effectiveness, we suggest that the reader consult Huang [2008]. For the rest of this chapter, it is sufficient to assume that the base document-document similarity measure is cosine or Jaccard similarity. In any event, the goal of a particular similarity algorithm is to find an optimal partitioning of data to simultaneously maximize intra-group similarity and minimize inter-group similarity. 