Segment_4 â€¦ Segment_N count(w1) = total number segments that contain w1 count(w2) = total number segments that contain w2 count(w1, w2) = total number segments that contain both w1 and w2 the maximum likelihood estimate (MLE), where we simply normalize the observed counts. Using MLE, we can compute these probabilities as follows. For estimating the probability that we see a word occuring in a segment, we simply normalize the count of segments that contain this word. On the right side of Figure 13.14, you see a list of some segments of data. In some segments you see both words occur, which is indicated as ones for both columns. In some other cases only one will occur, so only that column has a one and the other column has a zero. To estimate these probabilities, we simply need to collect the three counts: the count of w 1 (the total number of segments that contain w 1 ), the segment count for w 2 , and the count when both words occur (both columns have ones). Once we have these counts, we can just normalize these counts by N , which is the total number of segments, giving us the probabilities that we need to compute mutual information. There is a small problem when we have zero counts sometimes. In this case, we don't want a zero probability, so we use smoothing, as discussed previously in this book. To smooth, we will add a small constant to these counts so that we don't get zero probability in any case. Smoothing for this application is displayed in Figure 13.15. We pretend to observe pseudo-segments that would contribute additional counts of these words so that no event will have zero probability. In particular for this example, we introduce four pseudo-segments. Each is weighted at 1/4. These represent the four different combinations of occurrences of the two words. Each combination will have at least a non-zero count from a pseudo-segment; thus, in the actual segments that we'll observe, it's okay if we haven't observed all of 270 Chapter 13 Word Association Mining p(X w1 = 1) = Smoothing: Add pseudo data so that no event has zero counts (pretend we observed extra data) the combinations. More specifically, you can see the 0.5 coming from the two ones in the two pseudo-segments, because each is weighted at one quarter. If we add them up, we get 0.5. Similar to this, 0.25 comes from one single pseudo-segment that indicates the two words occur together. In the denominator, we add the total number of pseudo-segments, which in this case is four pseudo-segments. To summarize, syntagmatic relations can generally be discovered by measuring correlations between occurrences of two words. We've used three concepts from information theory: entropy, which measures the uncertainty of a random variable X; conditional entropy, which measures the entropy of X given we know Y ; and mutual information of X and Y , which matches the entropy reduction of X due to knowing Y , or entropy reduction of Y due to knowing X. These three concepts are actually very useful for other applications as well. Mutual information allows us to have values computed on different pairs of words that are comparable, allowing us to rank these pairs and discover the strongest syntagmatic relations from a collection of documents. Note that there is some relation between syntagmatic relation discovery and paradigmatic relation discovery. We already discussed the possibility of using BM25 to weight terms in the context and suggest candidates that have syntagmatic relations with the target word. Here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. This would give us another way to represent the context of a word. And if we do the same for all the words, then we can cluster these words or compute 