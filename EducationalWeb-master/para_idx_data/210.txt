what the user has said in text data (e.g., reviews), in which case, the results from mining knowledge about the observer would clearly be very useful for prediction. Futhermore, non-text data can be very important in predictive analysis. For example, if you want to predict stock prices or changes of stock prices, the historical stock price data are presumably the best data to use for prediction even though online discussions, news articles, or social media, may also be useful for further improvement of prediction accuracy by contributing additional effective features computed based on text data (which would be combined with non-text features). Non-text data can also be used for analyzing text by supplying context, thus opening up many interesting opportunities to mine context-sensitive knowledge from text data, i.e., associating the knowledge discovered from text data with the non-text data (e.g., associating topics discovered from text with time would generate temporal trends of topics). When we look at the text data alone, we'll be mostly looking at the content or opinions expressed in the text. However, text data generally also has context associated with it. For example, the time and the location of the production of the text data are both useful "metadata" values of a text document. This context can provide interesting angles for analyzing text data; we might partition text data into different time periods because of the availability of the time. Now, we can analyze text data in each time period and make a comparison. Similarly, we can partition text data based on location or any other metadata that's associated with it to form interesting comparisons in those areas. In this sense, non-text data can provide interesting angles or perspectives for text data analysis. It can help us make context-sensitive analysis of content, language usage, or opinions about the observer or the authors of text data. We discuss joint analysis of text and non-text data in detail in Chapter 19. This is a fairly general landscape of the topics in text mining and analytics. In this book, we will selectively cover some of those topics that are representative of the different kinds of text mining tasks. Chapters 2 and 3 already covered natural language processing and the basics of machine learning, which allow us to understand, represent, and classify text data-important steps in any text mining task. In the remaining chapters of Part III of the book, we will start to enumerate different text mining tasks that build upon the NLP and IR techniques discussed earlier. First, we will discuss how to mine word associations from text data (Chapter 13), revealing lexical knowledge about language. After word association mining, we will 250 Chapter 12 Overview of Text Data Analysis look at clustering text objects (Chapter 14). This groups similar objects together, allowing exploratory analysis, among many other applications. Chapter 15 covers text categorization, which expands on the introduction to machine learning given in Chapter 2. We also explore different methods of text summarization (Chapter 16). Next, we'll discuss topic mining and analysis (Chapter 17). This is only one way to analyze content of text, but it's very useful and used in a wide array of applications. Then, we will introduce opinion mining and sentiment analysis. This can be regarded as one example of mining knowledge about the observer, and will be covered in Chapter 18. Finally, we will briefly discuss text-based prediction problems where we try to predict some real-world variable based on text data and present a number of cutting-edge research results on how to perform joint analysis of text and non-text data (Chapter 19). In this chapter, we're going to talk about how to mine associations of words from text. This is an example of knowledge about the natural language that we can mine from text data. We'll first talk about what word association is and then explain why discovering such relations is useful. Then, we'll discuss some general ideas about how to mine word associations. In general, there are two types of word relations; one is called a paradigmatic relation and the other is a syntagmatic relation. Word w a and w b have a paradigmatic relation if they can be substituted for each other. That means the two words that have paradigmatic relation would be in the same semantic class, or syntactic class. We can replace one with another without affecting the understanding of the sentence. Chapter 14 gives some additional ideas not discussed in this chapter about how to group similar terms together. As an example, the words cat and dog have a paradigmatic relation because they are in the same word class: animal. If you replace cat with dog in a sentence, the sentence would still be (mostly) comprehensible. Similarly, Monday and Tuesday have a paradigmatic relation. The second kind of relation is called a syntagmatic relation. In this case, the two words that have this relation can be combined with each other. Thus, w a and w b have a syntagmatic relation if they can be combined with each other in a grammatical sentence-meaning that these two words are semantically related. For example, cat and sit are related because a cat can sit somewhere (usually anywhere they please). Similarly, car and drive are related semantically because they can be combined with each other to convey some meaning. However, we cannot replace cat with sit in a sentence or car with drive in the sentence and still have a valid sentence. Therefore, the previous pairs of words have a syntagmatic relation and not a paradigmatic relation. These two relations are in fact so fundamental that they can be generalized to capture basic relations between units in arbitrary sequences. They can be generalized to describe relations of any items in a language; that is, w a and w b don't 252 Chapter 13 Word Association Mining have to be words. They could be phrases or entities. If you think about the general problem of sequence mining, then we can think about any units being words. We think of paradigmatic relations as relations that are applied to units that tend to occur in a similar location in a sentence (or a sequence of data elements in general). Syntagmatic relations capture co-occurring elements that tend to show up in the same sequence. So, these two measures are complimentary and we're interested in discovering them automatically from text data. Discovering such word relations has many applications. First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because these relations capture some knowledge about language. If you know two words are synonyms, for example, that would help with many different tasks. Grammar learning can be also done by using such techniques; if we can learn paradigmatic relations, then we can form classes of words. If we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions by learning the sentence structure. Word relations can be also very useful for many applications in text retrieval and mining. In search and text retrieval, we can use word associations to modify a query for feedback, making search more effective. As we saw in Chapter 7, this is often called query expansion. We can also use related words to suggest related queries to a user to explore the information space. Yet another application is to use word associations to automatically construct a hierarchy for browsing. We can have words as nodes and associations as edges, allowing a user to navigate from one word to another to find information. Finally, such word associations can also be used to compare and summarize opinions. We might be interested in understanding positive and negative opinions about a new smartphone. In order to do that, we can look at what words are most strongly associated with a feature word like battery in positive vs. negative reviews. Such syntagmatic relations would help us show the detailed opinions about the product. 