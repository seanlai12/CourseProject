document as a sequence of words. Each word here is denoted by x i . Our model is a unigram language model, i.e., a word distribution that denotes the latent topic that we hope to discover. Clearly, the model has as many parameters as the number of words in our vocabulary, which is M in this case. For convenience, we will use θ i to denote the probability of word w i . According to our model, the probabilities of all the words must sum to one: M i=1 θ i = 1. Next, we see that our likelihood function is the probability of generating this whole document according to our model. In a unigram language model, we assume independence in generating each word so the probability of the document equals the product of the probability of each word in the document (the first line of the equation for the likelihood function). We can rewrite this product into a slightly different form by grouping the terms corresponding to the same word together so that the product would be over all the distinct words in the vocabulary (instead of over all the positions of words in the document), which is shown in the second line of the equation for the likelihood function. Since some words might have repeated occurrences, when we use a product over the unique words we must also incorporate the count of a word w i in document d, which is denoted by c(w i , d). Although the product is taken over the entire vocabulary, it is clear that if a word did not occur in the document, it would have a zero count (c(w i , d) = 0), and that corresponding term would be essentially absent in the formula, thus the product is still essentially over the words that actually occurred in the document. We often prefer such a form of the likelihood function where the product is over the entire vocabulary because it is convenient for deriving formulas for parameter estimation. 