At this point, you may be able to rationalize how Dirichlet prior smoothing for information retrieval language models or topic models works. However, our probabilities are over words now, not just a binary heads or tails outcome. Before we talk about the Dirichlet distribution, let's figure out how to represent the probability of observing a word from a vocabulary. For this, we can use a categorical distribution. In a text information system, a categorical distribution could represent a unigram language model for a single document. Here, the total number of outcomes is k = |V |, the size of our vocabulary. The word at index i would have a probability p i of occurring, and the sum of all words' probabilities would sum to one. 