In this section, we will discuss the issue of document length normalization. So far in our exploration of the vector space model we considered the TF or the count of a term in a document. We have also considered the global statistic IDF. However, we have not considered the document length. In Figure 6.16, we show two example documents. Document d 4 is very short with only one hundred words. Conversely, d 6 has five thousand words. If you look at the matching of these query words we see that d 6 has many more matchings of the query words; one might reason that d 6 may have matched these query words in a scattered manner. Perhaps d 6 's topic is not really the same as the query's topic. In the beginning of d 6   nothing to do with the mention of presidential at the end. In general, if you think about long documents, they would have a higher chance to match any query since they contain more words. In fact, if you generate a long document by randomly sampling words from the distribution of all words, then eventually you probably will match any query! In this sense, we should penalize long documents because they naturally have a better chance to match any query. This is our idea of document length normalization. On the one hand, we want to penalize a long document, but on the other hand, we also don't want to over-penalize them. The reason is that a document may be long because of different reason: in one case the document may be longer because it uses more words. For example, think about a research paper article. It would use more words than the corresponding abstract. This is the case where we probably should penalize the matching of a long document such as a full paper. When we compare matching words in such long document with matching words in the short abstract, the long papers generally have a higher chance of matching query words. Therefore, we should penalize the long documents. However, there is another case when the document is long-that is when the document simply has more content. Consider a case of a long document, where we simply concatenated abstracts of different papers. In such a case, we don't want to penalize this long document. That's why we need to be careful about using the right degree of length penalization, and an understanding of the discourse structure of documents is needed for optimal document length normalization. A method that has worked well is called pivoted length normalization, illustrated in Figure 6.17 and described originally in Singhal et al. [1996]. Here, the idea is to 6.3 Vector Space Retrieval Models 107 0 1 2 Shorter than avdl 