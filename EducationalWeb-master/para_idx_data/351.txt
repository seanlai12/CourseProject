To compute the α i (d) values, we must use maximum a posteriori. This means that we maximize the product of the prior of α (according to our assumed multivariate Gaussian distribution) and the likelihood of the rating r d : The likelihood rating is the probability of generating this observed overall rating given this particular α value and some other parameters. For more details about this model, we refer the reader to Wang et al. [2010]. Earlier, we talked about how to solve the LARA problem in two stages. First, we did segmentation of different aspects, and then used a latent regression model to learn the aspect ratings and weights. It's also possible to develop a unified generative model for solving this problem. That is, we not only model the generation of overall ratings based on text, but also model the generation of the text itself. A natural solution would be to use a topic model. Given an entity, we can assume there are aspects that are described by word distributions (i.e., topics). We then use a topic model to model the generation of the reviewed text. We assume words in the review text are drawn from these distributions in the same way as we assumed in PLSA. Then, we can plug in the latent regression model to use the text to further predict the overall rating. To predict the overall rating based on the generated text, we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. This gives us a unified generative model, where we model both the generation of text and the overall rating conditioned on text. We don't have space to discuss this model in detail, so we refer reader to Wang et al. [2011] for additional reading. Let's look at some applications enabled by using these kinds of generative models. First, consider the decomposed ratings for some hotels that have the same overall rating. If you just look at the overall rating, you can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimension (like value) while others might score better in other dimensions (like location). This breakdown can give us detailed opinions at the aspect level. Another application is that you can compare different reviews on the same hotel. At a high level, overall ratings may look the same, but after decomposing the ratings, you might see that they have high scores on different dimensions. This is because the model can discern differences in opinions of different reviewers. Such a detailed understanding can help us learn about the reviewers and better incorporate their feedback.  In Figure 18.13, we show some highly weighted words and the negatively weighted words for each of the four aspect dimensions: value, room, location, and cleanliness. Thus, we can also learn sentiment information directly from the data. This kind of lexicon is very useful because in general, a word like long may have different sentiment polarities for different contexts. If we see "The battery life of this laptop is long," then that's positive. But if we see "The rebooting time for the laptop is long," then that's clearly not good. Even for reviews about the same product (i.e., a laptop) the word long is ambiguous. However, with this kind of lexicon, we can learn whether a word is positive or negative for a particular aspect. Such a lexicon can be directly used to tag other reviews about hotels or tag comments about hotels in social media. Since this is almost completely unsupervised aside from the overall ratings, this can allow us to learn from a potentially larger amount of data on the internet to create a topic-specific sentiment lexicon. Recall that the model can infer whether a reviewer cares more about service or the price. How do we know whether the inferred weights are correct? This poses a very difficult challenge for evaluation. Figure 18.14 shows prices of hotels in different cities. These are the prices of hotels that are favored by different groups of reviewers. Here we show the ratio of importance of value to other aspects. For example, we have value vs. location. In the figure, "top ten" refers to the reviewers that have the highest ratios by a particular measure. This means these top ten reviewers tend to put a lot of weight on value as compared with other dimensions. The bottom ten refers to reviewers that have put higher weights on other aspects than value; these are people who care about another dimension and don't care so much about value, at least compared to the top ten group. These ratios are computed based on the inferred weights from the model. We can see the average prices of hotels favored by the top ten reviewers are indeed much cheaper than those that are favored by the bottom ten. This provides some indirect way of validating the inferred weights. Looking at the average price in these three cities, you can actually see the top ten group tends to have below average prices, whereas the bottom half (that cares about aspects like service or room condition) tend to have hotels that have higher prices than average. With these results, we can build many interesting applications. For example, a direct application would be to generate a collective summary for each aspect, including the positive sand negative sentences about each aspect. This is more informative than the original review that just has an overall rating and review text. Figure 18.15 shows some interesting results on analyzing user rating behavior. What you see is average weights along different dimensions by different groups of reviewers. On the left side you see the weights of viewers that like the expensive hotels. They gave the expensive hotels five stars, with heavy aspect weight on service. That suggests that people like expensive hotels because of good service, which is not surprising. This is another way to validate the model by the inferred weights. The five-star ratings on the right side correspond to the reviewers that like the cheaper hotels. As expected, they put more weight on value. If you look at when they didn't like cheaper hotels, you'll see that they tended to have more weights on the condition of the room cleanliness. This shows that by using this model, we can  infer some information that's very hard to obtain even if you read all the reviews. This is a case where text mining algorithms can go beyond what humans can do, to discover interesting patterns in the data. We can compare different hotels by comparing the opinions from different consumer groups in different locations. Of course, the model is quite general, so it can be applied to any reviews with an overall ratings. Finally, the results of applying this model for personalized ranking or recommendation of entities are shown in Figure 18.16. Because we can infer the reviewers' weights on different dimensions, we can allow a user to indicate what they actually care about. For example, we have a query here that shows 90% of the weight should be on value and 10% on others. That is, this user just cares about getting a cheap hotel-an emphasis on the value dimension. With this model, we can find the reviewers whose weights are similar to the query user's. Then, we can use those reviewers to recommend hotels; this is what we call personalized or query specific recommendation. The non-personalized recommendations are shown on the top, and you can see the top results generally have much higher price than the bottom group. That's because the reviewers on the bottom cared more about the value. This shows that by doing text mining we can better understand and serve the users. To summarize our discussion, sentiment analysis is an important topic with many applications. Text sentiment analysis can be readily done by using just text categorization, but standard techniques tend to be insufficient so we need to have an enriched feature representation. We also need to consider the order of the sentiment categories if there are more than two; this is where our discussion on ordinal regression comes into play. We have also shown that generative models are pow- erful for mining latent user preferences, in particular the generative model for mining latent rating regression. Here, we embedded some interesting preference information and sentiment by weighting words in the model. For product reviews, the opinion holder and the opinion target are clear, making them easy to analyze. There, of course, we have many practical applications. Opinion mining from news and social media is also important, but that's more difficult than analyzing review data mainly because the opinion holders and opinion targets are not clearly defined. This challenge calls for more advanced natural language processing. 