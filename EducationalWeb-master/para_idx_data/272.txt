In Chapter 6 we emphasized the importance of the document representation in retrieval performance. In Chapter 2, we emphasized the importance of the feature representation in general. The case is the same-if not greater-in text categorization. Suppose we wish to determine whether a document has positive or negative sentiment. Clearly, a bad text representation method could be the average sentence length. That is, the document term vector is a histogram of sentence lengths for each document. Intuitively, sentence length would not be a good indicator of sentiment. Even the best learning algorithm would not be able to distinguish between positive and negative documents based only on sentence lengths. 1 On the other hand, suppose our task is basic essay scoring, where Y = {fail, pass}. In this case, sentence length may indeed be some indicator of essay quality. While not perfect, we can imagine that a classifier trained on documents represented as sentence lengths would get a higher accuracy than a similar classification setup predicting sentiment. 2 As a slightly more realistic example, we return to the sentiment analysis problem. Instead of using sentence length, we decide to use the standard unigram words representation. That is, each feature can be used to distinguish between positive or negative sentiment. Usually, most features are not useful, and the bulk of the decision is based on a smaller subset of features. Determining this smaller subset is the definition of feature selection, but we do not discuss this in depth at this point. Although most likely effective, even unigram words may not be the best representation. Consider the terms good and bad, as mentioned in the classifier example in the previous section. In this scenario, context is very important: I thought the movie was good. I thought the movie was not bad. Alternatively, I thought the movie was not good. I thought the movie was bad. 