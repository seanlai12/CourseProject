The title and a short snippet is shown from d along with the top few high-probability words from each topic. The human judge must determine which θ is θ u . As with the word intrusion test, the human judge should have a fairly easy task if the top three topics make sense together and with the document title and snippet. If it's hard to discern θ u , then the top topics must not be an adequate representation of d. Of course, this process is repeated for many different documents in the collection. Directly from Chang et al. [2009]: . . . we demonstrated that traditional metrics do not capture whether topics are coherent or not. Traditional metrics are, indeed, negatively correlated with the measures of topic quality. "Traditional metrics" refers to log-likelihood of held-out data in the case of generative models. This misalignment of results is certainly a pressing issue, though most recent research still relies on the traditional measures to evaluate new models. Downstream task improvement is perhaps the most effective (and transparent) evaluation metric. If a different topic analysis variant is shown to statistically significantly improve some task precision, then an argument may be made to prefer the new model. For example, if the topic analysis is meant to produce new features for text categorization, then classification accuracy is the metric we'd wish to improve. In such a case, log-likelihood of held-out data and even topic coherency is not a concern if the classification accuracy improves-although model interpretability may be compromised if topics are not human-distinguishable. 