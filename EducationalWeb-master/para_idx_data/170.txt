In order to create a test collection, we have to create a set of queries, a set of documents, and a set of relevance judgments. It turns out that each requirement has its own challenges. First, the documents and queries must be representative. They must represent real queries and real documents that users interact with. We also have to use many queries and many documents in order to avoid biased conclusions. In order to evaluate a high-recall retrieval task, we must ensure there exist many relevant documents for each query. If a query has only one relevant document in the collection, then it's not very informative to compare different methods using such a query because there is not much room to see a difference. In terms of relevance judgements, the challenge is to ensure complete judgements of all the documents for all the queries while simultaneously minimizing human effort. Because we have to use human effort to label these documents, it's a very labor-intensive task. As a result, it's usually impossible to actually label all of the documents for all the queries, especially considering a data set like the Web. It's also challenging to correlate the evaluation measures with the perceived utility of users. We have to consider carefully what the users care about and then design measures to capture their preferences. With a certain probability, we can mathematically quantify whether the evaluation scores of two systems are indeed different. The way we do this is with a  statistical significance test. The significance test gives us an idea as to how likely a difference in evaluation scores is due to random chance. This is the reason why we have to use a lot of queries; the more data points we have, the more confident we can be in our measure. Figure 9.8 displays some sample average precision results from system A and system B in two different experiments. As you can see in the bottom of the figure, we have the MAP for each system in each experiment. They happen to be identical in experiment one and two. Yet if you look at the exact average precisions for different queries, you will realize that in one case you might feel that you can trust the conclusion here given by the average. In the other case, you might not feel as confident. Based on only the MAP score, we can easily say that system B is better. After all, it's 0.4, which is twice as much as 0.2. Clearly, that's better performance. But if you look at these two experiments and look at the detailed results, you will see that we'll be more confident to say that in experiment 1 that system B is in fact better since the average precisions are consistently better than system A's. In experiment 2, we're not sure that system B is better since the scores fluctuate so wildly. How can we quantitatively answer this question? This is why we need to do a statistical significance test. The idea behind these tests is to assess the variance in average precision scores (or any other score) across these different queries. If there's a big variance, that means that the results could fluctuate according to different queries, which makes the result unreliable.  So let's look at these results again in the second case. In Figure 9.9, we show two different ways to compare them. One is a sign test. If system B is better than system A, then we have a plus sign. If system A is better, we have a minus sign. Using this, we have four cases where system B is better and three cases where system A is better. Intuitively, these results appear random. If you flip seven coins, using plus to denote heads and minus to denote tails, then these could easily be the results of just randomly flipping the seven coins. The fact that the average is larger doesn't tell us anything! This intuition can be quantified by the concept of a p value. A p value is the probability that this result is in fact from random fluctuation. In this case, the probability is one; it means it surely is a random fluctuation. 