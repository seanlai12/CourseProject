Tokenizer Document tokenization is the first step in any text mining task. This determines how we represent a document. We saw in the previous chapter that we often represent documents as document vectors, where each index corresponds to a single word. The value stored in the index is then a raw count of the number of occurrences of that word in a particular document. When running information retrieval scoring functions on these vectors, we usually prefer some alternate representation of term count, such as smoothed term count, or TF-IDF weighting. In real search engine systems, we often leave the term scoring up to the index scorer module. Thus, in tokenization we will simply use the raw count of features (words), since the raw count can be used by the scorer to calculate some weighted term representation. Additionally, calculating something like TF-IDF is more involved than a simple scanning of a single document (since we need to calculate IDF). Furthermore, we'd like our scorer to be able to use different scoring functions as necessary; storing only TF-IDF weight would then require us to always use TF-IDF weighting. Therefore, a tokenizer's job is to segment the document into countable features or tokens. A document is then represented by how many and what kind of tokens appear in it. The raw counts of these tokens are used by the scorer to formulate the retrieval scoring functions that we discussed in the previous chapter. The most basic tokenizer we will consider is a whitespace tokenizer. This tokenizer simply delimits words by their whitespace. Thus, whitespace_tokenizer(Mr. Quill's book is very very long.) could result in {Mr.: 1, Quill s: 1, book: 1, is: 1, very: 2, long.: 1}. A slightly more advanced unigram words tokenizer could first lowercase the sentence and split the words based on punctuation. There is a special case here where the period after Mr. is not split (since it forms a unique word): {mr.: 1, quill: 1, 's: 1, book: 1, is: 1, very: 2, long: 1, .: 1}. Of course, we aren't restricted to using a unigram words representation. Look back to the exercises from Chapter 4 to see some different ways in which we can represent text. We could use bigram words, POS-tags, grammatical parse tree features, or any combination. Common words (stop words) could be removed and words could also be reduced to their common stem (stemming). Again, the exercises in Chapter 4 give good examples of these transformations using META. In essence, the indexer and scorer shouldn't care how the term IDs were generated; this is solely the job of the tokenizer. Another common task of the tokenizer is to assign document IDs. It is much more efficient to refer to documents as unique numbers as opposed to strings such as /home/jeremy/docs/file473.txt. It's much faster to do integer comparisons than string comparisons, in addition to integers taking up much less space. The same argument may be made for string terms vs. term IDs. Finally, it will almost always be necessary to map terms to counts or documents to counts. In C++, we could of course use some structure internally such as std::unordered_ map<std::string, uint64_t>. As you know, using a hash table like this gives amortized O(1) lookup time to find a uint64_t corresponding to a particular std::string. However, using term IDs, we can instead write std::vector<uint64_t>. This data structure takes up less space, and allows true O(1) access to each uint64_t using a term ID integer as the index into the std::vector. Thus, for term ID 57, we would look up index 57 in the array. Using term IDs and the second tokenizer example, we could set mr.→ term id 0, quill→ term id 1 and so on, then our document vector looks like {1, 1, 1, 1, 1, 2, 1, 1}. Of course, a real document vector would be much larger and much sparser-that is, most of the dimensions will have a count of zero. This process is also called feature generation. It defines the building blocks of our document objects and gives us meaningful ways to compare them. Once we define how to conceptualize documents, we can index them, cluster them, and classify them, among many other text mining tasks. As mentioned in the Introduction, tokenization is perhaps the most critical component of our indexer, since all downstream operations depend on its output. 