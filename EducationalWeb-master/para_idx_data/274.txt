Here, we see three versions of increasingly "high-level" syntactic features. The bottom left square shows rewrite rules; these are the grammatical productions found in the sentence containing the syntactic node categories. For example, the S represents sentence, which is composed of a noun phrase (NP) followed by a verb phrase (VP), ending with a period. The middle square in Figure 15.2 omits all node labels except the roots of all subtrees. This captures a more abstract view of the production rules, focused more on structure. Lastly, the right square is a fully abstracted structural feature set, with no syntactic category labels at all. The authors found that these structural features combined with low-level lexical features (i.e., unigram words) improved the classification accuracy over using only one feature type. Another interesting feature generation method is described in Massung and Zhai [2015] and called SYNTACTICDIFF. The idea of SYNTACTICDIFF is to define three basic (and therefore general) edit operations: insert a word, remove a word, and substitute one word for another. These edits are used to transform a given sentence. With a source sentence S and a reference text collection R, it applied edits that make S appear to come from R. In the non-native text analysis scenario [Massung and Zhai 2016], we operate on text from writers who are not native English speakers. Thus, transforming S with respect to R is a form of grammatical error correction. While this itself is a specific application task, the series of edits performed on each sentence can also be used to represent the sentences themselves. could be a feature vector for a particular sentence. This is just one example of a feature representation that goes beyond bag-of-words. The effectiveness of these "edit features" determines how effective the classifier can be in learning a model to separate different classes. In this example, the features can be used to distinguish between different native languages of essay writers. Again, it's important to emphasize that almost all machine learning algorithms are not affected by the type of features employed (in terms of operation; of course, the accuracy may be affected). Since internally the machine learning algorithms will simply refer to each feature as an ID, the algorithm may never even know if it's operating on a parse tree, a word, bigram POS tags, or edit features. The NLP pipeline discussed in Chapter 3 and the tokenization schemes discussed in Chapter 4 give good examples of the options for effective feature representations. Usually, unigram words will be the default method, and more advanced techniques are added as necessary in order to improve accuracy. With these more 15.5 Classification Algorithms 307 advanced techniques comes a requirement for more processing time. When using features from grammatical parse trees, a parser must first be run across the dataset, which is often at least an order of magnitude slower than simple whitespacedelimited unigram words processing. Running a parser requires the sentence to be part-of-speech tagged, and running coreference resolution requires grammatical parse trees. The level of sophistication in the syntactic or semantic features usually depends on the practitioner's tolerance for processing time and memory usage. 