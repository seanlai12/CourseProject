When a document is represented as a term vector (as discussed in Chapter 6), and a Euclidean distance function is used, the K-means algorithm can be shown to minimize an objective function that computes the average distances of all the data points in a cluster to the centroid of the cluster. The algorithm is also known to converge to a local minimum, but not guaranteed to converge to a global minimum. Thus, multiple trials are generally needed in order to obtain a good local minimum. The K-means algorithm can be repeatedly applied to divide the data set gradually into smaller and smaller clusters, thus creating a hierarchy of clusters similar to what we can achieve with the agglomerative hierarchical clustering algorithm. Thus both agglomerative hierarchical clustering and K-means can be used for hierarchical clustering; they complement each other in the sense that K-means constructs the hierarchy by incrementally dividing the whole set of data (a top-down strategy), while agglomerative hierarchical clustering constructs the hierarchy by incrementally merging data points (a bottom-up strategy). Note that although in its basic form, agglomerative hierarchical clustering generates a binary tree, it can easily adapted to generate more than two branches by merging more than two groups into a cluster at each iteration. Similarly, if we only allow a binary tree, then we also do not have to set K in the K-means algorithm for creating a hierarchy. 