PLSA works well as a completely unsupervised method for analyzing topics in text data, thus it does not require any manual effort. While this is an advantage in the sense of minimizing human effort, the discovery of topics is solely driven by the data characteristics with no consideration of any extra knowledge about the topics and their coverage in the data set. Since we often have such extra knowledge or our application imposes a particular preference for the topics to be analyzed, it is beneficial or even necessary to impose some prior knowledge about the parameters to be estimated so that the estimated parameters would not only explain the text data well, but also be consistent with our prior knowledge. Prior knowledge or preferences may be available for all the parameters. First, a user may have some expectations about which topics to analyze in the text data, and such knowledge can be used to define a prior on the topic word distributions. For example, an analyst may expect to see "retrieval models" as a topic in a data set with research articles about information retrieval, thus we would like to tell the model to allocate one topic to capture the retrieval models topic. Similarly, a user may be interested in analyzing review data about a laptop with a focus on specific aspects such as battery life and screen size, thus we again want the model to allocate two topics for battery life and screen size, respectively. Second, users may have knowledge about what topics are (or are not) covered in a document. For example, if we have (topical) tags assigned to documents by users, we may regard the tags assigned to a document as knowledge about what topics 378 Chapter 17 Topic Analysis are covered in the document. Thus, we can define a prior on the topic coverage to ensure that a document can only be generated using topics corresponding to the tags assigned to it. This essentially gives us a constraint on what topics can be used to generate words in a document, which can be useful for learning co-occuring words in the context of a topic when the data are sparse and pure co-occurrence statistics are insufficient to induce a meaningful topic. All such prior knowledge can be incorporated into PLSA by using Maximum A Posteriori Estimation (MAP) instead of Maximum Likelihood estimation. Specifically, we denote all the parameters by and introduce a prior distribution p( ) over all the possible values of to encode our preferences. Such a prior distribution would technically include a distribution over all possible word distributions (for topic characterization) and all possible coverage distributions of topics in a document (for topic coverage), and can be defined based on whatever knowledge or preferences we would like to inject into the model. With such a prior, we can then estimate parameters by using MAP as follows: * = arg max p( )p(Data | ), (17.6) where p(Data | ) is the likelihood function, which would be the sole term to maximize in the case of ML estimation. Adding the prior p( ) would encourage the model to seek a compromise of the ML estimate (which maximizes p(Data | )) and the mode of the prior (which maximizes p( )). There are potentially many different ways to define p( ). However, it is particularly convenient to use a conjugate prior distribution, in which the prior density function p( ) is of the same form as the likelihood function p(Data | ) as a function of the parameter . Due to the same form of the two functions, we can generally merge the two to derive a single function (again, of the same form). In other words, our posterior distribution is written as a function of the parameter, so the maximization of the posterior probability would be similar to the maximization of the likelihood function. Since the posterior distribution is of the same form as the likelihood function of the original data, we can interpret the posterior distribution as the likelihood function for an imagined pseudo data set that is formed by augmenting the original data with additional "pseudo data" such that the influence of the prior is entirely captured by the addition of such pseudo data to the original data. When using such a conjugate prior, the computation of MAP can be done by using a slightly modified version of the EM algorithm that we introduced earlier for PLSA where appropriate counts of pseudo data are added to incorporate the prior. As a specific example, if we define a conjugate prior on the word distributions  representing the topics p(w | θ j ), then the EM algorithm for computing the MAP is shown in Figure 17.35. We see that the difference is adding an additional pseudo count for word w in the M-step which is proportional to the probability of the word in the prior p(w | θ j ). Specifically, the pseudo count is μp(w | θ j ) for word w. The denominator needs to be adjusted accordingly (adding μ which is the sum of all the pseudo counts for all the words) to ensure the estimated word probabilities for a topic sum to one. Here, μ ∈ [0, +∞) is a parameter encoding the strength of our prior. If μ = 0, we recover the original EM algorithm for PLSA, i.e., with no prior influence. A more interesting case is when μ = +∞, in such a case, the M-step is simply to set the estimated probability of a word p(w | θ j ) to the prior p(w | θ j ), i.e., the word distribution is fixed to the prior. This is why we can interpret our heuristic inclusion of a background word distribution as a topic in PLSA as simply imposing such an infinitely strong prior on one of the topics. Intuitively, in Bayesian inference, this means that if the prior is infinitely strong, then no matter how much data we collect, we will not be able to override the prior. In general, however, as we increase the amount of data, we will be able to let the data dominate the estimate, eventually overriding the prior completely as we collect infinitely more data. A prior on the coverage distribution π can be added in a similar way to the updating formula for π d , j to force the updated parameter value to give some topics higher probabilities by reducing the probabilities of others. In the extreme, it is also possible to achieve the effect of setting the probability of a topic to zero by using an infinitely strong prior that gives such a topic a zero probability. PLSA is a generative model for modeling the words in a given document, but it is not a generative model for documents since it cannot give a probability of a new unseen document; it cannot give a distribution over all the possible documents. However, we sometimes would like to have a generative model for documents. For example, if we can estimate such a model for documents in each topic category, then we would be able to use the model for text categorization by comparing the probability of observing a document from the generative model of each category and assigning the document to the category whose generative model gives the highest probability to the document. The difficulty in giving a new unseen document a probability using PLSA is that the topic coverage parameter in PLSA is tied to an observed document, and we do not have available in the model the coverage of topics in a new unseen document, which is needed in order to generate words in a new document. Although it is possible to use a heuristic approach to estimate the topic coverage in an unseen document, a more principled way to solve the problem is to add priors on the parameters of PLSA and make a Bayesian version of the model. This has led to the development of the Latent Dirichlet Allocation (LDA) model. Specifically, in LDA, the topic coverage distribution (a multinomial distribution) for each document is assumed to be drawn from a prior Dirichlet distribution, which defines a distribution over the entire space of the parameters of a multinomial distribution, i.e., a vector of probabilities of topics. Similarly, all the word distributions representing the latent topics in a collection of text are also assumed to be drawn from another Dirichlet distribution. In PLSA, both the topic coverage distribution and the word distributions are assumed to be (unknown) parameters in the model. In LDA, they are no longer parameters of the model since they are assumed to be drawn from the corresponding Dirichlet (prior) distributions. Thus, LDA only has parameters to characterize these two kinds of Dirichlet distributions. Once these parameters are fixed, the behavior of these two Dirichlet distributions would be fixed, and thus the behavior of the entire generative model would also be fixed. Once we have sampled all the word distributions for the whole collection (which shares these topics), and the topic coverage distribution for a document, the rest of the process of generating words in the document is exactly the same as in PLSA. The generalization of PLSA to LDA by imposing Dirichlet priors is illustrated in Figure 17.36, where we see that the Dirichlet distribution governing  the topic coverage has k parameters, α 1 , . . . , α k , and the Dirichlet distribution governing the topic word distributions has M parameters, β 1 , . . . , β M . Each α i can be interpreted as the pseudo count of the corresponding topic θ i according to our prior, while each β i can be interpreted as the pseudo count of the corresponding word w i according to our prior. With no additional knowledge, they can all be set to uniform counts, which in effect, assumes that we do not have any preference for any word in each word distribution and we do not have any preference for any topic either in each document. The likelihood function of LDA is given in Figure 17.37 where we also make a comparison between the likelihood of PLSA and that of LDA. The comparison allows us to see that both PLSA and LDA share the common generative model component to define the probability of observing a word w in document d from a mixture model involving k word distributions, θ 1 , . . . , θ k , representing k topics with a topic coverage distribution π d , j . Indeed, such a mixture of unigram language models is the common component in most topic models, and is key for modeling documents with multiple topics covered in the same document. However, the likelihood function for a document and the entire collection C is clearly different with LDA adding the uncertainty of the topic coverage distribution and the uncertainty of all the word distributions in the form of an integral. 