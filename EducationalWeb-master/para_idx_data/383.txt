In the mixture model feedback approach [Zhai and Lafferty 2001], we assume that the feedback documents F = {d 1 , . . . , d k } are "generated" from a mixture model with two multinomial component models. One component model is the background model p(w | C) and the other is an unknown topic language model p(w | θ F ) to be estimated. (w is a word.) The idea is to model the common (nondiscriminative) words in F with p(w | C) so that the topic model θ F would attract more discriminative content-carrying words. The log-likelihood of the feedback document data for this mixture model is where d ij is the j th word in document d i , |d i | is the length of d i , and λ is a parameter that indicates the amount of "background noise" in the feedback documents, which will be set empirically. We thus assume λ to be known, and want to estimate p(w | θ F ). 