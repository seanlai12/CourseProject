The main idea of the EM algorithm is to "augment" our data with some latent variables so that the "complete" data has a much simpler likelihood functionsimpler for the purpose of finding a maximum. The original data are thus treated as "incomplete." As we will see, we will maximize the incomplete data likelihood (our original goal) through maximizing the expected complete data likelihood (since it is much easier to maximize) where expectation is taken over all possible values of the hidden variables (since the complete data likelihood, unlike our original incomplete data likelihood, would contain hidden variables). In our example, we introduce a binary hidden variable z for each occurrence of a word w to indicate whether the word has been "generated" from the background model p(w | C) or the topic model p(w | θ F ). Let d ij be the j th word in document d i . We have a corresponding variable z ij defined as follows: We thus assume that our complete data would have contained not only all the words in F , but also their corresponding values of z. The log-likelihood of the complete data is thus Note the difference between L c (θ F ) and L(θ F ): the sum is outside of the logarithm in L c (θ F ), and this is possible because we assume that we know which component model has been used to generated each word d ij . What is the relationship between L c (θ F ) and L(θ F )? In general, if our parameter is θ, our original data is X, and we augment it with a hidden variable H , then the first term, which is the expectation of the complete likelihood, or the so-called "Q-function" denoted by Q(θ ; θ (n) ). Q(θ; θ (n) ) = E p(H |X, θ (n) ) [L c (θ )] = H L c (θ )p(H | X, θ (n) ). The Q-function for our mixture model is the following 