Algorithmically, the basic idea of EM is to start with some initial guess of the parameter values θ (0) and then iteratively search for better values for the parameters. Assuming that the current estimate of the parameters is θ (n) , our goal is to find another θ (n+1) that can improve the likelihood L(θ ). Let us consider the difference between the likelihood at a potentially better parameter value θ and the likelihood at the current estimate θ (n) , and relate it with the corresponding difference in the complete likelihood: Our goal is to maximize L(θ ) − L(θ (n) ), which is equivalent to maximizing L(θ ). Now take the expectation of this equation w.r.t. the conditional distribution of the hidden variable given the data X and the current estimate of parameters θ (n) , i.e., p(H | X, θ (n) ). We have Note that the left side of the equation remains the same as the variable H does not occur there. The last term can be recognized as the KL-divergence of p(H | X, θ (n) ) and p(H | X, θ), which is always non-negative. We thus have We thus obtain a lower bound for the original likelihood function. The main idea of EM is to maximize this lower bound so as to maximize the original (incomplete) likelihood. Note that the last two terms in this lower bound can be treated as constants as they do not contain the variable θ, so the lower bound is essentially C APPENDIX 