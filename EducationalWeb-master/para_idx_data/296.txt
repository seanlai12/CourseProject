words. The generation will work the same way as in the unigram case: say we have the word w i and wish to generate w i+1 with a bigram language model. Our bigram language model gives us a distribution of words that occur after w i and we draw the next word from there in the same way depicted in Figure 16.3. The sentence generation from a bigram language model proceeds as follows: start with (e.g.) The. Then, pick from the distribution p(w | The) using the cumulative sum technique. The next selected word could be cat. Then, we use the distribution p(w | cat) to find the next w, and so on. While the unigram model only had one "sum table" (Figure 16.3), the bigram case needs V tables, one for each w in p(w | w ). Typically, the n-value will be around three to five depending on how much original data there is. We saw what happened when n is too small; we get a jumble of words that don't make sense together. But we have another problem if n is too large. Consider the extreme case where n = 20. Then, given 19 words, we wish to generate the next one using our 20-gram language model. It's very unlikely that those 19 words occurred more than once in our original document. That means there would only be one choice for the 20th word. Because of this, we would just be reproducing the original document, which is not a very good summary. In practice, we would like to choose an n-gram value that is large enough to produce coherent text yet small enough to not simply reproduce the corpus. There is one major disadvantage to this abstractive summarization method. Due to its nature, a given word only depends on the n surrounding words. That is, there will be no long-range dependencies in our generated text. For example, consider the following sentence generated from a trigram language model: They imposed a gradual smoking ban on virtually all corn seeds planted are hybrids. All groups of three words make sense, but as a whole the sentence is incomprehensible; it seems the writer changed the topic from a smoking ban to hybrid crops mid-sentence. In special cases, when we restrict the length of a summary to a few words when summarizing highly redundant text, such a strategy appears to be effective as shown in the micropinion summarization method described in Ganesan et al. [2012]. 