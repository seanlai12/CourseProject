Information theory deals with uncertainty and the transfer or storage of quantified information in the form of bits. It is applied in many fields, such as electrical engineering, computer science, mathematics, physics, and linguistics. A few concepts from information theory are very useful in text data management and analysis, which we introduce here briefly. The most important concept of information theory is entropy, which is a building block for many other measures. The problem can be formally defined as the quantified uncertainty in predicting the value of a random variable. In the common example of a coin, the two values would be 1 or 0 (depicting heads or tails) and the random variable representing these outcomes is X. In other words, The more random this random variable is, the more difficult the prediction of heads or tails will be. How does one quantitatively measure the randomness of a random variable like X? This is precisely what entropy does. Roughly, the entropy of a random variable X, H (X), is a measure of expected number of bits needed to represent the outcome of an event x ∼ X. If the outcome is known (completely certain), we don't need to represent any information and H (X) = 0. If the outcome is unknown, we would like to represent the outcome in bits as efficiently as possible. That means using fewer bits for common occurrences and more bits when the event is less likely. Entropy gives us the expected number 32 Chapter 2 Background of bits for any x ∼ X using the formula H (X) = − x∈X p(x) log 2 p(x). (2.14) In the cases where we have log 2 0, we generally just define this to be 0 since log 2 0 is undefined. We will get different H (X) for different random variables X. The exact theory and reasoning behind this formula are beyond the scope of this book, but it suffices to say that H (X) = 0 means there is no randomness, H (X) = 1 means there is complete randomness in that all events are equally likely. Thus, the amount of randomness varies from 0 to 1. For our coin example where the sample space is two events (heads or tails), the entropy function looks like H (X) = −p(X = 0) log 2 p(X = 0) − p(X = 1) log 2 p(X = 1). For a fair coin, we would have p(X = 1) = p(X = 0) = 1 2 . To calculate H (X), we'd have the calculation whereas for a completely biased coin with p(X = 1) = 1, p(X = 0) = 0 we would have H (X) = −0 log 2 0 − 1 log 2 1 = 0. For this example, we had only two possible outcomes (i.e., a binary random variable). As we can see from the formula, this idea of entropy easily generalizes to random variables with more than two outcomes; in those cases, the sum is over more than two elements. If we plot H (X) for our coin example against the probability of heads p(X = 1), we receive a plot like the one shown in Figure 2.1. At the two ends of the x-axis, the probability of X = 1 is either very small or very large. In both these cases, the entropy function has a low value because the outcome is not very random. The most random is when p(X = 1) = 1 2 . In that case, H (X) = 1, the maximum value. Since the two probabilities are symmetric, we get a symmetric inverted U -shape as the plot of H (X) as p(X = 1) varies. It's a good exercise to consider when a particular random variable (not just the coin example) has a maximum or minimal value. In particular, let's think about some special cases. For example, we might have a random variable Y that always takes a value of 1. Or, there's a random variable Z that is equally likely to take a value of 1, 2, or 3. In these cases, H (Y ) < H (Z) since the outcome of Y is much easier to predict than the outcome of Z. This is precisely what entropy captures. You can calculate H (Y ) and H (Z) to confirm this answer. For our applications, it may be useful to consider the entropy of a word w in some context. Here, high-entropy words would be harder to predict. Let W be the random variable that denotes whether a word occurs in a document in our corpus. Say W = 1 if the word occurs and W = 0 otherwise. How do you think H (W the ) compares to H (W computer )? The entropy of the word the is close to zero since it occurs everywhere. It's not surprising to see this word in a document, thus it is easy to predict that W the = 1. This case is just like the biased coin that always lands one way. The word computer, on the other hand, is a less common word and is harder to predict whether it occurs or not, so the entropy will be higher. When we attempt to quantify uncertainties of conditional probabilities, we can also define conditional entropy H (X | Y ), which indicates the expected uncertainty of X given that we observe Y , where the expectation is taken under the distribution of all possible values of Y . Intuitively, if X is completely determined by Y , then H (X | Y ) = 0 since once we know Y , there would be no uncertainty in X, whereas if X and Y are independent, then H (X | Y ) would be the same as the original entropy of X, i.e., H (X | Y ) = H (X) since knowing Y does not help at all in resolving the uncertainty of X. 34  