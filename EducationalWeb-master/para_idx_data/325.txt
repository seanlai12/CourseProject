The discussion of the behaviors of the ML estimate of the mixture model provides an intuition about why we can use a mixture model to mine one topic from a document with common words factored out through the use of a background model. In this section, we further discuss how we can compute such an ML estimate. Unlike the simplest unigram language model, whose ML estimate has an analytical solution, there is no analytical solution to the ML estimation problem for the two-component mixture model even though we have exactly the same number of parameters to estimate as a single unigram language model after we fix the background model and the choice probability of the component models (i.e., p(θ d )). We must use a numerical optimization algorithm to compute the ML estimate. In this section, we introduce a specific algorithm for computing the ML estimate of the two-component mixture model, called the Expectation-Maximization (EM) algorithm. EM is a family of useful algorithms for computing the maximum likelihood estimate of mixture models in general. Recall that we have assumed both p(w | θ B ) and p(θ B ) are already given, so the only "free" parameters in our model are p(w | θ d ) for all the words subject to the constraint that they sum to one. This is illustrated in Figure 17.22. Intuitively, when we compute the ML estimate, we would be exploring the space of all possible values for the word distribution θ d until we find a set of values that would maximize the probability of the observed documents. According to our mixture model, we can imagine that the words in the text data can be partitioned into two groups. One group will be explained (generated) by ∑ w′2V Figure 17.22 Estimation of a topic when each word is known to be from a particular distribution. the background model. The other group will be explained by the unknown topical model (the topic word distribution). The challenge in computing the ML estimate is that we do not know this partition due to the possibility of generating a word using either of the two distributions in the mixture model. If we actually know which word is from which distribution, computation of the ML estimate would be trivial as illustrated in Figure 17.22, where d is used to denote the pseudo document that is composed of all the words in document d that are known to be generated by θ d , and the ML estimate of θ d is seen to be simply the normalized word frequency in this pseudo document d . That is, we can simply pool together all the words generated from θ d , compute the count of each word, and then normalize the count by the total counts of all the words in such a pseudo document. In such a case, our mixture model is really just two independent unigram language models, which can thus be estimated separately based on the data points generated by each of them. Unfortunately, the real situation is such that we don't really know which word is from which distribution. The main idea of the EM algorithm is to guess (infer) which word is from which distribution based on a tentative estimate of parameters, and then use the inferred partitioning of words to improve the estimate of parameters, which, in turn, enables improved inference of the partitioning, leading to an iterative hill-climbing algorithm to improve the estimate of the parameters until hitting a local maximum. In each iteration, it would invoke an E-step followed by an M-step, which will be explained in more detail. 