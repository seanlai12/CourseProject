This section is about feedback for language modeling in the query likelihood model of information retrieval. Recall that we derive the query likelihood ranking function by making various assumptions, such as term independence. As a basic retrieval function, that family of functions worked well. However, if we think about incorporating feedback information, it is not immediately obvious how to modify 7.2 Feedback in Language Models 139 query likelihood to perform feedback. Many times, the feedback information is additional information about the query, but since we assumed that the query is generated by assembling words from an ideal document language model, we don't have an easy way to add this additional information. However, we have a way to generalize the query likelihood function that will allow us to include feedback documents more easily: it's called a Kullback-Leibler divergence retrieval model, or KL-divergence retrieval model for short. This model actually makes the query likelihood retrieval function much closer to the vector space model. Despite this, the new form of the language model retrieval can still be regarded as a generalization of query likelihood (in that it covers query likelihood without feedback as a special case). Here, the feedback can be achieved through query model estimation or updating. This is very similar to Rocchio feedback which updates the query vector; in this case, we update the query language model instead. Figure 7.3 shows the difference between our original query likelihood formula and the generalized KL-divergence model. On top, we have the query likelihood retrieval function. The KL-divergence retrieval model generalizes the query term frequency into a probabilistic distribution. This distribution is the only difference, which is able to characterize the user's query in a more general way. This query language model can be estimated in many different ways-including using feedback information. This method is called KL-divergence because this can be interpreted as measuring the divergence (i.e., difference) between two distributions; one is the query model p(w |Î¸ Q ) and the other is the document language model from before. We won't go into detail on KL-divergence, but there is a more detailed explanation in appendix C. 