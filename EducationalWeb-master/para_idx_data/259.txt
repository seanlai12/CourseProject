Utility. How useful are the discovered clusters for an application? As with most text mining (and many other) tasks, we can evaluate in one of two broad strategies: manual evaluation (using humans) or automatic evaluation (using predefined measures). Of the three criteria mentioned above, coherence and separation can be measured automatically with measures such as vector similarity, purity, or mutual information. There is a slight challenge when evaluating term clustering, since word-to-word similarity algorithms may not be as obvious as document-to-document similarities. We may choose to encode terms as word vectors and use the document similarity measures, or we may wish to use some other concept of semantic similarity as defined by preexisting ontologies like WordNet. 2 Although slightly more challenging, the concept of utility can also be captured if the final system output can be measured quantitatively. For example, if clustering is used as a component in search, we can see if using a different clustering algorithm improves F 1 , MAP, or NCDG (see Chapter 9). All clustering methods need some notion of similarity (or bias). After all, we wish to find groups of objects that are similar to one another in some way. We mainly discussed unigram words representations, though in this book we have elaborated on many different feature types. Indeed, feature engineering is an important component of implementing a clustering algorithm, and in fact any text mining algorithm in general. Choosing the right representation for your text allows you to quantify the important differences between items that cause them to end up in either the 