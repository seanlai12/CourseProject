In most discussions on probability, a good example to investigate is flipping a coin. For example, we may be interested in modeling the presence or absence of a particular word in a text document, which can be easily mapped to a coin flipping problem. There are two possible outcomes in coin flipping: heads or tails. The probability of heads is denoted as θ , which means the probability of tails is 1 − θ . To model the probability of success (in our case, "heads"), we can use the Bernoulli distribution. The Bernoulli distribution gives the probability of success for a single event-flipping the coin once. If we want to model n throws and find the probability of k successes, we instead use the binomial distribution. The binomial distribution is a discrete distribution since k is an integer. We can write it as p(k heads) = n k θ k (1 − θ) n−k . (2.6) We can also write it as follows: p(k heads) = n! k!(n − k)! θ k (1 − θ) n−k . (2.7) But why is it this formula? Well, let's break it apart. If we have n total binary trials, and want to see k heads, that means we must have flipped k heads and n − k tails. The probability of observing each of the k heads is θ , while the probability of observing each of the remaining n − k tails is 1 − θ . Since we assume all these flips are independent, we simply multiply all the outcomes together. Since we don't care about the order of the outcomes, we additionally multiply by the number of possible ways to choose k items from a set of n items. What if we do care about the order of the outcomes? For example, what is the probability of observing the particular sequence of outcomes (h, t , h, h, t) where h and t denote heads and tails, respectively? Well, it is easy to see that the probability 2.1 Basics of Probability and Statistics 27 of observing this sequence is simply the product of observing each event, i.e., θ × (1 − θ) × θ × θ × (1 − θ) = θ 3 (1 − θ) 2 with no adjustment for different orders of observing three heads and two tails. The more commonly used multinomial distribution in text analysis, which models the probability of seeing a word in a particular scenario (e.g., in a document), is very similar to this Bernoulli distribution, just with more than two outcomes. 