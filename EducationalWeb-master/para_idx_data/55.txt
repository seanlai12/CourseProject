The first step in creating an index over any sort of text data is the "tokenization" process. At a high level, this simply means converting individual text documents into sparse vectors of counts of terms-these sparse vectors are then typically consumed by an indexer to output an inverted_index over your corpus. META structures this text analysis process into several layers in order to give the user as much power and control over the way the text is analyzed as possible. An analyzer, in most cases, will take a "filter chain" that is used to generate the final tokens for its tokenization process: the filter chains are always defined as a specific tokenizer class followed by a sequence of zero or more filter classes, each of which reads from the previous class's output. For example, here is a simple filter chain that lowercases all tokens and only keeps tokens with a certain length range: icu_tokenizer → lowercase_filter → length_filter Tokenizers always come first. They define how to split a document's string content into tokens. Some examples are as follows. icu_tokenizer. converts documents into streams of tokens by following the Unicode standards for sentence and word segmentation. character_tokenizer. converts documents into streams of single characters. Filters come next, and can be chained together. They define ways that text can be modified or transformed. Here are some examples of filters. length_filter. this filter accepts tokens that are within a certain length and rejects those that are not. icu_filter. applies an ICU (International Components for Unicode) 3 transliteration to each token in the sequence. For example, an accented character likë ı is instead written as i. META defines a sane default filter chain that users are encouraged to use for general text analysis in the absence of any specific requirements. To use it, one should specify the following in the configuration file: [[analyzers]] method = "ngram-word" ngram = 1 filter = "default-chain" This configures the text analysis process to consider unigrams of words generated by running each document through the default filter chain. This filter chain should work well for most languages, as all of its operations (including but not limited to tokenization and sentence boundary detection) are defined in terms of the Unicode standard wherever possible. To consider both unigrams and bigrams, the configuration file should look like the following:  [[analyzers]] block defines a single analyzer and its corresponding filter chain: as many can be used as desired-the tokens generated by each analyzer specified will be counted and placed in a single sparse vector of counts. This is useful for combining multiple different kinds of features together into your document representation. For example, the following configuration would combine unigram words, bigram part-of-speech tags, tree skeleton features, and subtree features. [[analyzers]] method = "ngram-word" ngram = 1 filter = "default-chain" [[analyzers]] method = "ngram-pos" ngram = 2 filter = [{type = "icu-tokenizer"}, {type = "ptb-normalizer"}] crf-prefix = "path/to/crf/model" [[analyzers]] method = "tree" filter = [{type = "icu-tokenizer"}, {type = "ptb-normalizer"}] features = ["skel", "subtree"] tagger = "path/to/greedy-tagger/model" parser = "path/to/sr-parser/model" If an application requires specific text analysis operations, one can specify directly what the filter chain should look like by modifying the configuration file. Instead of filter being a string parameter as above, we will change filter to look very much like the [[analyzers]] blocks: each analyzer will have a series of [[analyzers.filter]] blocks, each of which defines a step in the filter chain. All filter chains must start with a tokenizer. Here is an example filter chain for unigram words like the one at the beginning of this section: META provides many different classes to support building filter chains. Please look at the API documentation 5 for more information. In particular, the analyzers::tokenizers namespace and the analyzers::filters namespace should give a good idea of the capabilities. The static public attribute id for a given class is the string needed for the "type" in the configuration file. 