With such a formula, an unseen word would not have a zero probability, and the estimated probability is, in general, more accurate. We can replace p(w | C) in the previous scoring function with our smoothed version. In the example, this brings the score for artichoke much lower since we "pretend" to have seen a count of it in the background. Words that actually are semantically related (i.e., that occur much more frequently in the context of computer) would not be affected by this smoothing and instead would "rise up" as the unrelated words are shifted downwards in the list of sorted scores. From Chapter 6 we learned that this Add-1 smoothing may not be the best smoothing method as it applies too much probability mass to unseen words. In an even more improved scoring function, we could use other smoothing methods such as Dirichlet prior or Jelinek-Mercer interpolation. In any event, this semantic relatedness is what we wish to capture in our term clustering applications. However, you can probably see that it would be infeasible to run this calculation for every term in our vocabulary. Thus, in the next section, we will examine a more efficient method to cluster terms together. The basic idea of the problem is exactly the same. 