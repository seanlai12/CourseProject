Classifier 1 How many parameters are there in total? → Share training data precisely the idea of ordinal logistic regression, which is an improvement over the k − 1 independent logistic regression classifiers, as shown in Figure 18.7. The improvement is to tie the β parameters together; that means we are going to assume the β values are the same for all the k − 1 classifiers. This encodes our intuition that positive words (in general) would make a higher rating more likely. In fact, this would allow us to have two benefits. One is to reduce the number of parameters significantly. The other is to allow us to share the training data amongst all classifiers since the parameters are the same. In effect, we have more data to help us choose good β values. The resulting formula would look very similar to what we've seen before, only now the β parameter has just one index that corresponds to a single feature; it no longer has the other indices that correspond to rating levels. However, each classifier still has a distinct predicted rating value. Of course, this value is needed to predict the different rating levels. So α j is different since it depends on j , but the rest of the parameters (the β i 's) are the same. We now have M + k − 1 parameters. It turns out that with this idea of tying all the parameters, we end up having a similar way to make decisions, as shown in Figure 18.8. More specifically, the criteria whether the predictor probabilities are at least 0.5 or above is equivalent to whether the score of the object is larger than or equal to α k . The scoring function is just taking a linear combination of all the features with the β values. This means now we can simply make a rating decision by looking at the value of this scoring function and seeing which bracket it falls into. In this approach, we're going to score the object by using the features and trained parameter values. 