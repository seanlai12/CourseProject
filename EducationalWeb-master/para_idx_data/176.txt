The crawler is also called a spider or a software robot that crawls (traverses, parses, and downloads) pages on the web. Building a toy crawler is relatively easy because you just need to start with a set of seed pages, fetch pages from the web, and parse these pages' new links. We then add them to a queue and then explore those page's links in a breadth-first search until we are satisfied. Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with. One issue is robustness: What if the server doesn't respond or returns unparseable garbage? What if there's a trap that generates dynamically generated pages that attract your crawler to keep crawling the same 10.1 Web Crawling 193 site in circles? Yet another issue is that we don't want to overload one particular server with too many crawling requests. Those may cause the site to experience a denial of service; some sites will also block IP addresses that they believe to be crawling them or creating too many requests. In a similar vein, a crawler should respect the robot exclusion protocol. A file called robots.txt at the root of the site tells crawlers which paths they are not allowed to crawl. You also need to handle different types of files such as images, PDFs, or any other kinds of formats on the web that contain useful information for your search application. Ideally, the crawler should recognize duplicate pages so it doesn't repeat itself or get stuck in a loop. Finally, it may be useful to discover hidden URLs; these are URLs that may not be linked from any page yet still contain content that you'd like to index. So, what are the major crawling strategies? In general, breadth-first search is the most common because it naturally balances server load. Parallel crawling is also very natural because this task is very easy to parallelize. One interesting variation is called focused crawling. Here, we're going to crawl some pages about a particular topic, e.g., all pages about automobiles. This is typically going to start with a query that you use to get some results. Then, you gradually crawl more. An even more extreme version of focused crawling is (for example) downloading and indexing all forum posts on a particular forum. In this case, we might have a URL such as http://www.text-data-book-forum.com/boards?id=3 which refers to the third post on the forum. By changing the id parameter, we can iterate through all forum posts and index them quite easily. In this scenario, it's especially important to add a delay between requests so that the server is not overwhelmed. Another challenge in crawling is to find new pages that have been created since the crawler last ran. This is very challenging if the new pages have not been linked to any old page. If they are, then you can probably find them by following links from existing pages in your index. Finally, we might face the scenario of incremental crawling or repeated crawling. Let's say you want to be able to create a web search engine. Clearly, you first crawl data from the web. In the future we just need to crawl the updated pages. This is a very interesting research question: how can we determine when a page needs to be recrawled (or even when a new page has been created)? There are two major factors to consider here, the first of which is whether a particular page would be updated frequently. If the page is a static page that hasn't been changed for months, it's probably not necessary to re-crawl it every day since it's unlikely that it will be changed frequently. On the other hand, if it's (for example) a sports score page that gets updated very frequently, you may need to re-crawl even multiple times on the same day. The second factor to consider is how frequently a particular page is accessed by users of the search engine system. If it's a high-utility page, it's more important to ensure it is fresh. Compare it with another page that has never been fetched by any users for a year; even though that unpopular page has been changed a lot, it's probably not necessary to crawl that page-or at least it's not as urgent-to maintain its freshness. 