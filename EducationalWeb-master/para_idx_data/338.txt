In summary, we introduced techniques for topic analysis in this chapter. We started with the simple idea of using one term to represent a topic, and discussed the deficiency of such an approach. We then introduced the idea of representing a topic with a word distribution, or a unigram language model, and introduced the PLSA model, which is a mixture model with k unigram language models representing k topics. We also added a pre-specified background language model to help discover discriminative topics, because this background language model can help attract the common terms. We used the maximum likelihood estimator (computed using the EM algorithm) to estimate the parameters of PLSA. The estimated parameter values enabled us to discover two things, one is k word distributions with each one representing a topic, and the other is the proportion of each topic in each document. The topic word distributions and the detailed characterization of coverage of topics in each document can enable further analysis and applications. For exam-ple, we can aggregate the documents in a particular time period to assess the coverage of a particular topic in the time period. This would allow us to generate a temporal trend of topics. We can also aggregate topics covered in documents associated with a particular author to reveal the expertise areas of the author. Furthermore, we can also cluster terms and cluster documents. In fact, each topic word distribution can be regarded as a cluster (for example, the cluster can be easily obtained by selecting the top N words with the highest probabilities). So we can generate term clusters easily based on the output from PLSA. Documents can also be clustered in the same way: we can assign a document to the topic cluster that's covered most in the document. Recall that π d , j indicates to what extent each topic θ j is covered in document d. We can thus assign the document to the topical cluster that has the highest π d , j . Another use of the results from PLSA is to treat the inferred topic coverage distribution in a document as an alternative way of representing the document in a low-dimensional semantic space where each dimension corresponds to a topic. Such a representation can supplement the bagof-words representation to enhance inexact matching of words in the same topic, which can generally be beneficial (e.g., for information retrieval, text clustering, and text categorization). Finally, a variant of PLSA called latent Dirichlet allocation (LDA) extends PLSA by adding priors to the document-topic distributions and topic-word distributions. These priors can force a small number of topics to dominate in each document, which makes sense because usually a document is only about one or two topics as opposed to a true mixture of all k topics. Secondly, adding these priors can give us sparse word distributions in each topic as well, which mimics the Zipfian distribution of words we've discussed previously. Finally, LDA is a generative model, which can be used to simulate (generate) values of parameters in the model as well as apply the model to a new, unseen document [Blei et al. 2003]. 