In Chapter 13, we discussed in length how to represent a term as a term vector based on the words in the context where the term occurs, and compute term similarity based on the similarity of their vector representations. Such a contextual view of term representation can not only be used for discovering paradigmatic relations, but also support term clustering in general since we can use any document clustering algorithm by viewing a term as a "document" represented by a vector. It can also help word sense disambiguation since when an ambiguous word takes a different sense, it tends to "attract" different words in its surrounding text, thus would have a different context representation. This technique is not limited to unigram words, and we can think of other representations for the vector such as part-of-speech tags or even elements like sentiment. Adding these additional features means expanding the word vector from |V | to whatever size we require. Additionally, aside from finding semanticallyrelated terms, using this richer word representation has the ability to improve downstream tasks such as grammatical parsing or statistical machine translation. However, the heuristic way to obtain vector representation discussed in Chapter 13 has the disadvantage that we need to make many ad hoc choices, especially in how to obtain the term weights. Another deficiency is that the vector spans the entire space of words in the vocabulary, increasing the complexity of any further processing applied to the vectors. As an alternative, we can use a neural language model [Mikolov et al. 2010] to systematically learn a vector representation for each word by optimizing a meaningful objective function. Such an approach is also called word embedding, which refers to the mapping of a word into a vector representation in a low-dimensional space. The general idea of these methods is to assume that each word corresponds to a vector in an unknown (latent) low-dimensional space and define a language model solely based on the vector representations of the involved words so that the parameters for such a language model would be the vector representations of words. As a result, by fitting the model to a specific data set, we can learn the vector representations for all the words. These language models are called neural language models because they can be represented as a neural network. For example, to model an n-gram language model p(w n | w n−1 , . . . , w 1 ), the neural network would have w n−1 , . . . , w 1 as input and w n as the output. In some neural language models, the hidden layer in the neural network connected to a word can be interpreted as a vector representation of the word with the elements being the weights on the edges connected to the word. For example, in the skip-gram neural language model [Mikolov et al. 2013], the objective function is to use each word to predict all other words in its context as defined by a window around the word, and the probability of predicting word w 1 given word w 2 is given by where v i is the corresponding vector representation of word w i . In words, such a model says that the probability p(w 1 | w 2 ) is proportional to the dot product of the 