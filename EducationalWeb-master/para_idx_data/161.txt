We have only seen three relevant documents there, but we can imagine there are other documents judged for this query. Intuitively we thought that system A is better because it did not have much noise. In particular we have seen, out of three results, two are relevant. On the other hand, in system B we have five results and only three of them are relevant. Based on this, it looks like system A is more accurate. This can be captured by the measure of precision, where we simply compute to what extent all the retrieval results are relevant. 100% precision would mean all the retrieved documents are relevant. Thus in this case, system A has a precision of 2 3 = 0.66. System B has 3 5 = 0.60. This shows that system A is better according to precision. But we also mentioned that system B might be preferred by some other users who wish to retrieve as many relevant documents as possible. So, in that case we have to compare the number of total relevant documents to the number that is actually retrieved. This is captured in another measure called recall, which measures the completeness of coverage of relevant documents in your retrieval result. We assume that there are ten relevant documents in the collection. Here we've got two of them from system A, so the recall is 2 10 = 0.20. System B has 3 10 = 0.30. Therefore, system B is better according to recall. These two measures are the very basic foundation for evaluating search engines. They are very important because they are also widely used in many other evaluation problems. For example, if you look at the applications of machine learning you tend to see precision and recall numbers being reported for all kinds of tasks. Now, let's define these two measures more precisely and how these measures are used to evaluate a set of retrieved documents. That means we are considering that approximation of a set of relevant documents. We can distinguish the results in four cases, depending on the situation of a document, as shown in Figure 9.2. A document is either retrieved or not retrieved since we're talking about the set of results. The document can be also relevant or not relevant, depending on whether the user thinks this is a useful document. We now have counts of documents in each of the four categories. We can have a represent the number of documents that are retrieved and relevant, b for documents that are not retrieved but relevant, c for documents that are retrieved but not relevant, and d for documents that are both not retrieved and not relevant. With this table, we have defined precision as the ratio of the relevant retrieved documents a to the total number of retrieved documents a and c: a a+c . In this case, the denominator is all the retrieved documents. Recall is defined by dividing a by the sum of a and b, where a + b is the total number of relevant documents. Precision and recall are focused on looking at a, the number of retrieved relevant documents. The two measures differ based on the denominator of the formula. In reality, high recall tends to be associated with low precision So what would be an ideal result? If precision and recall are both 1.0, that means all the results that we returned are relevant, and we didn't miss any relevant documents; there's no single non-relevant document returned. In reality, however, high recall tends to be associated with low precision; as you go down the list to try to get as many relevant documents as possible, you tend to include many nonrelevant documents, which decreases the precision. We often are interested in the precision up to ten documents for web search. This means we look at the top ten results and see how many documents among them are actually relevant. This is a very meaningful measure, because it tells us how many relevant documents a user can expect to see on the first page of search results (where there are typically ten results). In the next section, we'll see how to combine precision and recall into one score that captures both measures. 