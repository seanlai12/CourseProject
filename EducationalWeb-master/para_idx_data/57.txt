In its simplest form, text data could be a single document in .txt format. This exercise will get you familiar with various techniques that are used to analyze text. We'll use the novel A Tale of Two Cities by Charles Dickens as example text. The book is called two-cities.txt, and is located at http://sifaka.cs.uiuc.edu/ir/ textdatabook/two-cities.txt. You can also use any of your own plaintext files that have multiple English sentences. Like all future exercises, we will assume that the reader followed the META setup guide and successfully compiled the executables. In this exercise, we'll only be using the profile program. Running ./profile from inside the build/ directory will print out the following usage information: Usage: ./profile config.toml file.txt [OPTION] where [OPTION] is one or more of: --stem perform stemming on each word --stop remove stop words --pos annotate words with POS tags --pos-replace replace words with their POS tags --parse create grammatical parse trees from file content --freq-unigram sort and count unigram words --freq-bigram sort and count bigram words --freq-trigram sort and count trigram words --all run all options If it was known that a document contained these words, would there be any idea what the document was about? Probably not. These types of words are called stop words. Specifically, they are very high frequency words that do not contain content information. They are used because they're grammatically required, such as when connecting sentences. Since these words do not contain any topical information, they are often removed as a preprocessing step in text analysis. Not only are these (usually) useless words ignored, but having less data can mean that algorithms run faster! ./profile config.toml two-cities.txt --stop Now, use the profile program to remove stop words from the document twocities.txt. Can you still get an idea of what the book is about without these words present? 