Linear classifiers are inherently binary classifiers. Consider the following linear classifier: It takes the dot product between the unseen document vector and the weights vector w (where |w| = |x| = |V |). Training a linear classifier is learning (setting) the values in the weights vector w such that dotting it with a document vector produces a value greater than 0 for a positive instance and less than zero for a negative instance. There are many such algorithms, including the state-of-the-art support vector machines (SVM) classifier [Campbell and Ying 2011]. We call this group of learning algorithms linear classifiers because their decision is based on a linear combination of feature weights (w and x). Figure 15.4 shows how the dot product combination creates a decision boundary between two label groups plotted in a simple two-dimensional example. Two possible decision boundaries Of course, it's possible that not all data points are linearly separable, so the decision boundary will be created such that it splits the two classes as accurately as possible. Naive Bayes can also be shown to be a linear classifier. This is in contrast to k-NN-since it only considers a local subspace in which the query is plotted, it ignores the rest of the corpus and no lines are drawn. Some more advanced methods such as the kernel trick may change linear classifiers into nonlinear classifiers, but we refer the reader to a text more focused on machine learning for the details [Bishop 2006]. In this book, we choose to examine the relatively simple perceptron classifier, on which many other linear classifiers are based. We need to specify several parameters which are used in the training algorithm. Let T represent the maximum number of iterations to run training for. Let Î± > 0 be the learning rate. The learning rate controls by how much we adjust the weights at each step. We may terminate training early if the change in w is small; this is usually measured by comparing the norm of the current iteration's weights to the norm of the previous iteration's weights. There are many discussions about the choice of learning rate, convergence criteria and more, but we do not discuss these in this book. Instead, we hope to familiarize the reader with the general spirit of the algorithm, and again refer the reader to Bishop [2006] for many more details on algorithm implementation and theoretical properties. 