Presence and absence of w1: p(X w1 = 1) + p(X w1 = 0) = 1 Presence and absence of w2: p(X w2 = 1) + p(X w2 = 0) = 1 Co-occurrences of w1 and w2: Constraints: p(X w1 = 0, X w2 = 1) + p(X w1 = 0, X w2 = 0) = p(X w1 = 0) p(X w1 = 1, X w2 = 1) + p(X w1 = 0, X w2 = 1) = p(X w2 = 1) p(X w1 = 1, X w2 = 0) + p(X w1 = 0, X w2 = 0) = p(X w2 = 0) Figure 13.12 Constraints on probabilities in the mutual information function. or absence of this word. These all sum to one as well. Finally, we have a lot of joint probabilities that represent the scenarios of co-occurrences of the two words. They also sum to one because the two words can only have the four shown possible scenarios. Once we know how to calculate these probabilities, we can easily calculate the mutual information. It's important to note that there are some constraints among these probabilities. The first was that the marginal probabilities of these words sum to one. The second was that the two words have these four scenarios of co-occurrence. The additional constraints are listed at the bottom of Figure 13.12. 