As in the case of the simple mixture model, the process of generating a word still consists of two steps. The first is to choose a component model to use; this decision is controlled by both a parameter λ B (denoting the probability of choosing the background model) and a set of π d , i (denoting the probability of choosing topic θ i if we decided not to use the background model). If we do not use the background model, we must choose one from the k topics, which has the constraint k i=1 π d , i = 1. Thus, the probability of choosing the background model is λ B while the probability of choosing topic θ i is (1 − λ B )π d , i . Once we decide which component word distribution to use, the second step in the generation process is simply to draw a word from the selected distribution, exactly the same as in the simple mixture model. As usual, once we design the generative model, the next step is to write down the likelihood function. We ask the question: what's the probability of observing a word from such a mixture model? As in the simple mixture model, this probability is a sum over all the different ways to generate the word; we have a total of k + 1 different component models, thus it is a sum of k + 1 terms, where each term captures the probability of observing the word from the corresponding component word distribution, which can be further written as the product of the probability Coverage of topic θ j in doc d Figure 17.30 The likelihood function of PLSA. of selecting the particular component model and the probability of observing the particular word from the particular selected word distribution. The likelihood function is as illustrated in Figure 17.30. Specifically, the probability of observing a word from the background distribution is λ B p(w | θ B ), while the probability of observing a word from a topic θ j is (1 − λ B )π d , j p(w | θ j ). The probability of observing the word regardless of which distribution is used, p d (w), is just a sum of all these cases. Assuming that the words in a document are generated independently, it follows that the likelihood function for document d is the second equation in Figure 17.30, and that the likelihood function for the entire collection C is given by the third equation. What are the parameters in PLSA? First, we see λ B , which represents the percentage of background words that we believe exist in the text data (and that we would like to factor out). This parameter can be set empirically to control the desired discrimination of the discovered topic models. Second, we see the background language model p(w | θ B ), which we also assume is known. We can use any large collection of text, or use all the text that we have available in collection C to estimate p(w | θ B ) (e.g., assuming all the text data are generated from θ B , we can use the ML estimate to set p(w | θ B ) to the normalized count of word w in the data). Third, we see π d , j , which indicates the coverage of topic θ j in document d. This parameter encodes the knowledge we hope to discover from text. Finally, we see the k word distributions, 