Pointwise Mutual Information (PMI) treats word occurrences as random variables and quantifies the probability of their co-occurrence within some context of a window of words. For example, to find words that co-occur with w i using a window of size n, we look at the words w i−n , . . . , w i−1 , w i , w i+1 , . . . , w i+n . This allows us to calculate the probability of w i and w j co-occurring, which is represented as the joint probability p(w i , w j ). Along with the individual probabilities p(w i ) and p(w j ), we can write the formula for PMI: Note that if w i and w j are independent, then p(w i )p(w j ) = p(w i , w j ). This forces us to take a logarithm of 1, which yields a PMI of zero; there is no measure of information transferred by two independent words. If, however, the probability of observing the two words occurring together, i.e., p(w i , w j ) is substantially larger than their expected probability of co-occurrence if there were independent, i.e., p(w i )p(w j ), then the PMI would be high as we would expect. Depending on our application, we can define the context as the aforementioned window of size n, a sentence, a document, and so on. Changing the context modifies the interpretation of PMI-for example, if we only considered a context to be of size n = 1, we will get significantly different results than if we set the context to be an entire document from the corpus. In order to have comparable PMI scores, we also need to ensure that our PMI measure is symmetric; this again depends on our definition of context. If we define context to be "w j follows w i ", then pmi(w i , w j ) = pmi(w j , w i ), which is required to cluster terms. It is possible to normalize the PMI score in the range [0, 1]: (14.8) making comparisons between different word pairs possible. However, this normalization doesn't fix a major issue in the PMI formula itself. Imagine that we have a rare word that always occurs in the context of another (perhaps very common) word. It would seem that this word pair is very highly related, but in fact our data is just too sparse to model the connection appropriately. This problem can be alleviated by using the mutual information measure introduced in Chapter 13 which considers not just the case when the rare word is observed, but also the case when it is not observed. Indeed, since mutual information is bounded by the entropy of one of the two variables, and a rare word has very low entropy, it generally wouldn't have a high mutual information with any other word. Despite their drawbacks, however, PMI and nPMI are often used in practice and are also useful building blocks for more advanced methods as well as allowing us to understand the basic idea behind information capture in word co-occurrence. We thus included a discussion in this book. Below we will briefly introduce two advanced methods for term clustering. The windowing technique employed here is critical in both of the following advanced methods. 