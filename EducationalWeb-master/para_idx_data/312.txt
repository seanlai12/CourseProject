However, representing a topic by a distribution over words can involve many words to describe a topic and model subtle differences of topics. Through adjusting probabilities of different words, we may model variations of the general "sports" topic to focus more on a particular kind of sports such as basketball (where we would expect basketball to have a very high probability) or football (where football would have a much higher probability than basketball). Similarly, in the distribution for "travel," we see top words like attraction, trip, flight, and so on. In "science," we see scientist, spaceship, and genomics, which are all intuitively related to the corresponding topic. It is important to note that it doesn't mean sports-related terms will necessarily have zero probabilities in a distribution representing the topic "science," but they generally have much lower probabilities. Note that there are some words that are shared by these topics, meaning that they have reasonably high probabilities for all these topics. For example, the word travel occurred in the top-word lists for all the three topics, but with different probabilities. It has the highest probability for the "travel" topic, 0.05, but with much smaller probabilities for "sports" and "science," which makes sense. Similarly, you can see star also occurred in "sports" and "science" with reasonably high probabilities because the word is actually related to both topics due to its ambiguous nature. We have thus seen that representing a topic by a word distribution effectively addresses all the three problems of a topic as a single term mentioned earlier. . It now uses multiple words to describe a topic, allowing us to describe fairly complicated topics. . It assigns weights to terms, enabling the modeling of subtle differences of semantics in related topics. We can also easily bring in related words together to model a topic and estimate the coverage of the topic. . Because we have probabilities for the same word in different topics, we can accommodate multiple senses of a word, addressing the issue of word ambiguity. Next, we examine the task of discovering topics represented in this way. Since the representation is a probability distribution, it is natural to use probabilistic models for discovering such word distributions, which is referred to as probabilistic topic modeling. When using a word distribution to denote a topic, our task of topic analysis can be further refined based on the formal definition in Figure 17.3 by making each topic a word distribution. That is, each θ i is now a word distribution, and we have 338 Chapter 17 Topic Analysis w∈V p(w | θ i ) = 1. (17.1) Naturally, we still have the same constraint on the topic coverage, i.e., k j =1 As a computation problem, our input is text data, a collection of documents C, and we assume that we know the number of topics, k, or hypothesize that there are k topics in the text data. As part of our input, we also know the vocabulary V , which determines what units would be treated as the basic units (i.e., words) for analysis. In most cases, we will use words as the basis for analysis simply because they are the most natural units, but it is easy to generalize such an approach to use phrases or any other units that we can identify in text, as the basic units and treat them as if they were words. Our output consists of two families of probability distributions. The first is a set of topics represented by a set of θ i 's, each of which is a word distribution. The second is a topic coverage distribution for each document The question now is how to generate such output from our input. There are potentially many different ways to do this, but here we introduce a general way of solving this problem called a generative model. This is, in fact, a very general idea and a principled way of using statistical modeling to solve text mining problems. The basic idea of this approach is to first design a generative model for our data, i.e., a probabilistic model to model how the data are generated, or a model that can allow us to compute the probability of how likely we will observe the data we have. The actual data aren't necessarily (indeed often unlikely) generated this way, but by assuming the data to be generated in a particular way according to a particular model, we can have a formal way to characterize our data which further facilitates topic discovery. In general, our model will have some parameters (which can be denoted by ); they control the behavior of the model by controlling what kind of data would have high (or low) probabilities. If you set these parameters to different values, the model would behave differently; that is, it would tend to give different data points high (or low) probabilities. We design the model in such a way that its parameters would encode the knowledge we would like to discover. Then, we attempt to estimate these parameters based on the data (or infer the values of parameters based on the observed data) so as to generate the desired output in the form of parameter values, which we have 