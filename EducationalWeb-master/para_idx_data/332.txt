Hidden variable (= topic indicator): z d,w 2 {B, 1, 2, …, k} p(z d,w  from topic θ j , but rather this probability conditioned on having not chosen the background model. In other words, the probability of generating a word using θ j is (1 − p(z d , w = B))p(z d , w = j). In the case of having just one topic other than the background model, we would have p(z d , w = j) = 1 only for θ j . Note that we use document d here to index the word w. In our model, whether w has been generated from a particular topic actually depends on the document! Indeed, the parameter π d , j is tied to each document, and thus each document can have a potentially different topic coverage distribution. Such an assumption is reasonable as different documents generally have a different emphasis on specific topics. This means that in the E-step, the inferred probability of topics for the same word can be potentially very different for different documents since different documents generally have different π d , j values. The M-step is also similar to that in the simple mixture model. We show the equations in Figure 17.33. We see that a key component in the two equations, for re-estimating π and p(w | θ) respectively, is c(w, d)(1 − p(z d , w = B))p(z d , w = j), which can be interpreted as the allocated counts of w to topic θ j . Intuitively, we use the inferred distribution of z values from the E-step to split the counts of w among all the distributions. The amount of split counts of w that θ j can get is determined based on the inferred likelihood that w is generated by topic θ j . Once we have such a split count of each word for each distribution, we can easily pool together these split counts to re-estimate both π and p(w | θ), as shown in Figure 17.33. To re-estimate π d , j , the probability that document d covers topic θ j , 