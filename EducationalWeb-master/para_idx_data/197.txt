In evaluation setups for collaborative filtering, we have a set P of pairs of predicted ratingsr and actual ratings r across all user-item pairs. A very common measure called root-mean squared error (RMSE) has a mathematical formula that follows its name: A similar metric is mean absolute error (MAE), defined as Due to the square in RMSE, RMSE is more affected by larger errors. The similarity between RMSE and MAE should remind the reader of the differences between gMAP and MAP. Both these measures quantify the difference in values between r and the true rating r. Using such a measure is natural when we have ratings on some ordinal scale. One important note is that these measures only capture the accuracy of predicted ratings; in an actual recommender system, the top k elements are recommended to the user. Despite having a low RMSE or MAE, it may be possible that the top documents may not actually be relevant to the user. In the extreme case where k = 1, we may recommend the one element that received an erroneous high score. To combat this issue, we can rank all ratings for a particular user and then use an information retrieval metric such as NDCG to view the list as a whole when compared to the true rating r. In information filtering tasks, a system pushes items to a user if it thinks the user would like the item. In this case, there are no explicit ratings for each item, but rather a relevant vs not-relevant judgement. Once the user sees the item, the user then determines if the suggestion was a good one. For a single user, it's easy to see that we can use some evaluation metrics from information retrieval, as discussed in Chapter 9. Since items are pushed to users as soon as they become available, we can't use any rank-sensitive measures. Still, we can examine the set of all documents pushed to the user in some time period and compute statistics like precision, recall, F 1 score, and other similar measures. In a true information filtering system, there will be many users who receive all pushed items. A simple way to aggregate scores would be to take an average of the individual user metrics, e.g., average F 1 score across all users. However, this may not be the best measure if some users have more recommendations than others. Since θ is set on a per-user basis, different users will aggregate different numbers of seen documents. Furthermore, in a real system users may not all join at the same time, or some users may have more training data available than others if they have more complete user profiles or if they have been in the system longer. For these reasons, it could be advantageous to instead take a weighted average of user metrics as an overall metric. The weight may be assigned such that all weights sum to one, and each user's weight is determined by that user's total number of judgements. It may also be interesting to compute the precision or recall over time, where time is measured as the number of documents that the filtering system has seen. A variant of this is to measure time based on the number of elements judged by the user (which are only those elements that are shown to the user). Ideally, as the number of documents increases, the overall precision (or precision of the last k documents) should increase. Once the precision has reached a flat line, we are most likely at θ opt given the current system setup. Of course, this learning-over-time evaluation can also be generalized to multiple users in the same way as previously discussed. As a final note for both recommender system types, it is valuable to find those users affecting the evaluation metric the most. That is, are there any outliers that cause the evaluation metric to be significantly lower than expected? If so, these Exercises 235 users can be further investigated and the overall system may be adjusted to ensure their satisfaction. Of course, these outliers depend heavily on the evaluation metric used, so using multiple metrics will give the most complete view of user satisfaction. 