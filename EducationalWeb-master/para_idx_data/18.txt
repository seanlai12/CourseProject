One potential problem of MLE is that it is often inaccurate when the size of the data sample is small since it always attempts to fit the data as well as possible. Consider an extreme example of observing just two data points of flipping a coin which happen to be all heads. The MLE would say that the probability of heads is 1.0 while the probability of tails is 0. Such an estimate is intuitively inaccurate even though it maximizes the probability of the observed two data points. This problem of "overfitting" can be addressed and alleviated by considering the uncertainty on the parameter and using Bayesian parameter estimation instead of MLE. In Bayesian parameter estimation, we consider a distribution over all the possible values for the parameter; that is, we treat the parameter itself as a random variable. Specifically, we may use p(θ) to represent a distribution over all possible values for θ, which encodes our prior belief about what value is the true value of θ , while the data D provide evidence for or against that belief. The prior belief p(θ) can then be updated based on the observed evidence. We'll use Bayes' rule to rewrite p(θ | D), or our belief of the parameters given data, as , (2.8) where p(D) can be calculated by summing over all configurations of θ. For a continuous distribution, that would be 2.1 Basics of Probability and Statistics 29 p(D) = θ p(θ )p(D | θ )dθ (2.9) which means the probability for a particular θ is (2.10) We have special names for these quantities: The last one is called the marginal likelihood because the integration "marginalizes out" (removes) the parameter θ from the equation. Since the likelihood of the data remains constant, observing the constraint that p(θ | D) must sum to one over all possible values of θ , we usually just say That is, the posterior is proportional to the prior times the likelihood. The posterior distribution of the parameter θ fully characterizes the uncertainty of the parameter value and can be used to infer any quantity that depends on the parameter value, including computing a point estimate of the parameter (i.e., a single value of the parameter). There are multiple ways to compute a point estimate based on a posterior distribution. One possibility is to compute the mean of the posterior distribution, which is given by the weighted sum of probabilities and the parameter values. For a discrete distribution, (2.11) while in a continuous distribution, (2.12) Sometimes, we are interested in using the mode of the posterior distribution as our estimate of the parameter, which is called Maximum a Posteriori (MAP) estimate, given by: (2.13) 