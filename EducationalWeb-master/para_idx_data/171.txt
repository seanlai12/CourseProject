There are many different significance tests that we can use to quantify the likelihood that the observed difference in the results is simply due to random fluctuations. A particularly interesting test is the Wilcoxon signed-rank test. It's a nonparametric test, and we not only look at the signs, but also consider the magnitude of the difference in scores. Another is the (parametric) t-test where a normal distribution is assumed. In any event, we would draw a similar conclusion in our example case: the outcome is very likely to be random. For further study on this and other statistical significance tests, we suggest the reader start with Smucker et al. [2007]. To illustrate the concept of p-values, consider the distribution in Figure 9.10. This is a normal distribution, with a mean of zero in the center. Say we started with the assumption that there's no difference between the two systems. But, we assume that because of random fluctuations depending on the queries we might observe a difference; thus, the actual difference might be on the left side or right side. This curve shows the probability that we would observe values that are deviating from zero here when we subtract system A's MAP from system B's MAP (or even vice versa). Based on the picture, we see that if a difference is observed at the dot shown in the figure, then the chance is very high that this is in fact a random observation. We can define the region of likely observation due to random fluctuation; we usually use the value 95% of all outcomes. Inside this interval the observed values are from random fluctuation with 95% chance. If you observe a value in the tails on the side, then the difference is unlikely from random fluctuation (only 5% likely). This 95% value determines where the lines are drawn on the x axis. If we are only confident in believing a 1% chance is due to random fluctuations, then the vertical lines are redrawn farther from the mean; determining the exact x values where the lines are drawn depends on the specific significance test used. The takeaway message here is that we need to use many queries to avoid jumping to an incorrect conclusion that one system is better than another. There are many different ways of doing this statistical significance test, which is essentially determining where to place the boundary lines between random chance and an actual difference in systems. Now, let's discuss the problem of making relevance judgements. As mentioned earlier, it's very hard to judge all the documents completely unless it is a very small data set. The question is, if we can't afford judging all the documents in the collection, which subset should we judge? The solution here is pooling. This is a strategy that has been used in many cases to solve this problem. First, choose a diverse set of ranking methods; these are different types of retrieval systems. We hope these methods can help us nominate likely relevant documents. The goal is to pick out the relevant documents so the users can make judgements on them. That way, we would have each system return the top k documents according to its ranking function. The k value can vary between systems, but the point is to ask them to suggest the most likely relevant documents. We then simply combine all these top k sets to form a pool of documents for human assessors to judge. Of course, there will be many duplicated documents since many systems might have retrieved Bibliographic Notes and Further Reading 187 the same documents. There are also unique documents that are only returned by one system, so the idea of having a diverse set of result ranking methods is to ensure the pool is broad. We can include as many possible random documents as possible. Then, the human assessors would make complete judgements on this data set, or pool. The remaining unjudged documents are assumed to be non-relevant and the human annotators do not need to spend time and effort manually judging them. If the pool is large enough, this assumption is perfectly fine. That means if your system participates in contributing to the pool then it's unlikely that it will be penalized since the top-ranked documents have all been judged. However, this is problematic for evaluating a new system that may not have contributed to the pool, since the documents it returns may not have been judged and are assumed to be non-relevant. What we haven't covered are some other evaluation strategies such as A-B testing; this is where an evaluating system would mix the results of two methods randomly, showing the mix of results to users. Of course, the users don't see which result is from which method, so the users would judge those results or click on those documents in a search engine application. In this case, then, the system can keep track of the clicked documents, and see if one method has contributed more to the clicked documents. If the user tends to click on one of the results from one method, then that method may be better. A-B testing can also be used to compare two different retrieval interfaces. Text retrieval evaluation is extremely important since the task is empirically defined. If we don't rely on users, there's no way to tell whether one method works better. If we have an inappropriate experiment design, we might misguide our research or applications, drawing the wrong conclusions. The main strategy is the Cranfield evaluation methodology for all kinds of empirical evaluation tasks (not just for search engines). MAP and NDCG are the two main measures that you should definitely know about since you will see them often in research papers. Finally, retrieving up to ten documents (or some small number) is easier to interpret from a user's perspective since this is the number of documents they would likely see in a real application. 