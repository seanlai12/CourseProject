Algorithmic hierarchical clustering methods using linkage measures tend to be easy to understand and are often efficient in clustering. They are commonly used in many clustering analysis applications. However, algorithmic hierarchical clustering methods can suffer from several drawbacks. First, choosing a good distance measure for hierarchical clustering is often far from trivial. Second, to apply an algorithmic method, the data objects cannot have any missing attribute values. In the case of data that are partially observed (i.e., some attribute values of some objects are missing), it is not easy to apply an algorithmic hierarchical clustering method because the distance computation cannot be conducted. Third, most of the algorithmic hierarchical clustering methods are heuristic, and at each step locally search for a good merging/splitting decision. Consequently, the optimization goal of the resulting cluster hierarchy can be unclear. Probabilistic hierarchical clustering aims to overcome some of these disadvantages by using probabilistic models to measure distances between clusters. One way to look at the clustering problem is to regard the set of data objects to be clustered as a sample of the underlying data generation mechanism to be analyzed or, formally, the generative model. For example, when we conduct clustering analysis on a set of marketing surveys, we assume that the surveys collected are a sample of the opinions of all possible customers. Here, the data generation mechanism is a probability distribution of opinions with respect to different customers, which cannot be obtained directly and completely. The task of clustering is to estimate the generative model as accurately as possible using the observed data objects to be clustered. In practice, we can assume that the data generative models adopt common distribution functions, such as Gaussian distribution or Bernoulli distribution, which are governed by parameters. The task of learning a generative model is then reduced to finding the parameter values for which the model best fits the observed data set. Example 10.6 Generative model. Suppose we are given a set of 1-D points X = {x 1 , . . . , x n } for clustering analysis. Let us assume that the data points are generated by a Gaussian distribution, where the parameters are µ (the mean) and σ 2 (the variance). The probability that a point x i ∈ X is then generated by the model is Consequently, the likelihood that X is generated by the model is The task of learning the generative model is to find the parameters µ and σ 2 such that the likelihood L(N (µ, σ 2 ) : X) is maximized, that is, finding Given a set of objects, the quality of a cluster formed by all the objects can be measured by the maximum likelihood. For a set of objects partitioned into m clusters C 1 , . . . , C m , the quality can be measured by where P() is the maximum likelihood. If we merge two clusters, C j 1 and C j 2 , into a cluster, C j 1 ∪ C j 2 , then, the change in quality of the overall clustering is When choosing to merge two clusters in hierarchical clustering, m i=1 P(C i ) is constant for any pair of clusters. Therefore, given clusters C 1 and C 2 , the distance between them can be measured by A probabilistic hierarchical clustering method can adopt the agglomerative clustering framework, but use probabilistic models (Eq. 10.20) to measure the distance between clusters. Upon close observation of Eq. (10.19), we see that merging two clusters may not always lead to an improvement in clustering quality, that is, P(C j 1 )P(C j 2 ) may be less than 1. For example, assume that Gaussian distribution functions are used in the model of Figure 10.11. Although merging clusters C 1 and C 2 results in a cluster that better fits a Gaussian distribution, merging clusters C 3 and C 4 lowers the clustering quality because no Gaussian functions can fit the merged cluster well. Based on this observation, a probabilistic hierarchical clustering scheme can start with one cluster per object, and merge two clusters, C i and C j , if the distance between them is negative. In each iteration, we try to find C i and C j so as to maximize log The iteration continues as long as log P(C i ∪C j ) P(C i )P(C j ) > 0, that is, as long as there is an improvement in clustering quality. The pseudocode is given in Figure 10.12. Probabilistic hierarchical clustering methods are easy to understand, and generally have the same efficiency as algorithmic agglomerative hierarchical clustering methods; in fact, they share the same framework. Probabilistic models are more interpretable, but sometimes less flexible than distance metrics. Probabilistic models can handle partially observed data. For example, given a multidimensional data set where some objects have missing values on some dimensions, we can learn a Gaussian model on each dimension independently using the observed values on the dimension. The resulting cluster hierarchy accomplishes the optimization goal of fitting data to the selected probabilistic models. A drawback of using probabilistic hierarchical clustering is that it outputs only one hierarchy with respect to a chosen probabilistic model. It cannot handle the uncertainty of cluster hierarchies. Given a data set, there may exist multiple hierarchies that Figure 10.11 Merging clusters in probabilistic hierarchical clustering: (a) Merging clusters C 1 and C 2 leads to an increase in overall cluster quality, but merging clusters (b) C 3 and (c) C 4 does not. Algorithm: A probabilistic hierarchical clustering algorithm. 