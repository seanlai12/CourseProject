We have 30 articles on the Iraq war and 26 articles on the Afghanistan war. Our goal is to compare the two sets of articles to discover the common topics shared by the two sets and understand the variations of these topics in each set. The results in Figure 19.6 show that CPLSA can discover meaningful topics. The first column (Cluster 1) shows a very meaningful common topic about the United Nations on the first row, which intuitively makes sense given the topics of these two collections. Such a topic may not be surprising to people who know the background about these articles, but the result shows that CPLSA can automatically discover it. What's more interesting, however, is the two cells of word distributions shown on the second and third rows in the first column, right below the cell about the United Nations. These two cells show the Iraq-specific view of the topic about the United Nations and the Afghanistan view of the same topic, respectively. The results show that in the Iraq War, the United Nations was more involved in weapon inspections, whereas in the Afghanistan War, it was more involved in, perhaps, aid to the Northern Alliance. These two context-specific views of the topic of the United Nations show different variations of the topic in the two wars, and reveal a more detailed understanding of topics in a context-specific way. This table is not only immediately useful for understanding the major topics and their variations in these two sets of news articles, but can also serve as entry points to facilitate browsing into very specific topics in the text collection; e.g., each cell can be made clickable to enable a user to examine the relevant discussion in the news articles in detail. The second column shows another shared common topic about fatalities, which again should not be surprising given that these articles are about wars. It also again confirms that CPLSA is able to extract meaningful common topics. As in the case of the first column, the collection-specific topics in the third column also further show the variations of the topic in these two different contexts. In Figure 19.7, we show the temporal trends of topics discovered from blog articles about Hurricane Katrina. The x-axis is the time, and the y-axis is the coverage of a topic. The plots are enabled directly by the parameters of CPLSA where we used time and location as context. In Figure 19.7, we show a visualization of the trends of topics over time. The top plot shows the temporal trends of two topics. One is oil price, and one is about the flooding of the city of New Orleans. The plot is based on the conditional probability of a topic given a particular time period, which is one of the parameters in CPLSA for capturing time-dependent coverage of topics. Here we see that initially the two curves tracked each other very well. Interestingly, later, the topic of New Orleans was mentioned again but oil prices was not. This turns out to be the time period when another hurricane (Hurricane Rita) hit the region, which apparently triggered more discussion about the flooding of the city, but not the discussion of oil price. The bottom figure shows the coverage of the topic about flooding of the city New Orleans by blog article authors in different locations (different states in the U.S.). We see that the topic was initially heavily covered by authors in the victim areas (e.g., Louisiana), but the topic was then picked up by the authors in Texas, which might be explained by the move of people from the state of Louisiana to Texas. Thus, these topical trends not only are themselves useful for revealing the topics and their dynamics over time, but also enable comparative analysis of topics across different contexts to help discover interesting patterns and potentially interesting events associated with the patterns. In Figure 19.8, we show spatiotemporal patterns of the coverage of the topic of government response in the same data set of blog articles about Hurricane Katrina. These visualizations show the distribution of the coverage of the topic in different weeks of the event over the states in the U.S. We see that initially, the coverage is concentrated mostly in the victim states in the south, but the topic gradually spread to other locations over time. In week four (shown on the bottom left), the coverage distribution pattern was very similar to that of the first week (shown on the top left). This can again be explained by Hurricane Rita hitting the region around  that time. These results show that CPLSA can leverage location and time as context to reveal interesting topical patterns in text data. Note that the CPLSA model is completely general, so it can be easily applied to other kinds of text data to reveal similar spatiotemporal patterns or topical patterns. In Figure 19.9, we show yet another application of CPLSA for analysis of the impact of an event. The basic idea is to compare the views of topics covered in text before and after an event so as to reveal any difference.  assumed to be potentially related to the impact of the event. The results shown here are the topics discovered from research articles on information retrieval, particularly SIGIR papers. The topic we are focusing on is about the retrieval models (shown on the left). The goal is to analyze the impact of two events. One is the launch of the Text and Retrieval Conference (TREC) around 1992, a major annual evaluation effort sponsored by the U.S. government, which is known to have made a huge impact on the topics of research in information retrieval. The other event is the publication of a seminal paper in 1998 by Ponte and Croft [1998], in which the language modeling approach to information retrieval was introduced. The paper is also known to have made a high impact on information retrieval research. To understand the impact of these two events, we can use time periods before and after an event as different contexts and apply CPLSA. The results on the top show that before TREC, the study of retrieval models was mostly on the vector space model and Boolean models, but after TREC, the study of retrieval models apparently explored a variety application tasks (e.g., XML 428 Chapter 19 Joint Analysis of Text and Structured Data retrieval, email retrieval, and subtopic retrieval, which are connected to some tasks introduced in TREC over the years). The results on the bottom show that before the language modeling paper was published in 1998, the study of retrieval models focused on probabilistic, logic, and Boolean models, but after 1998, there was a clear focus on language modeling approaches and parameter estimation, which is an integral part of studies of language models for IR. Thus, these results can help potentially reveal the impact of an event as reflected in the text data. 