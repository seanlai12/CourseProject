Constrained Optimization: each representing a topic p(w | θ j ). This parameter also encodes the knowledge we would like to discover from the text data. Can you figure out how many unknown parameters are there in such a PLSA model? This would be a useful exercise to do, which helps us understand what exactly are the outputs that we would generate by using PLSA to analyze text data. After we have obtained the likelihood function, the next question is how to perform parameter estimation. As usual, we can use the Maximum Likelihood estimator as shown in Figure 17.31, where we see that the problem is essentially a constrained optimization problem, as in the case of the simple mixture model, except that: . we now have a collection of text articles instead of just one document; . we have more parameters to estimate; and . we have more constraint equations (which is a consequence of having more parameters). Despite the third point, the kinds of constraints are essentially the same as before; namely, there are two. One ensures the topic coverage probabilities sum to one for each document over all the possible topics, and the other ensures that the probabilities of all the words in each topic sum to one. As in the case of simple mixture models, we can also use the EM algorithm to compute the ML estimate for PLSA. In the E-step, we have to introduce more hidden variables because we have more topics. Our hidden variable z, which is a topic indicator for a word, now would take k + 1 values {1, 2, . . . , k, B}, corresponding to the k topics and the extra background topic. The E-step uses Bayes' Rule to infer the probability of each value for z, as shown in Figure 17.32. A comparison between these equations as the E-step for the simple two-component mixture model would reveal immediately that the equations are essentially similar, only now we have more topics. Indeed, if we assume there is just one topic, k = 1, then we would recover the E-step equation of the simple mixture model with just one small difference: p(z d , w = j) is not quite the probability that the word is generated Probability that w in doc d is generated from topic θ j Probability that w in doc d is generated from background θ B 