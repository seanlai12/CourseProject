Which heuristics from the vector space models are captured in the general smoothed query likelihood formula? 6.20. Is the following formula an acceptable scoring function? Why or why not? . n n avg , Exercises 131 where: k > 0 is some parameter; c(w, C) and c(w, d) are the count of the current word in the collection and current document, respectively; N is the total number of documents; df(w) is the number of documents that the current word w appears in; n is the document length of the current document; n avg is the average document length of the corpus. In this chapter, we will discuss feedback in a TR system. Feedback takes the results of a user's actions or previous search results to improve retrieval results. This is illustrated in Figure 7.1. As shown, feedback is often implemented as updates to a query, which alters the list of returned documents. We can see the user would type in a query and then the query would be sent to a standard search engine, which returns a ranked list of results (we discussed this in depth in Chapter 6). These search results would be shown to the user. The user can make judgements about whether each returned document is useful or not. For example, the user may say one document is good or one document is not very useful. Each decision on a document is called a relevance judgment. This overall process is a type of relevance feedback, because we've got some feedback information from the user based on the judgements of the search results. As one would expect, this can be very useful to the retrieval system since we should be able to learn what exactly is interesting to a particular user or users. The feedback module would then take these judgements as input and also use the document collection to try to improve future rankings. As mentioned, it would typically involve updating the query so the system can now rank the results more accurately for the user; this is the main idea behind relevance feedback. These types of relevance judgements are reliable, but the users generally don't want to make extra effort unless they have to. There is another form of feedback called pseudo relevance feedback, or blind feedback. In this case, we don't have to involve users since we simply assume that the top k ranked documents are relevant. Let's say we assume the top k = 10 documents are relevant. Then, we will use these documents to learn and to improve the query. But how could this help if the topranked documents are random? In fact, the top documents are actually similar to relevant documents, even if they are not relevant. Otherwise, how would they have appeared high in the ranked list? So, it's possible to learn some related terms to the query from this set anyway regardless whether the user says that a document is relevant or not. You may recall that we talked about using language models to analyze word associations by learning related words to the word computer (see Chapter 3). First, we used computer to retrieve all the documents that contain that word. That is, imagine the query is computer. Then, the results will be those documents that contain computer. We take the top k results that match computer well and we estimate term probabilities (by counting them) in this set for our topic language model. Lastly, we use the background language model to choose the terms that are frequent in this retrieved set but not frequent in the whole collection. If we contrast these two ideas, what we can find is that we'll learn some related terms to computer. These related words can then be added to the original query to expand the query, which helps find documents that don't necessarily match computer, but match other words like program and software that may not have been in the original query. 