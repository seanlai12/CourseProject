What parameters do we have in such a two-component mixture model? In Figure 17.16, we summarize the mixture of two unigram language models, list all the parameters, and illustrate the parameter estimation problem. First, our data is just one document d, and the model is a mixture model with two components. Second, the parameters include two unigram language models and a distribution (mixing weight) over the two language models. Mathematically, θ d denotes the topic of document d while θ B represents the background word distribution, which we can set to a fixed word distribution with high probabilities on common words. We denote all the parameters collectively by . (Can you see how many parameters exactly we have in total?) The figure also shows the derivation of the likelihood function. The likelihood function is seen to be a product over all the words in the document, which is exactly the same as in the case of a simple unigram language model. The only difference is that inside the product, it's now a sum instead of just one probability as in the simple unigram language model. We have this sum due to the mixture model where we have an uncertainty in using which model to generate a data point. Because of this uncertainty, our likelihood function also contains a parameter to denote the probability of choosing each particular component distribution. The second line of the equation for the likelihood function is just another way of writing the product, which is now a product over all the unique words in our vocabulary instead of over all the positions in the document as in the first line of the equation. We have two types of constraints: one is that all the word distributions must sum to one, and the other constraint is that the probabilities of choosing each topic must sum to one. The maximum likelihood estimation problem can now be seen as a constrained optimization problem where we seek parameter values that can maximize the likelihood function and satisfy all the constraints. 