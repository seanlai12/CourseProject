With the E-step and M-step as the basis, the EM algorithm works as follows. First, we initialize all the (unknown) parameters values randomly. This allows us to have a complete specification of the mixture model, which further enables us to use Bayes' rule to infer which distribution is more likely to generate each word. This prediction (i.e., E-step) essentially helps us (probabilistically) separate words generated by the two distributions. Finally, we will collect all the probabilistically allocated counts of words belonging to our topic word distribution and normalize them into probabilities, which serve as an improved estimate of the parameters. The process can then be repeated to gradually improve the parameter estimate until the likelihood function reaches a local maximum. The EM algorithm can guarantee reaching such a local maximum, but it cannot guarantee reaching a global maximum when there are multiple local maxima. Due to this, we usually repeat the algorithm multiple times with different initializations in practice, using the run that gives the highest likelihood value to obtain the estimated parameter values. The EM algorithm is illustrated in Figure 17.24 where we see that a binary hidden variable z has been introduced to indicate whether a word has been generated from the background model (z = 1) or the topic model (z = 0). For example, the illustration shows that the is generated from background, and thus the z value is 1.0, while text is from the topic, so its z value is 0. Note that we simply assumed (imagined) the existence of such a binary latent variable associated with each word but we don't really observe these z values. This is why we referred to such a variable as a hidden variable. A main idea of EM is to leverage such hidden variables to simplify the computation of the ML estimate since knowing the values of these hidden variables makes the ML estimate trivial to compute; we can pool together all the words whose z values are 0 and normalize their counts. Knowing z values can potentially help simplify the task of computing the ML estimate, and EM exploits this fact by alternating the E-step and M-step in each iteration so as to improve the parameter estimate in a hill-climbing manner. Specifically, the E-step is to infer the value of z for all the words, while the M-step is to use the inferred z values to split word counts between the two distributions, and use the allocated counts for θ d to improve its estimation, leading to a new generation of improved parameter values, which can then be used to perform a new iteration of E-step and M-step to further improve the parameter estimation. In the M-step, we adjust the count c(w, d) based on p(z = 0 | w) (i.e., probability that the word w is indeed from θ d ) so as to obtain a discounted count c(w, d)p(z = 0 | w) which can be interpreted as the expected count of the event that word w is generated from θ d . Similarly, θ B has its own share of the count, which is c (w, d) 5) showing that all the counts of word w have been split between the two distributions. Thus, the M-step is simply to normalize these discounted counts for all the words to obtain a probability distribution over all the words, which can then be regarded as our improved estimate of p(w | θ d ). Note that in the M-step, if p(z = 0 | w) = 1 for all words, we would simply compute the simple single unigram language model based on all the observed words (which makes sense since the E-step would have told us that there is no chance that any word has been generated from the background). In Figure 17.25, we further illustrate in detail what happens in each iteration of the EM algorithm. First, note that we used superscripts in the formulas of the Estep and M-step to indicate the generation of parameters. Thus, the M-step is seen to use the n-th generation of parameters together with the newly inferred z values to obtain a new (n + 1)th generation of parameters (i.e., p n+1 (w | θ d )). Second, we assume the two component models (θ d and θ B ) have equal probabilities; we also assume that the background model word distribution is known (fixed as shown in the third column of the table). The computation of EM starts with preparation of relevant word counts. Here we assume that we have just four words, and their counts in the observed text data 17.3 Mining One Topic from Text 365  are shown in the second column of the table. The EM algorithm then initializes all the parameters to be estimated. In our case, we set all the probabilities to 0.25 in the fourth column of the table. In the first iteration of the EM algorithm, we will apply the E-step to infer which of the two distributions has been used to generate each word, i.e., to compute p(z = 0 | w) and p(z = 1 | w). We only showed p(z = 0 | w), which is needed in our M-step (p(z = 1 | w) = 1 − p(z = 0 | w)). Clearly, p(z = 0 | w) has different values for different words, and this is because these words have different probabilities in the background model and the initialized θ d . Thus, even though the two distributions are equally likely (by our prior) and our initial values for p(w | θ d ) form a uniform distribution, the inferred p(z = 0 | w) would tend to give words with smaller probabilities if p(w | θ B ) give them a higher probability. For example, Once we have the probabilities of all these z values, we can perform the M-step, where these probabilities would be used to adjust the counts of the corresponding words. For example, the count of the is 4, but since p(z = 0 | the) = 0.33, we would obtain a discounted count of the, 4 × 0.33, when estimating p(the | θ d ) in the M-step. Similarly, the adjusted count for text would be 4 × 0.71. After the M-step, p(text | θ d ) would be much higher than p(the | θ d ) as shown in the table (shown in the first column under Iteration 2). Those words that are believed to have come from the topic word distribution θ d according to the E-step would have a higher probability. This new generation of parameters would allow us to further adjust the inferred latent variable or hidden variable values, leading to a new generation of probabilities for the z values, which can be fed into another M-step to generate yet another generation of potentially improved estimate of θ d . In the last row of the table, we show the log-likelihood after each iteration. Since each iteration would lead to a different generation of parameter estimates, it would also give a different value for the log-likelihood function. These log-likelihood values are all negative because the probability is between 0 and 1, which becomes a negative value after the logarithm transformation. We see that after each iteration, the log-likelihood value is increasing, showing that the EM algorithm is iteratively improving the estimated parameter values in a hill-climbing manner. We will provide an intuitive explanation of why it converges to a local maximum later. For now, it is worth pointing out that while the main goal of our EM algorithm is to obtain a more discriminative word distribution to represent the topic that we hope to discover, i.e., p(w | θ d ), the inferred p(z = 0 | w) after convergence is also meaningful and may sometimes be a useful byproduct. Specifically, these are the probabilities that a word is believed to have come from the topic distribution, and we can add them up to obtain an estimate of to what extent the document has covered background vs. content, or to what extent the content of the document deviates from a "typical" background document. This would give us a single numerical score for each document, so we can then use the score to compare different documents or different subsets of documents (e.g., those associated with different authors or from different sources). Thus, our simple two-component mixture model can not only help us discover a single topic from the document, but also provide a useful measure of "typicality" of a document which may be useful in some applications. Next, we provide some intuitive explanation why the EM algorithm will converge to a local maximum in Figure 17.26. Here we show the parameter θ d on the X-axis, and the Y-axis denotes the likelihood function value. This is an over-simplification since θ d is an M-dimensional vector, but the one-dimensional view makes it much easier to understand the EM algorithm. We see that, in general, the original likelihood function (as a function of θ d ) may have multiple local maxima. The goal of computing the ML estimate is to find the global maximum, i.e., the θ d value that makes the likelihood function reach it global maximum. The EM algorithm is a hill-climbing algorithm. It starts with an initial (random) guess of the optimal parameter value, and then iteratively improves it. The picture shows the scenario of going from iteration n to iteration n + 1. At iteration n, the current guess of the parameter value is p (n) (w | θ d ), and it is seen to be non-optimal in the picture. In the E-step, the EM algorithm (conceptually) computes an auxiliary function which lower bounds the original likelihood function. Lower bounding means that for any given value of θ d , the value of this auxiliary function would be no larger than that of the original likelihood function. In the M-step, the EM algorithm finds an optimal parameter value that would maximize the auxiliary function and treat this parameter value as our improved estimate, p (n+1) (w | θ d ). Since the auxiliary function is a lower bound of the original likelihood function, maximizing the auxiliary function ensures the new parameter to also have a higher value according to the original likelihood function unless it has already reached a local maximum, in which case, the optimal value maximizing the auxiliary function is also a local maximum of the original likelihood function. This explains why the EM algorithm is guaranteed to converge to a local maximum. You might wonder why we don't work on finding an improved parameter value directly on the original likelihood function. Indeed, it is possible to do that, but in the EM algorithm, the auxiliary function is usually much easier to optimize than the original likelihood function, so in this sense, it reduces the problem into a somewhat simpler one. Although the auxiliary function is generally easier to optimize, it does not always have an analytical solution, which means that the maximization of the auxiliary function may itself require another iterative process, which would be embedded in the overall iterative process of the EM algorithm. In our case of the simple mixture model, we did not explicitly compute this auxiliary function in the E-step because the auxiliary function is very simple and as a result, our M-step has an analytical solution, thus we were able to bypass the explicit computation of this auxiliary function and go directly to find a re-estimate of the parameters. Thus in the E-step, we only computed a key component in the auxiliary function, which is the probability that a word has been generated from each of the two distributions, and our M-step directly gives us an analytical solution to the problem of optimizing the auxiliary function, and the solution directly uses the values obtained from the E-step. The EM algorithm has many applications. For example, in general, parameter estimation of all mixture models can be done by using the EM algorithm. The hidden variables introduced in a mixture model often indicate which component model has been used to generate a data point. Thus, once we know the values of these hidden variables, we would be able to partition data and identify the data points that are likely generated from any particular distribution, thus facilitating estimation of component model parameters. In general, when we apply the EM algorithm, we would augment our data with supplementary unobserved hidden variables to simplify the estimation problem. The EM algorithm would then work as follows. First, it would randomly initialize all the parameters to be estimated. Second, in the E-step, it would attempt to infer the values of the hidden variables based on the current generation of parameters, and obtain a probability distribution of hidden variables over all possible values of these hidden variables. Intuitively, this is to take a good guess of the values of the hidden variables. Third, in the M-step, it would use the inferred hidden variable values to compute an improved estimate of the parameter values. This process is repeated until convergence to a local maximum of the likelihood function. Note that although the likelihood function is guaranteed to converge to a local maximum, there is no guarantee that the parameters to be estimated always have a stable convergence to a particular set of values. That is, the parameters may oscillate even though the likelihood is increasing. Only if some conditions are satisfied would the parameters be guaranteed to converge (see Wu 1983). 