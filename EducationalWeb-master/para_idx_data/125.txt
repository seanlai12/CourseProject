we only have word probabilities θ as parameters, just like in the simplest unigram language model. This gives us the following formula to estimate the feedback language model: We choose this probability distribution θ F to maximize the log likelihood of the feedback documents under our model. This is the same idea as the maximum likelihood estimator. Here though, the mathematical problem is to solve this optimization problem. We could try all possible θ values and select the one that gives the whole expression the maximum probability. Once we have done that, we obtain this θ F that can be interpolated with the original query model to do feedback. Of course, in practice it isn't feasible to try all values of θ, so we use the EM algorithm to estimate its parameters [Zhai and Lafferty 2001]. Such a model involving multiple component models combined together is called a mixture model, and we will further discuss such models in more detail in the topic analysis chapter (Chapter 17). Figure 7.6 shows some examples of the feedback model learned from a web document collection for performing pseudo feedback. We just use the top 10 documents, and we use the mixture model with parameters λ = 0.9 and λ = 0.7. The query is airport security. We select the top ten documents returned by the search engine for this query and feed them to the mixture model. The words in the two tables are learned using the approach we described. For example, the words airport and security still show up as high probabilities in each case naturally because they occur frequently in the top-ranked documents. But we also see beverage, alcohol, bomb, and terrorist. Clearly, these are relevant to this topic, and if combined with the original query can help us match other documents in the index more accurately. If we compare the two tables, we see that when λ is set to a smaller value, we'll still see some common words when we don't use the background model often. Remember that λ can "choose" the probability of using the background model to generate to the text. If we don't rely much on the background model, we still have to use the topic model to account for the common words. Setting λ to a very high value uses the background model more often to explain these words and there is no burden on explaining the common words in the feedback documents. As a result, the topic model is very discriminative-it contains all the relevant words without common words. To summarize, this section discussed feedback in the language model approach; we transform our original query likelihood retrieval function to a more general KLdivergence model. This generalization allows us to use a language model for the query, which can be manipulated to include feedback documents. We described a method for estimating the parameters in this feedback model that discriminates between topic words (relevant to the query) and background words (useless stop words). In this chapter, we talked about the three major feedback scenarios: relevance feedback, pseudo feedback, and implicit feedback. We talked about how to use Rocchio to do feedback in the vector-space model and how to use query model estimation for feedback in language models. We briefly talked about the mixture model for its estimation, although there are other methods to estimate these parameters that we mention later on in the book. 