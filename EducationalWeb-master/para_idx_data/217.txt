By definition, two words are paradigmatically related if they share a similar context. Naturally, our idea of discovering such a relation is to look at the context of each word and then try to compute the similarity of those contexts. In Figure 13.3, we have taken the word cat out of its context. The remaining words in the sentences that contain cat are the words that tend to co-occur with it. We can do the same thing for another word like dog. In general, we would like to capture such contexts and then try to assess the similarity of the context of cat and the context of a word like dog. The question is how to formally represent the context and define the similarity function between contexts. First, we note that the context contains many words. These words can be regarded as a pseudo document, but there are also different ways of looking at the context. For example, we can look at the word that occurs before the word cat. We call this context the left context L. In this case, we will see words like my, his, big, a, the, and so on. Similarly, we can also collect the words that occur after the word cat, which is called the right context R. Here, we see words like eats, ate, is, and has. More generally, we can look at all the words in the window of text around the target word. For example, we can take a window of eight words around the target word. These word contexts from the left or from the right form a bag of words representation. Such a word-based representation would actually give us a useful way to define the perspective of measuring context similarity. For example, we can compare only the L context, the R context, or both. A context may contain adjacent High sim(word1, word2) → word1 and word2 are paradigmatically related Sim("cat", "dog") = Sim(Left1("cat"), Left1("dog")) + Sim(Right1("cat"), Right1("dog")) + … + Sim(Window8("cat"), Window8("dog")) = ? words like eats and my or non-adjacent words like Saturday or Tuesday. This flexibility allows us to match the similarity in somewhat different ways. We might want to capture similarity based on general content, which yields loosely related paradigmatic relations. If we only used words immediately to the left and right, we would likely capture words that are very much related by their syntactic categories. Thus, the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. For example, we can measure the similarity of cat and dog based on the similarity of their context, as shown in Figure 13.4. The similarity function can be a combination of similarities on different contexts, and we can assign weights to these different similarities to allow us to focus more on a particular kind of context. Naturally, this would be application-specific, but again, the main idea for discovering pardigmatically related words is to compute the similarity of their contexts. Let's see how we exactly compute these similarity functions. Unsurprisingly, we can use the vector space model on bag-of-words context data to model the context of a word for paradigmatic relation discovery. In general, we can represent a pseudo document or context of cat as one frequency vector d 1 and another word dog would give us a different context, d 2 . We can then measure the similarity of these two vectors. By viewing context in the vector space model, we convert the problem of paradigmatic relation discovery into the problem of computing the vectors and their similarity. The two questions that we have to address are how to compute each vector and how to compute their similarity. There are many approaches that can be used to solve the problem, and most of them are developed for information retrieval. They have been shown to work well for matching a query vector and a document vector. We can adapt many of the ideas to compute a similarity of context documents for our purpose. 