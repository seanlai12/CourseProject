In this section, we will discuss how to instantiate a vector space model so that we can get a very specific ranking function. As mentioned previously, the vector space model is really a framework: it doesn't specify many things. For example, it did not say how we should define the dimensions of the vectors. It also did not say how we place a document vector or query vector into this space. That is, how should we define/calculate the values of all the elements in the query and document vectors? Finally, it did not say how we should compute similarity between the query vector and the document vector. As you can imagine, in order to implement this model, we have to determine specifically how we should compute and use these vectors. In Figure 6.3, we illustrate the simplest instantiation of the vector space model. In this instantiation, we use each word in our vocabulary to define a dimension, thus giving |V | dimensions-this is the bag-of-words instantiation. Now let's look at how we place vectors in this space. Here, the simplest strategy is to use a bit vector to represent both a query and a document, and that means each element x i and y i would take a value of either zero or one. When it's one, it means the corresponding word is present in the document or query. When it's zero, it's absent. If the user types in a few words for a query, then the query vector would have a few ones and many, many zeros. The document vector in general would have more ones than the query vector, but there will still be many zeros since the vocabulary is often very large. Many words in the vocabulary don't occur in a single document; many words will only occasionally occur in a given document. Most words in the vocabulary will be absent in any particular document. Now that we have placed the documents and the query in the vector space, let's look at how we compute the similarity between them. A commonly used similarity measure is the dot product; the dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors. In Figure 6.3 we see that it's the product of x 1 and y 1 plus the product of x 2 and y 2 , and so on. This is only one of the many different ways of computing the similarity. So, we've defined the dimensions, the vector space, and the similarity function; we finally have the simplest instantiation of the vector space model! It's based on the bit vector representation, dot product similarity, and bag of words instantiation. Now we can finally implement this ranking function using a programming language and then rank documents in our corpus given a particular query. We've gone through the process of modeling the retrieval problem using a vector space model. Then, we made assumptions about how we place vectors in the vector space and how we define the similarity. In the end, we've got a specific retrieval function shown in Figure 6.3. The next step is to think about whether this individual function actually makes sense. Can we expect this function will actually perform well? It's worth thinking about the value that we are calculating; in the end, we've got a number, but what does this number mean? Please take a few minutes to think about that before proceeding to the next section. 