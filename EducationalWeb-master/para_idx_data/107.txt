bility. That is, the probability of the unseen word is assumed to be proportional to its probability in the entire collection. With this assumption, we've shown that we can derive a general ranking formula for query likelihood retrieval models that automatically contains the vector space heuristics of TF-IDF weighting and document length normalization. We also saw that through some rewriting, the scoring of such a ranking function is primarily based on a sum of weights on matched query terms, also just like in the vector space model. The actual ranking function is given to us automatically by the probabilistic derivation and assumptions we have made, unlike in the vector space model where we have to heuristically think about the forms of each function. However, we still need to address the question: how exactly should we smooth a document language model? How exactly should we use the reference language model based on the collection to adjust the probability of the MLE of seen terms? This is the topic of the next section. 