[2007] is a short summary of topic models alone. In the exercises, we mention supervised LDA [McAuliffe and Blei 2008]. There are many other variants of LDA such as MedLDA [Zhu et al. 2009] (another supervised model which attempts to maximize the distance between classes) and LabeledLDA [Ramage et al. 2009] (which incorporates metadata tags). Exercises 17.1. What is the input and output of the two-topic mixture model? What is the input and output of PLSA? For a product review dataset, there are k different product types. The true value of k is in the range [2,5]. How many product types do you think there were? How can you use topic analysis to help you? (A product type is something like "CPU" or "router"). Give an idea about how you could use topic models to enhance search results. What type of access mode does your suggestion support? Give an idea about how you could use topic models for a document representation in vector space. What does a similarity measure capture for this representation? Sketch an idea about how you could use PLSA to model topical trends over time, given a dataset of documents that are tagged with dates. Chapter 18 discusses sentiment analysis and opinion mining. In order to discover positive and negative sentiment topics, we set k = 2 and run a topic analysis method. What is an issue with this idea? 17.8. We mentioned that PLSA is a discriminative model and LDA is a generative model. Discuss how these differences affect: 17.9. An alternative topic analysis evaluation scheme is to hold out a certain number of words in the vocabulary from some documents. Explain how this can be used to evaluate topic models. Does this evaluate the topic-document distributions, topic-word distributions, or both? Exercises 387 17.10. Supervised LDA (sLDA) is a probabilistic model over labeled documents, where each document contains some real-valued response variable. For example, if the dataset is movie reviews, the response variable could be the average rating. Explain what additional knowledge sLDA can discover in comparison to LDA or PLSA aside from predicting response variables for a new document. 410 Chapter 18 Opinion Mining and Sentiment Analysis useful for determining whether a sentence (e.g.) is positive or negative. Based on the useful features, we can either adjust the algorithm or try to fine-tune the feature set. From a topic analysis viewpoint, we would like to ensure that the topics are coherent and have a believable coverage and distribution over the documents in the dataset. We mentioned that corpus log likelihood is a way to test how well the model fits to the data. While this evaluation metric doesn't always agree with human judges [Chang et al. 2009], it does serve as a sanity check or proxy for true usefulness. Additionally, we can test the effectiveness of adding opinion mining and sentiment analysis to an existing system by the method discussed in Chapter 13. That is, we can compare the system's performance before sentiment analysis, and then compare its performance afterwards. If the system performs statistically significantly better under some evaluation metric relevant to the task at hand, then the sentiment analysis is a definitive improvement. Opinion mining and sentiment analysis have been extensively studied. Two excellent books on this general topic are the book Opinion Mining and Sentiment Analysis [Pang and Lee 2008] and the book Sentiment Analysis and Opinion Mining [Liu 2012]. Multiple extensions of topic models for analyzing opinionated topics have been made (e.g., topic-sentiment mixture model [Mei et al. 2007a], multi-grain topic model [Titov and McDonald 2008], and aspect and sentiment unification model [Jo and Oh 2011]). Techniques for Latent Aspect Rating Analysis are mainly covered in two KDD papers [Wang et al. 2010], [Wang et al. 2011]. Exercises 18.1. In this chapter, we mainly discussed how to determine overall sentiment for a text object. Imagine that we already have the sentiment information as part of the object and we are instead interested in identifying the target of the sentiment. Brainstorm some ideas using NLP techniques mentioned in this book. META has an implementation of the LDA topic modeling algorithm, which contains symmetric priors for the word and topic distributions. Modify META to contain non-uniform priors on the topic-word distributions to encode some additional knowledge into the prior. For example, in the sentiment analysis task, create two prior distributions that have high weights on particular "good" and "bad" topic distributions. That is, the prior for the "good" topic should probably weight the Exercises 411 term excellent relatively high and the "bad" topic should weight the term broken relatively high. A simple version of sentiment analysis is called word valence scoring. A finite list of words has a positive or negative valence score. When determining the sentiment for a sentence, the valence scores are added up for each word in the sentence; if the result is positive, the sentiment is positive. If the result is negative, the sentiment is negative. For example, some valences could be bad = −4.5, awesome = 6.3, acceptable = 1.1. What is a potential weakness to this method? 18.4. How can we automatically create the sentiment word valence scores based on a list of sentences labeled as positive or negative? 18.5. The techniques discussed in this chapter can be applied to other problems aside from sentiment analysis in particular. Name some applications that also would benefit from the general methods discussed in this chapter. Explain how you would implement them. Chapter 15. How can cross-fold validation be used to detect potential overfitting? 18.7. We mentioned that a main challenge in designing features is to optimize the tradeoff between exhaustivity and specificity. Can you design an experimental training setup that takes these variables into account? 18.8. In LARA, why do you imagine the vector of weights α is drawn from a multivariate Gaussian distribution? That is, why not use some other distribution? 18.9. In LARA, each word is assigned an aspect, but this means that one sentence may be assigned many different aspects. Can you outline an adjustment to ensure that each sentence only covers one topic or a small number of topics? Similarly, can we ensure that sequences of words all belong to the same aspect? 18.10. Instead of a classification task, it may be beneficial to instead rank text objects by how positive they are (so low-ranking documents are very negative). Outline a few methods on how this may be achieved. What are the benefits and downsides compared to classification? 18.11. Imagine that you have an unlabeled dataset of product reviews. How can you design a sentiment classifier based on this dataset without manually labeling all of the documents? In this chapter, we discuss techniques for joint analysis of text and structured data, which not only enriches text analysis, but is often necessary for many prediction problems involving both structured data and unstructured text data. Due to the complexity of many of these methods and the limited space, we will only give a brief introduction to the main ideas and show sample results to provide a sense about what kind of applications these techniques can support. Details of these techniques can be found in the references provided at the end of this chapter. Here, we examine Bayesian statistics in more depth as a continuation of Chapter 2 and Chapter 17. From section 2.1.5, we already know the likelihood of our binomial distribution is but what about the prior, p(θ)? A prior should represent some "prior belief" about the parameters of the model. For our coin flipping (i.e., binomial distribution), it would make sense to have the prior also be proportional to the powers of θ and (1 − θ). Thus, the posterior will also be proportional to those powers: So we need to find some distribution of the form P (θ) ∝ θ a (1 − θ) b . Luckily, there is something called the Beta distribution. We say This is the probability density function (pdf) of the Beta distribution. But what is The ( . ) is the Gamma function. It can be thought of as the continuous version of the factorial function. That is, (x) = (x − 1) (x − 1). Or rather for an x ∈ Z + , (x) = (x − 1)!. That still doesn't explain the purpose of that constant in front of In fact, this constant just ensures that given the α and β parameters, the Beta distribution still integrates to one over its support. As you probably recall, this is a necessity for a probability distribution. Mathematically, we can write this as Note that the sum over the support of x is the reciprocal of that constant. If we divide by it (multiply by reciprocal), we will get one as desired: If you're proficient in calculus (or know how to use Wolfram Alpha or similar), you can confirm this fact for yourself. One more note on the Beta distribution: its expected value is We'll see how this can be useful in a minute. Let's finally rewrite our estimate of p(θ | D). The data we have observed is H , T . Additionally, we are using the two hyperparameters α and β for our Beta distribution prior. They're called hyperparameters because they are parameters for our prior distribution. But this is itself a Beta distribution! Namely, Finally, we can get our Bayesian parameter estimation. Unlike maximum likelihood estimation (MLE), where we have the parameter that maximizes our data, we integrate over all possible θ , and find its expected value given the data, E[θ | D]. In this case, our "data", is the flip results and our hyperparameters α and β: We won't go into detail with solving the integral since that isn't our focus. What we do see, though, is our final result. This result is general for any Bayesian estimate of a binomial parameter with a Beta prior.  This can be accomplished with α = 4, β = 1, or α = 16, β = 4, or even α = 0.4, β = 0.1. But what is the difference? Figure A.1 shows a comparison of the Beta distribution with varying parameters. It's also important to remember that a draw from a Beta prior θ ∼ Beta(α, β) gives us a distribution. Even though it's a single value on the range [0, 1], we are still using the prior to produce a probability distribution. Perhaps we'd like to choose a unimodal Beta prior, with a mean 0.8. As we can see from Figure A.1, the higher we set α and β, the sharper the peak at 0.8 will be. Looking at our parameter estimation, we can imagine the hyperparameters as pseudo counts-counts from the outcome of experiments already performed. The higher the hyperparameters are, the more pseudo counts we have, which means our prior is "stronger." As the total number of experiments increases, the sum H + T also increases, which means we have less dependence on our priors. Initially, though, when H + T is relatively low, our prior plays a stronger role in the estimation of θ . As we all know, a small number of flips will not give an accurate estimate of the true θ -we'd like to see what our estimate becomes as our number of flips approaches infinity (or some "large enough" value). In this sense, our prior also smooths our estimation. Rather than the estimate fluctuating greatly initially, it could stay relatively smooth if we have a decent prior. If our prior turns out to be incorrect, eventually the observed data will overshadow the pseudo counts from the hyperparameters anyway, since α and β are held constant. At this point, you may be able to rationalize how Dirichlet prior smoothing for information retrieval language models or topic models works. However, our probabilities are over words now, not just a binary heads or tails outcome. Before we talk about the Dirichlet distribution, let's figure out how to represent the probability of observing a word from a vocabulary. For this, we can use a categorical distribution. In a text information system, a categorical distribution could represent a unigram language model for a single document. Here, the total number of outcomes is k = |V |, the size of our vocabulary. The word at index i would have a probability p i of occurring, and the sum of all words' probabilities would sum to one. The categorical distribution is to the multinomial distribution as the Bernoulli is to the binomial. The multinomial is the probability of observing each word k i occur x i times in a total of n trials. If we're given a document vector of counts, we can use the multinomial to find the probability of observing documents with those counts of words (regardless of position). The probability density function is given as follows: We can also write its pdf as It should be straightforward to relate the more general multinomial distribution to its binomial counterpart. We now have the likelihood function determined for a distribution with k outcomes. The conjugate prior to the multinomial is the Dirichlet. That is, if we use a Dirichlet prior, the posterior will also be a Dirichlet. Like the multinomial, the Dirichlet is a distribution over positive vectors that sum to one. (The "simplex" is the name of the space where these vectors live.) Like the Beta distribution, the parameters of the Dirichlet are reals. Here's the pdf: (A.13) In this notation we have p(θ | α). θ is what we draw from the Dirichlet; in the Beta, it was the parameter to be used in the binomial. Here, it is the vector of parameters to be used in the multinomial. In this sense, the Dirichlet is a distribution that produces distributions (so is the Beta!). The hyperparameters of the Dirichlet are also a vector (denoted with an arrow for emphasis). Instead of just two hyperparameters as in the Beta, the Dirichlet needs k-one for each multinomial probability. Algorithmically, the basic idea of EM is to start with some initial guess of the parameter values θ (0) and then iteratively search for better values for the parameters. Assuming that the current estimate of the parameters is θ (n) , our goal is to find another θ (n+1) that can improve the likelihood L(θ ). Let us consider the difference between the likelihood at a potentially better parameter value θ and the likelihood at the current estimate θ (n) , and relate it with the corresponding difference in the complete likelihood: Our goal is to maximize L(θ ) − L(θ (n) ), which is equivalent to maximizing L(θ ). Now take the expectation of this equation w.r.t. the conditional distribution of the hidden variable given the data X and the current estimate of parameters θ (n) , i.e., p(H | X, θ (n) ). We have Note that the left side of the equation remains the same as the variable H does not occur there. The last term can be recognized as the KL-divergence of p(H | X, θ (n) ) and p(H | X, θ), which is always non-negative. We thus have We thus obtain a lower bound for the original likelihood function. The main idea of EM is to maximize this lower bound so as to maximize the original (incomplete) likelihood. Note that the last two terms in this lower bound can be treated as constants as they do not contain the variable θ, so the lower bound is essentially C APPENDIX This appendix is a more detailed discussion of the KL-divergence function and its relation to Dirichlet prior smoothing in the generalized query likelihood smoothing framework. We briefly touched upon KL-divergence in Chapter 7 and Chapter 13. As we have seen, given two probability mass functions p(x) and q(x), D(p q), the Kullback-Leibler divergence (or relative entropy) between p and q is defined as It is easy to show that D(p q) is always non-negative and is zero if and only if p = q. Even though it is not a true distance between distributions (because it is not symmetric and does not satisfy the triangle inequality), it is still often useful to think of the KL-divergence as a "distance" between distributions [Cover and Thomas 1991]. Suppose that a query q is generated by a generative model p(q | θ Q ) with θ Q denoting the parameters of the query unigram language model. Similarly, assume that a document d is generated by a generative model p(d | θ D ) with θ D denoting the parameters of the document unigram language model. If θ Q and θ D are the estimated query and document language models, respectively, then, the relevance value of d with respect to q can be measured by the following negative KL-divergence function [Zhai and Lafferty 2001]: Note that the second term on the right-hand side of the formula is a querydependent constant, or more specifically, the entropy of the query model θ Q . It can be ignored for the purpose of ranking documents. In general, the computation of the above formula involves a sum over all the words that have a non-zero probability according to p(w | θ Q ). However, when θ D is based on certain general smoothing method, the computation would only involve a sum over those that both have a non-zero probability according to p(w | θ Q ) and occur in document d. Such a sum can be computed much more efficiently with an inverted index. We now explain this in detail. The general smoothing scheme we assume is the following: where p s (w | d) is the smoothed probability of a word seen in the document, p(w | C) is the collection language model, and α d is a coefficient controlling the probability mass assigned to unseen words, so that all probabilities sum to one. In general, Thus, individual smoothing methods essentially differ in their choice of p s (w | d). The collection language model p(w | C) is typically estimated by c(w, C) , or a smoothed version , where V is an estimated vocabulary size (e.g., the total number of distinct words in the collection). One advantage of the smoothed version is that it would never give a zero probability to any term, but in terms of retrieval performance, there will not be any significant difference in these two versions, since w c(w , C) is often significantly larger than V . It can be shown that with such a smoothing scheme, the KL-divergence scoring formula is essentially (the two sides are equivalent for ranking documents) Note that the scoring is now based on a sum over all the terms that both have a non-zero probability according to p(w | θ Q ) and occur in the document, i.e., all "matched" terms. Dirichlet prior smoothing is one particular smoothing method that follows the general smoothing scheme mentioned in the previous section. In particular, Plugging these into equation C.1, we see that with Dirichlet prior smoothing, our You may be wondering how we can compute p(w | θ Q ). This is exactly where the KL-divergence retrieval method is better than the simple query likelihood methodwe can have different ways of computing it! The simplest way is to estimate this probability by the maximum likelihood estimator using the query text as evidence, which gives us Using this estimated value, you should see easily that the KL-divergence scoring formula is essentially the same as the query likelihood retrieval formula as presented in Zhai and Lafferty [2004]. A more interesting way of computing p(w | θ Q ) is to exploit feedback documents. Specifically, we can interpolate the simple p ml (w | θ Q ) with a feedback model p(w | θ F ) estimated based on feedback documents. That is, where α is a parameter that needs to be set empirically. Please note that this α is different from α d in the smoothing formula. Of course, the next question is how to estimate p(w | θ F )? One approach is to assume the following two component mixture model for the feedback documents, where F = {d 1 , . . . , d k } is the set of feedback documents, and λ is yet another parameter that indicates the amount of "background noise" in the feedback documents, and that needs to be set empirically. Now, given λ, the feedback documents F , and the collection language model p(w | C), we can use the EM algorithm to compute the maximum likelihood estimate of θ F , as detailed in Appendix B. He has given talks at Jump Labs Champaign and at UIUC for Data and Information Systems Seminar, Intro to Big Data, and Teaching Assistant Seminar. His research interests include text mining applications in information retrieval, natural language processing, and education. 