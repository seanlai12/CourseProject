In order to solve the problem of assigning highest probabilities to common words in the estimated unigram language model based on one document, it would be 346 Chapter 17 Topic Analysis useful to think about why we end up having this problem. It is not hard to see that the problem is due to two reasons. First, these common words are very frequent in our data, thus any maximum likelihood estimator would tend to give them high probabilities. Second, our generative model assumes all the words are generated from one single unigram language model. The ML estimate thus has no choice but to assign high probabilities to such common words in order to maximize the likelihood. Thus, in order to get rid of the common words, we must design a different generative model where the unigram language model representing the topic doesn't have to explain all the words in the text data. Specifically, our target topic unigram language model should not have to generate the common words. This further suggests that we must introduce another distribution to generate these common words so that we can have a complete generative model for all the words in the document. Since we intend for this second distribution to explain the common words, a natural choice for this distribution is the background unigram language model. We thus have a mixture model with two component unigram language models, one being the unknown topic that we would like to discover, and one being a background language model that is fixed to assign high probabilities to common words. In Figure 17.13, we see that the two distributions can be mixed together to generate the text data, with the background model generates common words while the topic language model to generate content-bearing words in the document. Thus, we can expect the discovered (learned) topic unigram language model to  