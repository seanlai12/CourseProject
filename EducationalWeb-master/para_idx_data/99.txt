In this section, we will look at a very different way to design ranking functions than the vector space model that we discussed before. In probabilistic models, we define the ranking function based on the probability that a given document d is relevant 6.4 Probabilistic Retrieval Models 111 to a query q, or p(R = 1 | d , q) where R âˆˆ {0, 1} is a binary random variable denoting relevance. In other words, we introduce a binary random variable R and we model the query and the documents as observations from random variables. Note that in the vector space model, we assume that documents are all equallength vectors. Here, we assumed they are the data observed from random variables. Thus, the problem is to estimate the probability of relevance. In this category of models, there are many different variants. The classic probabilistic model has led to the BM25 retrieval function, which we discussed in the vector space model section because its form is quite similar to these types of models. We will discuss another special case of probabilistic retrieval functions called language modeling approaches to retrieval. In particular, we're going to discuss the query likelihood retrieval model, which is one of the most effective models in probabilistic models. There is also another line of functions called divergence-fromrandomness models (such as the PL2 function [Amati and Van Rijsbergen 2002]). It's also one of the most effective state-of-the-art retrieval models. In query likelihood, our assumption is that this probability of relevance can be approximated by the probability of a query given a document and relevance, p(q | d , R = 1). Intuitively, this probability just captures the following probability: if a user likes document d, how likely would the user enter query q in order to retrieve document d? The condition part contains document d and R = 1, which can be interpreted as the condition that the user likes document d. To understand this idea, let's first take a look at the basic idea of probabilistic retrieval models. Figure 6.19 lists some imagined relevance status values (or relevance judgments) of queries and documents. It shows that q 1 is a query that the user typed in and d 1 is a document the user has seen. A "1" in the far right column means the user thinks d 1 is relevant to q 1 . The R here can be also approximated by the clickthrough data that the search engine can collect by watching how users interact with the search results. In this case, let's say the user clicked on document d 1 , so there's a one associated with the pair (q 1 , d 1 ). Similarly, the user clicked on d 2 , so there's a one associated with (q 1 , d 2 ). Thus, d 2 is assumed to be relevant to q 1 while d 3 is nonrelevant, d 4 is non-relevant, d 5 is again relevant, and so on and so forth. Perhaps the second half of the table (after the ellipses) is from a different user issuing the same queries. This other user typed in q 1 and then found that d 1 is actually not useful, which is in contrast to the first user's judgement. We can imagine that we have a large amount of search data and are able to ask the question, "how can we estimate the probability of relevance?" Simply, if we look at all the entries where we see a particular d and a particular q, we can calculate how likely we will see a one in the third column. We can first count how many times we see q and d as a pair in this table and then count how many times we actually have also seen a one in the third column and compute the ratio: Let's take a look at some specific examples. Suppose we are trying to compute this probability for d 1 , d 2 , and d 3 for q 1 . What is the estimated probability? If we are interested in q 1 and d 1 , we consider the two pairs containing q 1 and d 1 ; only in one of the two cases has the user said that the document is relevant. So R is equal to 1 in only one of the two cases, which gives our probability a value of 0.5. What about d 2 and d 3 ? For d 2 , R is equal to 1 in both cases. For d 3 , R is equal to 0 in both cases. We now have a score for d 1 , d 2 , and d 3 for q 1 . We can simply rank them based on these probabilities-that's the basic idea of probabilistic retrieval model. In our example, 