When there is no training data available (i.e., text data with known categories explicitly labelled), we often have to manually create heuristic rules to solve the problem 15.2 Overview of Text Categorization Methods 301 of categorization. For example, the rule if the word "governor" occurs â†’ assign politics label. Obviously, designing effective rules requires a significant amount of knowledge about the specific problem of categorization. Such a rule-based manual approach would work well if: (1) the categories are very clearly defined (usually means that the categories are relatively simple); (2) the categories are easily distinguished based on surface features in text (e.g., particular words only occur in a particular category of documents); and (3) sufficient domain knowledge is available to suggest many effective rules. However, the manual approach has some significant disadvantages. The first is that it is labor-intensive, thus it does not scale up well both to the number of categories (since a new category requires new rules) and to the growth of data (since new data may also need new rules). The second is that it may not be possible to come up with completely reliable rules and it is hard to handle the uncertainty in the rules. Finally, the rules may not be all consistent with each other. As a result, the categorization results may depend on the order of application of different rules. These problems with the rule-based manual approach can mostly be addressed by using machine learning where humans would help the machine by labeling some examples with the correct categories (i.e., creating training examples), and the machine will learn from these examples to somewhat automatically construct rules for categorization, only that the rules are somewhat "soft" and weighted, and how the rules should be combined is also learned based on the training data. Note that although in such a supervised machine learning approach, cateorization appears to be "automatic," it does require human effort in creating the training data, unless the training data is naturally available to us (which sometimes does happen). The human-created rules, if any, can also be used as features in such a learningbased approach, and they will be combined in a weighted manner to minimize the classification errors on the training data with the weights automatically learned. The machine may also automatically construct soft rules based on primitive features provided by humans as in the case of decision trees [Quinlan 1986], which can be easily interpreted as a "rule-based" classifier, but the paths from the root to the leaves (i.e., the rules) are inducted automatically by using machine learning. Once a classifier (categorizer) is trained, it can be used to categorize any unseen text data. In general, all these learning-based categorization methods rely on discriminative features of text objects to distinguish categories, and they would combine multiple features in a weighted manner where the weights are automatically learned (i.e., adjusted to minimize errors of categorization on the training data). Different methods tend to vary in their way of measuring the errors on the training data, i.e., they may optimize a different objective function (also called a loss/cost function), and their way of combining features (e.g., linear vs. non-linear). In the rest of the chapter, we will further discuss learning-based approaches in more detail. These automatic categorization methods generally fall into three categories. Lazy learners or instance-based classifiers do not model the class labels explicitly, but compare the new instances with instances seen before, usually with a similarity measure. These models are called "lazy" due to their lack of explicit generalization or training step; most calculation is performed at testing time. Generative classifiers model the data distribution in each category (e.g., unigram language model for each category). They classify an object based on the likelihood that the object would be observed according to each distribution. Discriminative classifiers compute features of a text object that can provide a clue about which category the object should be in, and combine them with parameters to control their weights. Parameters are optimized by minimizing categorization errors on training data. As with clustering, we will be able to leverage many of the techniques we've discussed in previous chapters to create classifiers, the algorithms that assign labels to unseen data based on seen, labeled data. This chapter starts out with an explanation of the categorization problem definition. Next, we examine what types of features (text representation) are often used for classification. Then, we investigate a few common learning algorithms that we can implement with our forward and inverted indexes. After that, we see how evaluation for classification is performed, since the problem is inherently different from search engine evaluation. 