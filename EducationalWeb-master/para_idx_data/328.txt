In this section, we introduce probabilistic latent semantic analysis (PLSA), the most basic topic model, with many applications. In short, PLSA is simply a generalization of the two-component mixture model that we discussed earlier in this chapter to discover more than one topic from text data. Thus, if you have understood the twocomponent mixture model, it would be straightforward to understand how PLSA works. As we mentioned earlier, the general task of topic analysis is to mine multiple topics from text documents and compute the coverage of each topic in each document. PLSA is precisely designed to perform this task. As in all topic models, we make two key assumptions. First, we assume that a topic can be represented as a word distribution (or more generally a term distribution). Second, we assume that a text document is a sample of words drawn from a probabilistic model. We illustrate these two assumptions in Figure 17.27, where we see a blog article about Hurricane Katrina and some imagined topics, each represented by a word distribution, including, e.g., a topic on government response (θ 1 ), a topic on the flood of the city of New Orleans (θ 2 ), a topic on donation (θ k ), and a background topic θ B . The article is seen to contain words from all these distributions. Specifically, we see there is a criticism of government response at the beginning of this excerpt, which is followed by discussion of flooding of the city, and then a sentence about donation. We also see background words mixed in throughout the article. 