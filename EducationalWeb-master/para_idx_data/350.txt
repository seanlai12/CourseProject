In the second stage, Latent Rating Regression, we're going to use these words and their frequencies in different aspects to predict the overall rating. This prediction happens in two stages. In the first stage, we're going to use the weights of these words in each aspect to predict the aspect rating. For example, if in the discussion of location, you see a word like amazing mentioned many times, it will have a high weight (in the figure it's given a weight of 3.9). This high weight increases the aspect rating for location. In the case of another word like far, which is mentioned many times, the weight will decrease. The aspect ratings assume that it will be a weighted combination of these word frequencies where the weights are the senti-18.3 Latent Aspect Rating Analysis 403 ment weights of the words. Of course, these sentiment weights might be different for different aspects. For each aspect i we have a set of term sentiment weights for word w denoted as β i , w . In the second stage, we assume that the overall rating is simply a weighted combination of these aspect ratings. We assume we have aspect weights α i (d), and these will be used to take a weighted average of the aspect ratings r i (d). This method assumes the overall rating is simply a weighted average of these aspect ratings, which allows us to predict the overall rating based on the observable word frequencies. On the left side of Figure 18.10 is all the observed information, r d (the overall rating) and c i (w, d). On the right side is all the latent (hidden) information that we hope to discover. This is a typical case of a generative model where we embed the interesting latent variables. Then, we set up a generative probability for the overall rating given the observed words. We can adjust these parameter values to maximize the conditional probability of the observed rating given the document. We have seen such cases before in other models such as PLSA, where we predict topics in text data. Here, we're predicting the aspect ratings and other parameters. More formally, the data we are modeling here is a set of review documents with overall ratings, as shown in Figure 18.11. Each review document is denoted as d and the overall ratings denoted by r d . We use c i (w, d) to denote the count of word w in aspect segment i. The model is going to predict the rating based on d, so we're interested in the rating regression problem of p(r d | d). This model is set • Data: a set of review documents with overall ratings: C = {(d, r d )} d is pre-segmented into k aspect segments c i (w, d) = count of word w in aspect segment i (zero if w didn't occur) • Model: predict rating based on d: p(r d |d) Overall rating = weighted average of aspect ratings Aspect rating = sum of sentiment weights of words in the aspect Aspect-specific sentiment of w Multivariate Gaussian prior up as follows. r d is assumed to follow a normal distribution with a mean that is a weighted average of the aspect ratings r i (d) and variance δ 2 . Of course, this is just our assumption. As always, when we make this assumption, we have a formal way to model the problem and that allows us to compute the interesting quantities-in this case, the aspect ratings and the aspect weights. Each aspect rating r i (d) is assumed to be a sum of sentiment weights of words in aspect i. The vector of weights α for the aspects in the overall rating is itself drawn from another multivariate Gaussian distribution, α(d) ∼ N (μ, ). This means when we generate our overall rating, we're going to first draw a set of α values from this multivariate Gaussian prior distribution. Once we get these α values, we're going to compute the weighted average of aspect ratings as the mean of the normal distribution to generate the overall rating r d . Note that β is indexed by both i and w. That gives us a way to model different aspect segments of the same word, since the same word might have positive sentiment for one aspect and negative for another. How can we estimate all these parameters? Let's collectively denote them as = (β i , w , μ, , δ 2 ). As usual, we can use the maximum likelihood estimate which yields parameters that maximize observed ratings conditioned on their respective reviews, as shown in Figure 18.12. Once we estimate the parameters, we can easily compute the aspect rating for a particular aspect by taking all counts of the words that occurred in segment i and multiplying them by β i , w , summing over all words. The sum would be zero for words that do not occur, so we can simply take the sum of all the words in the vocabulary.  α (d) Figure 18.12 Latent rating regression estimation. 