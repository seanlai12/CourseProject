n-gram language model? 13.10. Depending on the context size, we get many different types of semantic meanings from word associations. Give two extremes of the types of relations we get when the context is very large and when it is very small. 13.11. Is it possible to adjust the word association mining algorithms to find antonyms instead of synonyms? If so, explain how; if not, explain why it is not possible. That is, we would like to assign a high score to the pair (hot, cold) since they are opposites and a low score to (freezing, cold) since they are synonyms. Clustering is a natural problem in exploratory text analysis. In its most basic sense, clustering (i.e., grouping) objects together lets us discover some inherent structure in our corpus by collecting similar objects. These objects could be documents, sentences, or words. We could cluster search engine queries, search engine results, and even users themselves. Clustering is a general data mining technique very useful for exploring large data sets. When facing a large collection of text data, clustering can reveal natural semantic structures in the data in the form of multiple clusters (i.e., groups) of data objects. The clustering results can sometimes be regarded as knowledge directly useful in an application. For example, clustering customer emails can reveal major customer complaints about a product. In general, the clustering results are useful for providing an overview of the data, which is often very useful for understanding a data set at a high-level before zooming into any specific subset of the data for focused analysis. The clustering results can also support navigation into the relevant subsets of the data since the structures can facilitate linking of objects inside a cluster and linking of related clusters. In general, clustering methods are very useful for text mining and exploratory text analysis with widespread applications especially due to the fact that clustering algorithms are mostly unsupervised without requiring any manual effort, and can thus be applied to any text data set. The object types that we cluster necessitate different tasks, and this variation leads to many interesting applications. For example, clustering of retrieval results can be used as a result summary or as a way to remove redundant documents. Clustering the documents in our entire corpus lets us find common underlying themes and can give us a better understanding of the type of data it contains. Term clustering is a powerful way to find concepts or create a thesaurus. However, how do we formally define the problem of clustering? In particular, what does it actually mean for an object to be in a particular cluster? Intuitively, we imagine objects inside the same cluster are similar in some way-more so than objects that appear in two different clusters. However, such a definition of clustering is strictly speaking not well defined as we did not make it clear how exactly we should measure similarity. Indeed, an appropriate definition of similarity is quite crucial for clustering as a different definition would clearly lead to a different clustering result. Consider the illustration in Figure 14.1. How should we cluster the objects shown in the figure on the left side? What would an ideal clustering result look like? Clearly these questions cannot be answered until we define the perspective for measuring similarity very clearly, i.e., inject a particular "clustering bias." If we define similarity based on the shape of an object, we will obtain a clustering result as shown in the picture in the middle of the figure. However, if we define the similarity based on the size of an object, then we would have very different results as shown in the figure on the right side. Thus, when we define a clustering task, it is important to state the desired perspective of measuring similarity, which we refer to as a "clustering bias." This bias will also be the basis for evaluating clustering results. The ambiguity of perspective for similarity not only exists in such an artificial example, but also exists everywhere. Take words for example: are "car" and "horse" similar? A car and a horse are clearly not similar physically. However, if we look at them from the perspective of their functions, we may say that they are similar since they can both be used as transportation tools. The "right" clustering bias clearly has to be determined by the specific application. In different algorithms, the clustering bias is injected in different ways. For some clustering algorithms, it is up to the user to define or select explicitly a similarity algorithm for the clustering method to use. It will put (for example) documents that are all similar according to the chosen similarity algorithm in the same cluster. Other clustering algorithms are model-based (typically based on 14.1 Overview of Clustering Techniques 277 generative probabilistic models), where the objective function of the model for the data (e.g., the likelihood function in the case of a generative probabilistic model) is to create an indirect bias on how similarity is defined. With these model-based methods, it's often the case that an object is assigned a probability distribution over all the clusters, meaning there is no "hard cluster assignment" as in the similarity-based methods. We explore both similarity-based clustering and modelbased clustering in this book. This particular chapter focuses on similarity-based clustering, and the topic analysis chapter (Chapter 17) is a fine example of modelbased clustering. In this chapter, we examine clustering techniques for both words and documents. Clustering sentences can be viewed as a case of clustering small documents. We first start with an overview of clustering techniques, where we categorize the different approaches. Next, we discuss similarity-based clustering via two common methods (hierarchical and divisive methods). Then, we introduce term clustering via both semantic-relatedness and pointwise mutual information before mentioning two more advanced topics. We end with clustering evaluation. 