Now that we have a model for our coin flipping, how can we estimate its parameters given some observed data? For example, maybe we observe the data D that we discussed above where n = 5: Now we would like to figure out what θ is based on the observed data. Using maximum likelihood estimation (MLE), we choose the θ that has the highest likelihood given our data, i.e., choose the θ such that the probability of observed data is maximized. To compute the MLE, we would first write down the likelihood function, i.e., p(D | θ), which is θ 3 (1 − θ) 2 as we explained earlier. The problem is thus reduced to find the θ that maximizes the function f (θ) = θ 3 (1 − θ) 2 . Equivalently, we can attempt to maximize the log-likelihood: log f (θ) = 3 log θ + 2 log(1 − θ), since logarithm transformation preserves the order of values. Using knowledge of calculus, we know that a necessary condition for a function to achieve a maximum value at a θ value is that the derivative at the same θ value is zero. Thus, we just need to solve the following equation: and we easily find that the solution is θ = 3/5. More generally, let H be the number of heads and T be the number of tails. The MLE of the probability of heads is given by: 28 Chapter 2 Background The notation arg max represents the argument (i.e., θ in this case) that makes the likelihood function (i.e., p(D | θ)) reach its maximum. Thus, the value of an arg max expression stays the same if we perform any monotonic transformation of the function inside arg max. This is why we could use the logarithm transformation in the example above, which made it easier to compute the derivative. The solution to MLE shown above should be intuitive: the θ that maximizes our data likelihood is just the ratio of heads. It is a general characteristic of the MLE that the estimated probability is the normalized counts of the corresponding events denoted by the probability. As an example, the MLE of a multinomial distribution (which will be further discussed in detail later in the book) gives each possible outcome a probability proportional to the observed counts of the outcome. Note that a consequence of this is that all unobserved outcomes would have a zero probability according to MLE. This is often not reasonable especially when the data sample is small, a problem that motivates Bayesian parameter estimation which we discuss below. 