Clearly, a bigram words representation would most likely give better performance since we can capture not good and not bad as well as was good and was bad. As a counterexample, using only bigram words leads us to miss out on rarer informative single words such as overhyped. This term is now captured in bigrams such as overhyped (period) and very overhyped. If we see the same rarer informative word in a different context-such as was overhyped-this is now an out-of-vocabulary term and can't be used in determining the sentence polarity. Due to this phenomenon, it is very common to combine multiple feature sets together. In this case, we can tokenize documents with both unigram and bigram words. A well-known strategy discussed in Stamatatos [2009] shows that low-level lexical features combined with high-level syntactic features give the best performance in a classifier. These two types of features are more orthogonal, thus capturing different perspectives of the text to enrich the feature space. Having many different types of features allows the classifier a wide range of space on which to create a decision boundary between different class labels. An example of very high-level features can be found in Massung et al. [2013]. Consider the grammatical parse tree discussed in Chapter 4 reproduced in Figure 15 Figure 15.2 A grammatical parse tree and different feature representations derived from it. For each feature type, each dimension in a feature vector would correspond to a weight of a particular parse tree structure. 