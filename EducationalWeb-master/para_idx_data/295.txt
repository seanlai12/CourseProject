"dog" "a" "the" "fish" Say we have the unigram language model θ estimated on a document we wish to summarize. We wish to draw words w 1 , w 2 , w 3 , . . . from θ that will comprise our summary. We want the word w i to occur in our summary with about the same probability it occurred in the original document-this is how our generated text will approximate the longer document. Figure 16.3 depicts how we can accomplish this task. First, we create a list of all our parameters and incrementally sum their probabilities; this will allow us to use a random number on [0, 1] to choose a word w i . Simply, we get a uniform random floating point number between zero and one. Then, we iterate through the words in our vocabulary, summing their probabilities until we get to the random number. We output the term and repeat the process. In the example, imagine we have the following values: Say we generate a random number x 1 using a uniform distribution on [0, 1]. This is denoted as x 1 ∼ U(0, 1). Now imagine that x 1 = 0.032. We go to the cumulative point 0.032 in our distribution and output "a". We can repeat this process until our summary is of a certain length or until we generate an end-of-sentence token </s>. At this point, you may be thinking that the text we generate will not make any sense-that is certainly true if we use a unigram language model since each word is generated independently without regard to its context. If more fluent language is required, we can use an n-gram language model, where n > 1. Instead of each word being independently generated, the new word will depend on the previous n − 1 