Naive Bayes is an example of a generative classifier. It creates a probability distribution of features over each class label in addition to a distribution of the class labels themselves. This is very similar to language model topic probability calculation. With the language model, we create a distribution for each topic. When we see a new text object, we use our existing topics to find topic language modelθ that is most likely to have generated it. Recall from Chapter 2 that θ = arg max θ p(w 1 , . . . , w n | θ) = arg max θ n i=1 p(w i | θ). (15.1) 