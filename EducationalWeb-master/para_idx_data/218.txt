Probability that a randomly picked word from d 1 is w i Probability that two randomly picked words from d 1 and d 2 , respectively, are identical Count of word w i in d 1 Total counts of words in d 1  .5 shows one plausible approach, where we match the similarity of context based on the expected overlap of words, and we call this EOW. We represent a context by a word vector where each word has a weight that's equal to the probability that a randomly picked word from this document vector is the current word. Equivalently, given a document vector x, x i is defined as the normalized account of word w i in the context, and this can be interpreted as the probability that you would randomly pick this word from d 1 . The x i 's would sum to one because they are normalized frequencies, which means the vector is a probability distribution over words. The vector d 2 can be computed in the same way, and this would give us then two probability distributions representing two contexts. This addresses the problem of how to compute the vectors. For similarity, we simply use a dot product of two vectors. The dot product, in fact, gives us the probability that two randomly picked words from the two contexts are identical. That means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? If the two contexts are very similar, then we should expect we frequently will see the two words picked from the two contexts are identical. If they are very different, then the chance of seeing identical words being picked from the two contexts would be small. This is quite intuitive for measuring similarity of contexts. Let's look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical. Each term in the sum gives us the probability that we will see an overlap on a particular word w i , where x i gives us a probability that we will pick this particular word from d 1 , and y i gives us the probability of picking this word from d 2 . This is how expected overlap of words in context similarity works. As always, we would like to assess whether this approach would work well. Ultimately, we have to test the approach with real data and see if it gives us really semantically related words. Analytically, we can also analyze this formula. Initially, it does make sense because this formula will give a higher score if there is more overlap between the two contexts. However, if you analyze the formula more carefully, then you also see there might be some potential problems. The first problem is that it might favor matching one frequent term very well over matching more distinct terms. That is because in the dot product, if one element has a high value and this element is shared by both contexts, it contributes a lot to the overall sum. It might indeed make the score higher than in another case where the two vectors actually have much overlap in different terms. In our case, we should intuitively prefer a case where we match more different terms in the context, so that we have more confidence in saying that the two words indeed occur in similar context. If you only rely on one high-scoring term, it may not be robust. The second problem is that it treats every word equally. If we match a word like the, it will be the same as matching a word like eats, although we know matching the isn't really surprising because it occurs everywhere. This is another problem of this approach. We can introduce some heuristics used in text retrieval that solve these problems, since problems like these also occur when we match a query with a document. To tackle the first problem, we can use a sublinear transformation of term frequency. That is, we don't have to use the raw frequency count of the term to represent the context. To address this problem, we can transform it into some form that wouldn't emphasize the raw frequency so much. To address the second problem, we can reward matching a rare word. A sublinear transformation of term frequency and inverse document frequency (IDF) weighting are exactly what we'd like here; we discussed these types of weighting schemes in Chapter 6. In order to achieve this desired weighting, we will use BM25 weighting, which is of course based on the BM25 retrieval function. It is able to solve the above two problems by sublinearly transforming the count of w i in d 1 and including the IDF weighting heuristic in the similarity measure. For this similarity scheme, we define the document vector as containing elements representing normalized BM25 TF values, as shown in Figure 13.6. The normalization function takes a sum over all the words in order to normalize the weight of each word by the sum of the weights of all the words. This is to ensure all the x i 's will sum to one in this vector. This would be very similar to what we had before, in that this vector approximates a word distribution (since the x i 's will sum to one). For the IDF factor, the similarity function multiplies the IDF of word w i by x i y i , which is the similarity in the i th dimension. Thus, the first problem (sublinear scaling) is addressed in the vector representation and the second problem (lack of IDF) is addressed in the similarity function itself. 