Google's PageRank, a main technique that was used originally for link analysis, is a good example of leveraging page link information. PageRank captures page popularity, which is another word for authority. The intuition is that links are just like citations in literature. Think about one page pointing to another page; this is very similar to one paper citing another paper. Thus, if a page is cited often, we can assume this page is more useful. PageRank takes advantage of this intuition and implements it in a principled approach. In its simplest sense, PageRank is essentially doing citation counting or inlink counting. It improves this simple idea in two ways. One is to consider indirect citations. This means you don't just look at the number of inlinks, rather you also look at the inlinks of your inlinks, recursively. If your inlinks themselves have many inlinks, your page gets credit from that. In short, if important pages are pointing to you, you must also be important. On the other hand, if those pages that are pointing to you are not pointed to by many other pages, then you don't get that much credit. This is the concept of indirect citations, or cascading citations. Again, we can understand this idea by considering research papers. If you are cited by ten papers that are not very influential, that's not as good as if you're cited by ten papers that themselves have attracted a lot of other citations. Clearly, this is a case where we would like to consider indirect links, which is exactly what PageRank does. The other idea is that it's good to smooth the citations to accommodate potential citations that have not yet been observed. Assume that every page has a non-zero pseudo citation count. Essentially, you are trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone. Another way to understand PageRank is the concept of a random surfer visiting every web page. Let's take a look at this example in detail, illustrated in Figure 10 page, and the edges between documents are hyperlinks connecting them to each other. Let's assume that a random surfer or random walker can be on any of these pages. When the random surfer decides to move to a different page, they can either randomly follow a link from the current page or randomly choose a document to jump to from the entire collection. So, if the random surfer is at d 1 , with some probability that random surfer will follow the links to either d 3 or d 4 . The random surfing model also assumes that the surfer might get bored sometimes and decide to ignore the actual links, randomly jumping to any page on the web. If the surfer takes that option, they would be able to reach any of the other pages even though there is no link directly to that page. Based on this model, we can ask the question, "How likely, on average, would the surfer reach a particular page?" This probability is precisely what PageRank computes. The PageRank score of a document d i is the average probability that the surfer visits d i . Intuitively, this should be proportional to the inlink count. If a page has a high number of inlinks then it would have a higher chance of being visited since there will be more opportunities of having the surfer follow a link there. This is how the random surfing model captures the idea of counting the inlinks. But, it also considers the indirect inlinks; if the pages that point to d i have themselves a lot of inlinks, that would mean the random surfer would very likely reach one of them. This increases the chance of visiting d i . This is a nice way to capture both indirect and direct links. Mathematically, we can represent this document network as a matrix M, displayed in the center of Figure 10.7. Each row stands for a starting page. For example, row one would indicate the probability of going to any of the four pages from d 1 . We see there are only two non-zero entries. Each is one half since d 1 is pointing to only two other pages; thus if we can randomly choose to visit either of them from d 1 , they'd each have a probability of 1 2 . We have zeros for the first two columns for d 1 since d 1 doesn't link to itself and it doesn't link to d 2 . Thus, M ij is the probability of going from d i to d j . Each row's values should sum to one, because the surfer will have to go to precisely one of these pages. Now, how can we compute the probability of a surfer visiting a particular page? We can compute the probability of reaching a page as follows: reach d j by random jumping (10.1) On the left-hand side is the probability of visiting page d j at time t + 1, the next time count. On the right-hand side, we can see the equation involves the probability at page d i at time t, the current time step. The equation captures the two possibilities of reaching a page d j at time t + 1: through random surfing or following a link. The first part of the equation captures the probability that the random surfer would reach this page by following a link. The random surfer chooses this strategy with probability 1 − α; thus, there is a factor of 1 − α before this term. This term sums over all the possible N pages that the surfer could have been at time t. Inside the sum is the product of two probabilities. One is the probability that the surfer was at d i at time t. That's p t (d i ). The other is the transition probability from d i to d j , which we know is represented as M ij . So, in order to reach this d j page, the surfer must first be at d i at time t and would have to follow the link to go from d i to d j . The second part is a similar sum. The only difference is that now the transition probability is uniform: 1 N . This part captures the probability of reaching this page through random jumping, where α is the probability of random jumping. This also allows us to see why PageRank captures a smoothing of the transition matrix. You can think this 1 N comes from another transition matrix that has all the elements as 1 N . It is then clear that we can merge the two parts. Because they are of the same form, we can imagine there's a different matrix that's a combination of this M and the uniform matrix I . In this sense, PageRank uses this idea of smoothing to ensure that there's no 0 entry in the transition matrix. Now, we can imagine that if we want to compute average probabilities, they would satisfy this equation without considering the time index. So let's drop the time index and assume that they would be equal; this would give us N equations, since each page has its own equation. Similarly, there are also precisely N variables. This means we now have a system of N linear equations with N variables. The problem boils down to solving this system of equations, which we can write in the following form: 