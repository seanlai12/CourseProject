Use Lagrange multiplier approach Lagrange function: Figure 17.11 Computation of a maximum likelihood estimate for a unigram language model. Now that we have a well defined likelihood function, we will attempt to find the parameter values (i.e., word probabilities) that maximize this likelihood function. Let's take a look at the maximum likelihood estimation problem more closely in Figure 17.11. The first line is the original optimization problem of finding the maximum likelihood estimate. The next line shows an equivalent optimization problem with the log-likelihood. The equivalence is due to the fact that the logarithm function results in a monotonic transformation of the original likelihood function and thus does not affect the solution of the optimization problem. Such a transformation is purely for mathematical convenience because after the logarithm transformation our function will become a sum instead of product; the sum makes it easier to take the derivative, which is often needed for finding the optimal solution of this function. Although simple, this log-likelihood function reflects some general characteristics of a log-likelihood function of some more complex generative models. . The sum is over all the unique data points (the words in the vocabulary). . Inside the sum, there's a count of each unique data point, i.e., the count of each word in the observed data, which is multiplied by the logarithm of the probability of the particular unique data point. At this point, our problem is a well-defined mathematical optimization problem where the goal is to find the optimal solution of a constrained maximization problem. The objective function is the log-likelihood function and the constraint is that all the word probabilities must sum to one. How to solve such an optimization problem is beyond the scope of this book, but in this case, we can obtain a simple analytical solution by using the Lagrange multiplier approach. This is a commonly used approach, so we provide some detail on how it works in Figure 17.11. We will first construct a Lagrange function, which combines our original objective function with another term that encodes our constraint with the Lagrange multiplier, denoted by λ, introducing an additional parameter. It can be shown that the solution to the original constrained optimization problem is the same as the solution to the new (unconstrained) Lagrange function. Since there is no constraint involved any more, it is straightforward to solve this optimization problem by taking partial derivatives with respect to all the parameters and setting all of them to zero, obtaining an equation for each parameter. 1 We thus have, in total, M + 1 linear equations, corresponding to the M word probability parameters and λ. Note that the equation for the Lagrange multiplier λ is precisely our original constraint. We can easily solve this system of linear equations to obtain the Maximum Likelihood estimate of the unigram language model as This has a very meaningful interpretation: the estimated probability of a word is the count of each word normalized by the document length, which is also a sum of all the counts of words in the document. This estimate mostly matches our intuition in order to maximize the likelihood: words observed more often "deserve" higher probabilities, and only words observed are "allowed" to have non-zero probabilities (unseen words should have a zero probability). In general, maximum likelihood estimation tends to result in a probability estimated as normalized counts of the corresponding event so that the events observed often would have a higher probability and the events not observed would have zero probability. While we have obtained an analytical solution to the maximum likelihood estimate in this simple case, such an analytical solution is not always possible; indeed, it is often impossible. The optimization problem of the MLE can often be very complicated, and numerical optimization algorithms would generally be needed to solve the problem. What would the topic discovered from a document look like? Let's imagine the document is a text mining paper. In such a case, the estimated unigram language model (word distribution) may look like the distribution shown in Figure 17.12. On the top, you will see the high probability words tend to be those very common words, often function words in English. This will be followed by some content words that really characterize the topic well like text and mining. In the end, you also see there is a small probability of words that are not really related to the topic but might happen to be mentioned in the document. As a topic representation, such a distribution is not ideal because the high probability words are function words, which do not characterize the topic. Giving common words high probabilities is a direct consequence of the assumed generative model, which uses one distribution to generate all the words in the text. How can we improve our generative model to down-weight such common words in the estimated word distribution for our topic? The answer is that we can introduce a second background word distribution into the generative model so that the common words can be generated from this background model, and thus the topic word distribution would only need to generate the content-carrying topical words. Such a model is called a mixture model because multiple component models are "mixed" together to generate data. We discuss it in detail in the next section. 