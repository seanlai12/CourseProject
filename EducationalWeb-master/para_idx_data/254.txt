Brown clustering [Brown et al. 1992] is a model-based term clustering algorithm that constructs term clusters (called word classes) to maximize the likelihood of an n-gram class language model. However, since the optimization problem is intractable to solve computationally, the actual process of constructing term clusters is actually similar to hierarchical agglomerative clustering where single words are 14.3 Term Clustering 289 merged gradually, but the criterion for merging in Brown clustering is based on a similarity function derived from the likelihood function. Specifically, the maximization of the likelihood function is shown to be equivalent to maximization of the mutual information of adjacent word classes, thus when merging two words, the algorithm would favor merging two words that are distributed very similarly since when such words are replaced by their respective classes, it would minimize the decrease of mutual information between adjacent classes. Mathematically, assuming that we partition all the words in the vocabulary into C classes, the n-gram class language model defines the probability of observing a word w n given that we have already n − 1 words preceeding w n , i.e., w n−1 , . . . , w 1 as where c i is the class of word w i . It essentially assumes that the probability of observing w n only depends on the classes of the previous words, but does not depend on the specific words, thus unless C is the same as vocabulary size (i.e., every word is in its own class), the n-gram class language model always has fewer parameters than the regular n-gram language model. As a generative model, we would generate a word by first looking up the classes of the previous words, i.e., c n−1 , . . . , c 1 , then sample a class for the n-th position c n using p(c n | c n−1 , . . . , c 1 ), and finally sample a word at the n-th position by using p(w | c n ). The distribution p(w | c n ) captures how frequently we will observe word w when the latent class c n is used. If we are given the partitioning of words into C classses, then the maximum likelihood estimation is not hard as we can simply replace the words with their corresponding classes to estimate p(c n | c n−1 , . . . , c 1 ) in the same way as we would for estimating a regular n-gram language model, and the probability of a word given a particular class p(w | c) can also be easily estimated by pooling together all the observations of words in the data belonging to the class c and normalizing their counts, which gives an estimate of p(w | c) essentially based on the count of word w in the whole data set. However, finding the best partitioning of words is computationally intractable. Fortunately, we can use a greedy algorithm to construct word classes in very much the same way as agglomerative hierarchical clustering, i.e., gradually merging words to form classes by keeping track of the objective of maximizing the likelihood. A neat theoretical result is that the maximization of the likelihood is equivalent to maximization of the mutual information between all the adjacent classes in the case of bigram model. Thus, the best pairs of words to merge would tend to 290 Chapter 14 Text Clustering be those that are distributed in very similar contexts (e.g., Tuesday and Wednesday) since by putting such words in the same class, the prediction power of the class would be about the same as that of the original word, allowing to minimize the loss of mutual information. Computation-wise, we simply do agglomerative hierarchical clustering and measure the "distance" of two words based on a derived function based on the likelihood function that can capture the loss of mutual information due to merging the two words. Due to the complexity of the model, only bigrams (n = 2) were originally investigated [Brown et al. 1992]. Empirically, the bigram class language model has been shown to work very well and can generate very high-quality paradigmatic word associations directly by treating words in the same class as having paradigmatic relation. Figure 14.6 shows some sample word clusters taken from Brown et al. [1992]; they clearly capture paradigmatic relations well. The model can also be used to generate syntagmatic associations by essentially computing the pointwise mutual information between words that occur in different  positions. When the window of co-occurrences is restricted to two words (i.e., adjacent co-occurrences), the model can discover "sticky phrases" (see Figure 14.7 for sample results), which are non-compositional phrases whose meaning is not a direct composition of the meanings of individual words. Such non-compositional phrases can also be discovered using some other statistical methods (see, e.g., Zhai 1997, Lin 1999. 