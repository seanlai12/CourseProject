Matched query terms IDF weighting is in terms of words that we observe in the query. Just like in the vector space model, we are now able to take a sum of terms in the intersection of the query vector and the document vector. If we look at this rewriting further as shown in Figure 6.25, we can see how it actually would give us two benefits. The first benefit is that it helps us better understand the ranking function. In particular, we're going to show that from this formula we can see the connection of smoothing using a collection language model with weighting heuristics similar to TF-IDF weighting and length normalization. The second benefit is that it also allows us to compute the query likelihood more efficiently, since we only need to consider terms matched in the query. We see that the main part of the formula is a sum over the matching query terms. This is much better than if we take the sum over all the words. After we smooth the document using the collection language model, we would have nonzero probabilities for all the words w ∈ V . This new form of the formula is much easier to compute. It's also interesting to note that the last term is independent of the document being scored, so it can be ignored for ranking. Ignoring this term won't affect the order of the documents since it would just be the same value added onto each document's final score. Inside the sum, we also see that each matched query term would contribute a weight. This weight looks like TF-IDF weighting from the vector space models. First, we can already see it has a frequency of the word in the query, just like in the vector space model. When we take the dot product, the word frequency in the query appears in the sum as a vector element from the query vector. The corresponding term from the document vector encodes a weight that has an effect similar to TF-IDF weighting. p seen is related to the term frequency in the sense that if a word occurs very frequently in the document, then the seen probability will tend to be larger. This term is really doing something like TF weighting. In the denominator, we achieve the IDF effect through p(w | C), or the popularity of the term in the collection. Because it's in the denominator, a larger collection probability actually makes the weight of the entire term smaller. This means a popular term carries a smaller weight-this is precisely what IDF weighting is doing! Only now, we have a different form of TF and IDF. Remember, IDF has a logarithm of document frequency, but here we have something different. Intuitively, however, it achieves a similar effect to the VS interpretation. We also have something related to the length normalization. In particular, α d might be related to document length. It encodes how much probability mass we want to give to unseen words, or how much smoothing we are allowed to do. Intuitively, if a document is long then we need to do less smoothing because we can assume that it is large enough that we have probably observed all of the words that the author could have written. If the document is short, the number of unseen words is expected to be large, and we need to do more smoothing in this case. Thus, α d penalizes long documents since it tends to be smaller for long documents. The variable α d actually occurs in two places. Thus its overall effect may not necessarily be penalizing long documents, but as we will see later when we consider smoothing methods, α d would always penalize long documents in a specific way. This formulation is quite convenient since it means we don't have to think about the specific way of doing smoothing. We just need to assume that if we smooth with the collection language model, then we would have a formula that looks like TF-IDF weighting and document length normalization. It's also interesting that we have a very fixed form of the ranking function. Note that we have not heuristically put a logarithm here, but have used a logarithm of query likelihood for scoring and turned the product into a sum of logarithms of probabilities. If we only want to heuristically implement TF-IDF weighting, we don't necessarily have to have a logarithm. Imagine if we drop this logarithm; we would still have TF and IDF weighting. But, what's nice with probabilistic modeling is that we are automatically given a logarithm function which achieves sublinear scaling of our term "weights." In summary, a nice property of probabilistic models is that by following some assumptions and probabilistic rules, we'll get a formula by derivation. If we heuristically design the formula, we may not necessarily end up having such a specific form. Additionally, we talked about the need for smoothing a document language model. Otherwise, it would give zero probability for unseen words in the document, which is not good for scoring a query with an unseen word. It's also necessary to improve the accuracy of estimating the model representing the topic of this document. The general idea of smoothing in retrieval is to use the collection language model to give us some clue about which unseen word would have a higher proba- 