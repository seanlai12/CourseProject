unigram language model to model the data. The behaviors of the ML estimate of such a mixture model ensure that the use of a fixed background model in such a specialized mixture model can effectively factor out common words such as the in the other topic word distribution, making the discovered topic more discriminative. We may view our specialized mixture model as one where we have imposed a very strong prior on the model parameter and we use Bayesian parameter estimation. Our prior is on one of the two unigram language models and it requires that this particular unigram LM must be exactly the same as a pre-defined background language model. In general, Bayesian estimation would seek for a compromise between our prior and the data likelihood, but in this case, we can assume that our prior is infinitely strong, and thus there is essentially no compromise, holding one component model as constant (the same as the provided background model). It is useful to point out that this mixture model is precisely the mixture model for feedback in information retrieval that we introduced earlier in the book. 